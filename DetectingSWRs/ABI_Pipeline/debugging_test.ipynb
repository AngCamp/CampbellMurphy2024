{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allen Visual Behaviour SWR Collection Script\n",
    "# Called by a bash script\n",
    "# Produces SWR events at a 2.5 zscore threshold\n",
    "# also produces time series lsiting gamma band events (minimum 0.015 s long) and motion artifacts on a channel outside brain\n",
    "# TO DO:  Interpolate between all the channels some how and merge the lfp signals from ca1 into a big 6 by samples array to run the detector on.\n",
    "\n",
    "# Technical details of this dataset are located here: https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/f7/06/f706855a-a3a1-4a3a-a6b0-3502ad64680f/visualbehaviorneuropixels_technicalwhitepaper.pdf\n",
    "# IF that link does not work this url can also work: https://portal.brain-map.org/explore/circuits/visual-behavior-neuropixels \n",
    "\n",
    "# Allen Visual Behaviour SWR detection script\n",
    "\n",
    "# libraries\n",
    "import os\n",
    "import subprocess \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io, signal\n",
    "#from fitter import Fitter, get_common_distributions, get_distributions\n",
    "import scipy.ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "# for ripple detection\n",
    "import ripple_detection\n",
    "from ripple_detection import filter_ripple_band\n",
    "import ripple_detection.simulate as ripsim # for making our time vectors\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "from scipy import interpolate\n",
    "from scipy.signal import firwin, lfilter\n",
    "from multiprocessing import Pool, Queue, Process\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "import argparse\n",
    "\n",
    "# start timing\n",
    "start_time_outer = time.time()  # start timing\n",
    "\"\"\"\n",
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(description='Process parameters.')\n",
    "\n",
    "# Add the arguments\n",
    "parser.add_argument('--pool_size', type=int, help='The pool size')\n",
    "parser.add_argument('--sdk_cache_dir', type=str, help='The SDK cache directory')\n",
    "parser.add_argument('--output_dir', type=str, help='The output directory')\n",
    "parser.add_argument('--swr_output_dir', type=str, help='The SWR output directory')\n",
    "parser.add_argument('--run_name', type=str, help='The run name')\n",
    "parser.add_argument('--select_these_sessions', nargs='*', help='The selected sessions')\n",
    "parser.add_argument('--only_brain_observatory_sessions', type=bool, help='Only use brain observatory sessions')\n",
    "parser.add_argument('--dont_wipe_these_sessions', nargs='*', help='Don\\'t wipe these sessions')\n",
    "parser.add_argument('--gamma_event_thresh', type=int, help='The gamma event threshold')\n",
    "parser.add_argument('--gamma_filters_path', type=str, help='The gamma filters path')\n",
    "parser.add_argument('--theta_filter_path', type=str, help='The theta filter path')\n",
    "parser.add_argument('--ripple_band_threshold', type=int, help='The ripple band threshold')\n",
    "parser.add_argument('--movement_artifact_ripple_band_threshold', type=int, help='The movement artifact ripple band threshold')\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\"\"\"\n",
    "\n",
    "# change these as needed:\n",
    "pool_size = 6 # 10 was too high, program crashed \n",
    "sdk_cache_dir='/space/scratch/allen_visbehave_data'# path to where the cache for the allensdk is (wehre the lfp is going)\n",
    "output_dir = '/space/scratch/allen_visbehave_swr_data'\n",
    "#swr_output_dir = 'allen_visbehave_swr_2sd_envelope' # directory specifying the output\n",
    "swr_output_dir = 'testing_dir' # directory specifying the output\n",
    "run_name = 'final_run' # part of the nameing for the output files\n",
    "\n",
    "# example input\n",
    "select_these_sessions = []\n",
    "only_brain_observatory_sessions = True # if true only sessions from the brain observatory will be used\n",
    "#select_these_sessions = [715093703, 719161530, 721123822]\n",
    "#select_these_sessions = [715093703]\n",
    "#select_these_sessions = [746083955] # the first screwed up when selecting a probe. Session id\n",
    "#select_these_sessions = [816200189] # this one messed up when doing the movement artifact detection at probe 836943715\n",
    "dont_wipe_these_sessions = []\n",
    "\n",
    "# THRESHOLDS\n",
    "gamma_event_thresh = 3 # zscore threshold for gamma events\n",
    "\n",
    "# gamma filters are same as default for now\n",
    "gamma_filters_path = '/home/acampbell/NeuropixelsLFPOnRamp/PowerBandFilters/swr_detection_script_filters_1500Hz/frank2008_gamma_1500hz_bandpass_filter.npz'\n",
    "theta_filter_path = '/home/acampbell/NeuropixelsLFPOnRamp/PowerBandFilters/swr_detection_script_filters_1500Hz/theta_1500hz_bandpass_filter.npz'\n",
    "\n",
    "ripple_band_threshold = 2 # note this defines the threshold for envelopes, from these events identify ones with peaks that pass a peak-power threshold as well\n",
    "movement_artifact_ripple_band_threshold = 2\n",
    "\n",
    "\n",
    "\n",
    "# functions\n",
    "\n",
    "# subprocess is a default module\n",
    "def call_bash_function(bash_command = \"\"):\n",
    "    #example bash comand:\n",
    "    #bash_command = \"source /path/to/your/bash_script.sh && your_bash_function\"\n",
    "    process = subprocess.Popen(bash_command, stdout=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        print(\"Bash function executed successfully.\")\n",
    "        print(\"Output:\", output.decode('utf-8'))\n",
    "    else:\n",
    "        print(\"Error:\", error.decode('utf-8'))\n",
    "\n",
    "# Assuming you have your signal_array, b, and a defined as before\n",
    "def finitimpresp_filter_for_LFP(LFP_array, samplingfreq, lowcut = 1, highcut = 250,\n",
    "                    filter_order = 101):\n",
    "    \n",
    "    nyquist = 0.5 * samplingfreq\n",
    "\n",
    "    # Design the FIR bandpass filter using scipy.signal.firwin\n",
    "    fir_coeff = signal.firwin(filter_order, [lowcut / nyquist, highcut / nyquist],\n",
    "                              pass_zero=False, fs=samplingfreq)\n",
    "\n",
    "    # Apply the FIR filter to your signal_array\n",
    "    #filtered_signal = signal.convolve(LFP_array, fir_coeff, mode='same', method='auto')\n",
    "    filtered_signal = signal.lfilter(fir_coeff, 1.0, LFP_array, axis=0)\n",
    "    return(filtered_signal)\n",
    "\n",
    "\n",
    "def event_boundary_detector(time, five_to_fourty_band_power_df, envelope=True, minimum_duration = 0.02, maximum_duration = 0.4,\n",
    "                       threshold_sd=2.5, envelope_threshold_sd=1):\n",
    "    \"\"\"\n",
    "    Power threshold event detector, includes an envelope as well if wanted\n",
    "    \n",
    "    Originally for detecting sharp waves in the striatum radiatum, takes in power signal from \n",
    "    \n",
    "    From Fernández-Ruiz, A., Oliva, A., Fermino de Oliveira, E., Rocha-Almeida, F., Tingley, D., \n",
    "    & Buzsáki, G. (2019). Long-duration hippocampal sharp wave ripples improve memory. Science, 364(6445), 1082-1086.\n",
    "    \n",
    "    \n",
    "    Sharp waves were detected separately using LFP from a CA1 str. radiatum channel, filtered with band-pass filter boundaries\n",
    "   (5-40 Hz). LFP events of a minimum duration of 20 ms and maximum 400 ms exceeding 2.5 SD of the\n",
    "   background signal were included as candidate SPWs. Only if a SPW was simultaneously detected with\n",
    "   a ripple, a CA1 SPW-R event was retained for further analysis. SPW-R bursts were classified when more\n",
    "   than one event was detected in a 400 ms time window.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # make df to fill\n",
    "    row_of_info =  {\n",
    "        'start_time': [],\n",
    "        'end_time': [],\n",
    "        'duration': [],\n",
    "        }\n",
    "\n",
    "    #sharp_wave_events_df = pd.DataFrame()\n",
    "    #scored_wave_power = stats.zscore(five_to_fourty_band_df)\n",
    "    \n",
    "    # compute our power threshold\n",
    "    #wave_band_sd_thresh = np.std(five_to_fourty_band_df)*threshold_sd\n",
    "    five_to_fourty_band_power_df = stats.zscore(five_to_fourty_band_power_df)\n",
    "    past_thresh = five_to_fourty_band_power_df>=threshold_sd\n",
    "    \n",
    "    # now we expand the sections that are past thresh up to the points that \n",
    "    # are past the envelope thresh, so not all sections above envelope thresh are true\n",
    "    # but those sections which alse contain a region past the detection threshold are included\n",
    "    def expand_sections(z_scores, boolean_array, thresh):\n",
    "        # Find indices where boolean_array is True\n",
    "        true_indices = np.where(boolean_array)[0]\n",
    "\n",
    "        # Initialize an array to keep track of expanded sections\n",
    "        expanded_sections = np.zeros_like(z_scores, dtype=bool)\n",
    "\n",
    "        # Iterate over true_indices and expand sections\n",
    "        for index in true_indices:\n",
    "            # Find the start and end of the current section\n",
    "            start = index\n",
    "            end = index\n",
    "\n",
    "            # Expand section to the left (while meeting conditions)\n",
    "            while start > 0 and z_scores[start - 1] >  thresh:\n",
    "                start -= 1\n",
    "\n",
    "            # Expand section to the right (while meeting conditions)\n",
    "            while end < len(z_scores) - 1 and z_scores[end + 1] >  thresh:\n",
    "                end += 1\n",
    "\n",
    "            # Check if the expanded section contains a point above envelope_threshold_sd in z_scores\n",
    "            if any(z_scores[start:end + 1] >  thresh):\n",
    "                expanded_sections[start:end + 1] = True\n",
    "\n",
    "        # Update the boolean_array based on expanded_sections\n",
    "        boolean_array = boolean_array | expanded_sections\n",
    "\n",
    "        return boolean_array\n",
    "    \n",
    "    if envelope==True:\n",
    "        past_thresh = expand_sections(z_scores=five_to_fourty_band_power_df,\n",
    "                                  boolean_array= past_thresh,\n",
    "                                  thresh = envelope_threshold_sd)\n",
    "    \n",
    "    \n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info['start_time'] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info['end_time'] = time[ends]\n",
    "    \n",
    "    row_of_info['duration'] = [row_of_info['end_time'][i]-row_of_info['start_time'][i] for i in range(0,len(row_of_info['start_time']))]\n",
    "    \n",
    "    #turn the dictionary into adataframe\n",
    "    sharp_wave_events_df = pd.DataFrame(row_of_info)\n",
    "    \n",
    "    # filter for the duration range we want\n",
    "    in_duration_range = (sharp_wave_events_df.duration>minimum_duration)&(sharp_wave_events_df.duration<maximum_duration)\n",
    "    sharp_wave_events_df = sharp_wave_events_df[in_duration_range]\n",
    "    \n",
    "    return sharp_wave_events_df\n",
    "\n",
    "def event_boundary_times(time, past_thresh):\n",
    "    \"\"\"\n",
    "    finds the times of a vector of true statements and returns values from another\n",
    "    array representing the times\n",
    "    \"\"\"\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info['start_time'] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info['end_time'] = time[ends]\n",
    "    \n",
    "    row_of_info['duration'] = [row_of_info['end_time'][i]-row_of_info['start_time'][i] for i in range(0,len(row_of_info['start_time']))]\n",
    "    \n",
    "    #turn the dictionary into adataframe\n",
    "    events_df = pd.DataFrame(row_of_info)\n",
    "      \n",
    "    return events_df\n",
    "\n",
    "def peaks_in_events(events, time_values, signal_values):\n",
    "    # looks for the peaks in the ripple power signal, value of zscored raw lfp peak and returns time of peak\n",
    "    signal_values_zscore = stats.zscore(signal_values)\n",
    "    max_values = []\n",
    "    max_lfp_zscore_values = []\n",
    "    peak_times = []\n",
    "    for start, end in zip(events['start_time'], events['end_time']):\n",
    "        window_idx = (time_values >= start) & (time_values <= end)\n",
    "        ripplesignal = signal_values[window_idx]\n",
    "        ripple_lfp_zscore_signal = signal_values_zscore[window_idx]\n",
    "        maxpoint = np.argmax(ripplesignal)\n",
    "        max_values.append(ripplesignal[maxpoint])\n",
    "        max_lfp_zscore_values.append(ripple_lfp_zscore_signal[maxpoint])\n",
    "        rippletimepoints = time_values[window_idx]\n",
    "        peak_times.append(rippletimepoints[maxpoint])\n",
    "    return np.array(max_values), np.array(max_lfp_zscore_values),  np.array(peak_times)\n",
    "\n",
    "\n",
    "def resample_signal(signal, times, new_rate):\n",
    "    \"\"\"\n",
    "    Resample a 2D signal array to a new sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    signal (np.array): 2D array where each column is a source and each row is a time point.\n",
    "    times (np.array): 1D array of times corresponding to the rows of the signal array.\n",
    "    new_rate (float): The new sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "    new_signal (np.array): The resampled signal array.\n",
    "    new_times (np.array): The times corresponding to the rows of the new signal array.\n",
    "    \"\"\"\n",
    "    nsamples_new = int(len(times) * new_rate / (len(times) / times[-1]))\n",
    "    new_times = np.linspace(times[0], times[-1], nsamples_new)\n",
    "    new_signal = np.zeros((nsamples_new, signal.shape[1]))\n",
    "\n",
    "    for i in range(signal.shape[1]):\n",
    "        interp_func = interpolate.interp1d(times, signal[:, i], bounds_error=False, fill_value=\"extrapolate\")\n",
    "        new_signal[:, i] = interp_func(new_times)\n",
    "\n",
    "    return new_signal, new_times\n",
    "\n",
    "\n",
    "\n",
    "# Set up error logging \n",
    "MESSAGE = 25  # Define a custom logging level, between INFO (20) and WARNING (30)\n",
    "\n",
    "def listener_process(queue):\n",
    "    \"\"\"\n",
    "    This function listens for messages from the logging module and writes them to a log file.\n",
    "    It sets the logging level to MESSAGE so that only messages with level MESSAGE or higher are written to the log file.\n",
    "    This is a level we created to be between INFO and WARNING, so to see messages from this code and errors  but not other\n",
    "    messages that are mostly irrelevant and make the log file too large and uninterpretable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queue : multiprocessing.Queue\n",
    "        The queue to get messages from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    root = logging.getLogger()\n",
    "    h = logging.FileHandler(f'ibl_detector_{swr_output_dir}_{run_name}_app.log', mode='w')\n",
    "    f = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "    h.setFormatter(f)\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "\n",
    "    while True:\n",
    "        message = queue.get()\n",
    "        if message == 'kill':\n",
    "            break\n",
    "        logger = logging.getLogger(message.name)\n",
    "        logger.handle(message)\n",
    "\n",
    "def init_pool(*args):\n",
    "    h = logging.handlers.QueueHandler(queue)\n",
    "    root = logging.getLogger()\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "    \n",
    "# loading filters (crates artifacts in first and last ~ 3.5 seconds of recordings, remember to clip these off)\n",
    "gamma_filter = np.load(gamma_filters_path)\n",
    "gamma_filter = gamma_filter['arr_0']\n",
    "\n",
    "theta_filter = np.load(theta_filter_path)\n",
    "theta_filter = theta_filter['arr_0']\n",
    "# Setting up the ABI Cache\n",
    "manifest_path = os.path.join(sdk_cache_dir, \"manifest.json\")\n",
    "\n",
    "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
    "\n",
    "# we start by calling and filtering our dataframe of the sessions we will be working with\n",
    "sessions = cache.get_session_table()\n",
    "\n",
    "if only_brain_observatory_sessions:\n",
    "    sessions = sessions[sessions.session_type == 'brain_observatory_1.1']\n",
    "\n",
    "if len(select_these_sessions)>0:\n",
    "    sessions = sessions.loc[ sessions.index.intersection(select_these_sessions) ]\n",
    "\n",
    "\n",
    "# Looping through the sessions specified\n",
    "\"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "\n",
    "    # Replace this with your actual loop or task\n",
    "    for i in tqdm(range(10), desc=\"Processing\", unit=\"iteration\"):\n",
    "        # Simulate some work\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(\"Task completed!\")\n",
    "\"\"\"\n",
    "\n",
    "# include a data frame that lists the sessions used, channels taken from each session and for what (movement vs ripples),\n",
    "# and if there is behavioural data or lfp data\n",
    "\n",
    "\n",
    "# Create main folder\n",
    "swr_output_dir_path = os.path.join(output_dir, swr_output_dir)\n",
    "os.makedirs(swr_output_dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "#for seshnum in tqdm(range(0, sessions.shape[0]), desc=\"Processing\", unit=\"iteration\"):\n",
    "def process_session(session_id):\n",
    "    \"\"\"\n",
    "    This function takes in a session id and loops through the probes in that session,\n",
    "    for each probe it finds the CA1 channel with the highest ripple power and uses that\n",
    "    channel to detect SWR events.  It also detects gamma events and movement artifacts\n",
    "    on two channels outside of the brain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session_id : int\n",
    "        The session id for the session to be processed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    but...\n",
    "    Saves the following files to the folder specified by swr_output_dir_path:\n",
    "        - a csv file for each probe with the SWR events detected on the CA1 channel with the highest ripple power\n",
    "        - a csv file for each probe with the gamma events detected on the CA1 channel with the highest ripple power\n",
    "        - a csv file for each probe with the movement artifacts detected on the two channels outside of the brain\n",
    "        - a numpy array of the LFP from the CA1 channels used for SWR detection\n",
    "        - a numpy array of the LFP from the two channels outside of the brain used for movement artifact detection\n",
    "        - a numpy array of the times of samples in the interpolated LFP (from original rate to 1500 Hz) for all channels used\n",
    "        \n",
    "    Notes:\n",
    "    - The LFP is interpolated to 1500 Hz for all channels used.\n",
    "    - The SWR detector used is the Karlsson ripple detector from the ripple_detection module.\n",
    "    - The folders are titled by session and all files contain the name of the probe and the channel they originated from    \n",
    "    \"\"\"\n",
    "    session = cache.get_session_data(session_id)\n",
    "    print(\"Starting Session id \" + str(session_id))\n",
    "\n",
    "    # check if this session even has CA1 channels in it, if not skip this iteration and add the name to the list\n",
    "    sesh_has_ca1 = np.isin('CA1', list(session.channels.ecephys_structure_acronym.unique()))\n",
    "    if not sesh_has_ca1:\n",
    "        print(\"Session id \" + str(session_id) + \"Does not have CA1\")\n",
    "        return  # end the process\n",
    "    try:\n",
    "        #timing the probe run\n",
    "        start_time_sesh = time.time() \n",
    "        \n",
    "        # Create subfolder for session, will contain all csvs for events detected and .npy of ca1 channels and control channels \n",
    "        session_subfolder = \"swrs_session_\" + str(session_id)\n",
    "        session_subfolder = os.path.join(swr_output_dir_path, session_subfolder)\n",
    "        os.makedirs(session_subfolder, exist_ok=True) \n",
    "        \n",
    "        # get probes with CA1 recordings out of recording\n",
    "        probe_id_list = list(session.channels.probe_id.unique())\n",
    "        probes_of_interest = []\n",
    "\n",
    "        # find probes which contain channels from CA1\n",
    "        \n",
    "        for probe_id in probe_id_list:\n",
    "            has_ca1_and_exists = np.isin('CA1', list(session.channels[session.channels.probe_id == probe_id].ecephys_structure_acronym.unique()))\n",
    "            has_ca1_and_exists = has_ca1_and_exists & session.probes.has_lfp_data[probe_id]\n",
    "            if has_ca1_and_exists:\n",
    "                probes_of_interest.append(probe_id)\n",
    "        # create an arraey to be filled with channel ids fro ca1\n",
    "        ca1_chans_arr = np.array([], dtype=int)\n",
    "        used_channels_xarray_dict = {} # a list to put the lfp xarray objects into \n",
    "        \n",
    "        # create an array to be filled with outside of brain controls\n",
    "        outof_hp_chans_arr = np.array([], dtype=int)\n",
    "\n",
    "        # get lfp for each probe\n",
    "        for probe_id in probes_of_interest:    \n",
    "            \n",
    "            #timing the probe...\n",
    "            start_time_probe = time.time() \n",
    "            \n",
    "            # pull or laod the lfp for this probe\n",
    "            print(\"Probe id \" + str(probe_id))\n",
    "            lfp = session.get_lfp(probe_id)\n",
    "            sampling_rate_this_probe = session.probes.lfp_sampling_rate[probe_id]\n",
    "\n",
    "            print(\"Selecting CA1 channel...\")\n",
    "            # fetching channels in ca1 on this probe for this recording\n",
    "            ca1_chans = session.channels.probe_channel_number[(session.channels.probe_id==probe_id)&(session.channels.ecephys_structure_acronym=='CA1')]\n",
    "            ca1_idx = np.isin(lfp.channel.values, ca1_chans.index.values)\n",
    "            ca1_idx = lfp.channel.values[ca1_idx]\n",
    "            \n",
    "            # select ca1 channels \n",
    "            lfp_ca1  = lfp.sel(channel=ca1_idx)\n",
    "            lfp_ca1  = lfp_ca1.to_pandas()\n",
    "            lfp_ca1_chans  = lfp_ca1.columns\n",
    "            lfp_ca1  = lfp_ca1.to_numpy()\n",
    "\n",
    "            # check for nans indicating this is a bad probe\n",
    "            try:\n",
    "                if np.isnan(lfp_ca1).any():  # Check if there is any NaN in lfp_ca1\n",
    "                    del lfp_ca1, lfp  # Delete lfp_ca1 and lfp from memory\n",
    "                    raise ValueError(f\"During session {session_id} processing : Nan in lfp of Probe {probe_id}, probe skipped.\")  # Raise error\n",
    "            except ValueError as e:\n",
    "                logging.error(e)  # Log the error message, skip to next probe\n",
    "                del lfp  # Delete and lfp from memory to save RAM\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # get the timestamps for this lfp recording\n",
    "            #lfp_time_index = lfp_ca1.index.values \n",
    "            lfp_ca1, lfp_time_index = resample_signal(lfp_ca1, lfp.time.values, 1500.0) # note the original samplig rate is infered from the times object\n",
    "\n",
    "            #identify channel on probe with highest ripple power\n",
    "            #lfp_ca1_ripppleband = finitimpresp_filter_for_LFP(lfp_ca1, samplingfreq = sampling_rate_this_probe,  lowcut = 120, highcut = 250)\n",
    "            lfp_ca1_ripppleband = filter_ripple_band(lfp_ca1)\n",
    "            highest_rip_power = np.abs(signal.hilbert(lfp_ca1_ripppleband))**2\n",
    "            highest_rip_power = highest_rip_power.max(axis=0)\n",
    "            \n",
    "            # store channel identity in ca1_chans_arr and pull it for analysis of that channel\n",
    "            this_chan_id = int(lfp_ca1_chans[highest_rip_power.argmax()])\n",
    "            \n",
    "            # ideally we would store the channels for later use, but each lfp has it's own time and sampling rate that it goes through\n",
    "            #used_channels_xarray_dict[this_chan_id] = lfp.channel.values[this_chan_id]\n",
    "            \n",
    "            #GET LFP FOR ALL CHANNELS NEEDED, DELETE ARRAYS TO CLEAN UP MEMORY\n",
    "            lfp_time_index_og = lfp.time.values\n",
    "            peakripchan_lfp_ca1 = lfp_ca1[:,lfp_ca1_chans == this_chan_id]\n",
    "            \n",
    "            # as detailed in supplementry methods in Nitzan et al., (2022) on page 2 under Event Detection\n",
    "            \"\"\"\"\n",
    "            An additional ‘noise’ signal from a channel outside of the hippocampus was provided to exclude\n",
    "            simultaneously occurring high frequency events. \n",
    "            \"\"\"\n",
    "            # Bute we will take two control channels from the same probe rather than just one\n",
    "            idx = session.channels.probe_id == probe_id\n",
    "            organisedprobechans = session.channels[idx].sort_values(by='probe_vertical_position')\n",
    "            organisedprobechans = organisedprobechans[np.isin(organisedprobechans.index.values, lfp.channel.values) ]\n",
    "\n",
    "            # code for identifying first  and last ca1 channel, not used now but can be later to pick channels above or below ca1\n",
    "            # first_ca1 = organisedprobechans.probe_vertical_position[organisedprobechans.ecephys_structure_acronym == 'CA1'].tolist()[-1]\n",
    "            # last_ca1 = organisedprobechans.probe_vertical_position[organisedprobechans.ecephys_structure_acronym == 'CA1'].tolist()[0]\n",
    "            \n",
    "            not_a_ca1_chan = np.logical_not(np.isin(organisedprobechans.ecephys_structure_acronym,[ \"CA3\", \"CA2\", \"CA1\", \"HPF\", \"EC\", \"DG\"]))\n",
    "\n",
    "            # Find the indices of the blocks of False i.e. the channels that are ca1\n",
    "            take_two = np.random.choice(organisedprobechans.index[not_a_ca1_chan], 2, replace=False)\n",
    "            control_channels = []\n",
    "            \n",
    "            # movement control\n",
    "            for channel_outside_hp in take_two:\n",
    "                movement_control_channel = lfp.sel(channel=channel_outside_hp)\n",
    "                movement_control_channel = movement_control_channel.to_numpy()\n",
    "                # select ca1 channels\n",
    "                interp_func = interpolate.interp1d(lfp.time.values, movement_control_channel)\n",
    "                movement_control_channel = interp_func(lfp_time_index)\n",
    "                control_channels.append(movement_control_channel)\n",
    "            \n",
    "            # Saving LFP for all channels used\n",
    "            np.savez(os.path.join(session_subfolder, f\"probe_{probe_id}_channel_{this_chan_id}_lfp_ca1_peakripplepower.npz\"), lfp_ca1 = peakripchan_lfp_ca1)\n",
    "            np.savez(os.path.join(session_subfolder, f\"probe_{probe_id}_channel_{this_chan_id}_lfp_time_index_1500hz.npz\"), lfp_time_index = lfp_time_index)\n",
    "            for i in [0,1]:\n",
    "                channel_outside_hp = str(take_two[i])\n",
    "                np.savez(os.path.join(session_subfolder, f\"probe_{probe_id}_channel_{channel_outside_hp}_lfp_control_channel.npz\"), lfp_control_channel = control_channels[i])\n",
    "            \n",
    "            # clean memory...\n",
    "            del lfp\n",
    "            del lfp_ca1\n",
    "            \n",
    "            # COMPUTING LFP EVENTS\n",
    "            ca1_chans_arr = np.append(ca1_chans_arr, this_chan_id)\n",
    "            peakrippleband = lfp_ca1_ripppleband[:,highest_rip_power.argmax()]\n",
    "            # make fake speed variable, we can use this for now and fix it later              \n",
    "            dummy_speed = np.zeros_like(peakrippleband)\n",
    "            print(\"Detecting Putative Ripples\")\n",
    "            # we add a dimension to peakrippleband because the ripple detector needs it\n",
    "            Karlsson_ripple_times = ripple_detection.Karlsson_ripple_detector(\n",
    "                time = lfp_time_index, \n",
    "                zscore_threshold= ripple_band_threshold,\n",
    "                filtered_lfps = peakrippleband[:,None], \n",
    "                speed = dummy_speed, \n",
    "                sampling_frequency = 1500.0 # reinterploate to 1500 Hz, for edeno code\n",
    "            )\n",
    "            # there is no need for this criteria (Karlsson_ripple_times.duration>0.015)&(Karlsson_ripple_times.duration<0.25)\n",
    "            # because they are already filtered for minimum duration\n",
    "            # but we need to do it for maximum duration\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times[Karlsson_ripple_times.duration<0.25]\n",
    "            print(\"Done\")\n",
    "            # adds some stuff we want to the file\n",
    "            peakrippleband_power = np.abs(signal.hilbert(peakrippleband))**2\n",
    "            Karlsson_ripple_times['Peak_Amp_RipBandPower'], Karlsson_ripple_times['Peak_Amp_RipBandPower_zscore'],  Karlsson_ripple_times['Peak_time'] = peaks_in_events(events=Karlsson_ripple_times, \n",
    "                                                                                                                            time_values=lfp_time_index, \n",
    "                                                                                                                            signal_values=peakrippleband_power)\n",
    "            speed_cols = [col for col in Karlsson_ripple_times.columns if 'speed' in col]\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times.drop(columns=speed_cols)\n",
    "            csv_filename = f\"probe_{probe_id}_channel_{this_chan_id}_karlsson_detector_events.csv\"\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            Karlsson_ripple_times.to_csv(csv_path, index=True)\n",
    "            print(\"Writing to file.\")\n",
    "            print(\"Detecting gamma events.\")\n",
    "            # gamma power\n",
    "            # compute this later, I will have a seperate script called SWR filtering which will do this\n",
    "            #gamma_band = finitimpresp_filter_for_LFP(lfp_ca1[:,lfp_ca1_chans == this_chan_id], samplingfreq =  1500.0, lowcut = 20, highcut = 80)\n",
    "            #gamma_band = gamma_band_1500hzsig_filter(interpolated_1500hz_signal = peakripchan_lfp_ca1, filters_path = gamma_filters_paths)\n",
    "            gamma_band = np.convolve(peakripchan_lfp_ca1.reshape(-1), gamma_filter, mode='same') # reshape is needed to prevent \"to deep\" error\n",
    "            gamma_power = np.abs(signal.hilbert(gamma_band))**2\n",
    "            gamma_times = event_boundary_detector(time = lfp_time_index, threshold_sd = gamma_event_thresh, envelope=False, \n",
    "                                        minimum_duration = 0.015, maximum_duration = float('inf'),\n",
    "                                    five_to_fourty_band_power_df = gamma_power)\n",
    "            print(\"Done\")\n",
    "            csv_filename = f\"probe_{probe_id}_channel_{this_chan_id}_gamma_band_events.csv\"\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            gamma_times.to_csv(csv_path, index=True)\n",
    "            print(\"Writing to file.\")\n",
    "            print(\"Selecting reference channel for movement artifact filtering.\")\n",
    "            # movement artifact detector channel (SWR bandpass and power, then z-score)\n",
    "            # control_region_idx = session.channels.ecephys_structure_acronym.isna()\n",
    "            \n",
    "            # make theta band, not used now\n",
    "            #theta_band = np.convolve(peakripchan_lfp_ca1.reshape(-1), theta_filter, mode='same') # reshape is needed to prevent \"to deep\" error\n",
    "            \n",
    "            # writing bands to file\n",
    "            # Create the file path\n",
    "            lfpsignals_filename = f\"probe_{probe_id}_channel_{this_chan_id}_lfp_signals.npz\"\n",
    "            lfpfile_path = os.path.join(session_subfolder, lfpsignals_filename)\n",
    "\n",
    "            # Save the arrays to a compressed .npz file\n",
    "            np.savez_compressed(lfpfile_path, rawlfp=peakripchan_lfp_ca1)\n",
    "            # as detailed in supplementry methods in Nitzan et al., (2022) on page 2 under Event Detection\n",
    "            \"\"\"\"\n",
    "            An additional ‘noise’ signal from a channel outside of the hippocampus was provided to exclude\n",
    "            simultaneously occurring high frequency events. \n",
    "            \"\"\"\n",
    "                \n",
    "            # movement control\n",
    "            for i in [0,1]:\n",
    "                channel_outside_hp = take_two[i]\n",
    "                movement_control_channel = control_channels[i]\n",
    "                movement_control_channel = filter_ripple_band(movement_control_channel[:,None])\n",
    "\n",
    "                dummy_speed = np.zeros_like(movement_control_channel)\n",
    "\n",
    "                movement_controls = ripple_detection.Karlsson_ripple_detector(\n",
    "                    time = lfp_time_index.reshape(-1),\n",
    "                    filtered_lfps = movement_control_channel,\n",
    "                    speed = dummy_speed.reshape(-1),\n",
    "                    zscore_threshold= movement_artifact_ripple_band_threshold,\n",
    "                    sampling_frequency = 1500.0\n",
    "                )\n",
    "                print(\"Done\")\n",
    "                speed_cols = [col for col in movement_controls.columns if 'speed' in col]\n",
    "                movement_controls = movement_controls.drop(columns=speed_cols)\n",
    "                csv_filename = f\"probe_{probe_id}_channel_{channel_outside_hp}_movement_artifacts.csv\"\n",
    "                csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "                movement_controls.to_csv(csv_path, index=True)\n",
    "                print(\"Done Probe id \" + str(probe_id))\n",
    "            \n",
    "            #write these two to a numpy array finish loop\n",
    "            # write channel number and sessionid to a pandas array tracking where each channel came from\n",
    "            # so at the end of the loop you can identify which channel it called\n",
    "            \n",
    "            # At the end of the probe\n",
    "            end_time_probe = time.time()\n",
    "            elapsed_time_probe = end_time_probe - start_time_probe\n",
    "            hours_probe, rem_probe = divmod(elapsed_time_probe, 3600)\n",
    "            minutes_probe, seconds_probe = divmod(rem_probe, 60)\n",
    "            print(f\"Elapsed time for probe: {int(hours_probe)}:{int(minutes_probe)}:{seconds_probe:.2f}\")\n",
    "        \n",
    "        print(\"Done Session id \" + str(session_id))\n",
    "        #loop over global channels\n",
    "        # needs changing to all channels\n",
    "        # this is difficult because the samples need to be interpolated between and matched\n",
    "        \"\"\"\n",
    "        used_channels_xarray_dict[this_chan_id].append(lfp.channel.values[this_chan_id]).keys()\n",
    "        this_chan_id + used_channels_xarray_dict\n",
    "        \n",
    "        Karlsson_ripple_times = ripple_detection.Karlsson_ripple_detector(\n",
    "                time = lfp_time_index, \n",
    "                filtered_lfps = lfp.sel(channel=ca1_chans_arr), \n",
    "                speed = dummy_speed, \n",
    "                sampling_frequency = 1250.0)\n",
    "        \n",
    "        # save to \n",
    "        csv_filename = f\"global_session_{session_id}_karlsson_detector_events.csv\"\n",
    "        csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "        Karlsson_ripple_times.to_csv(csv_path, index=True)\n",
    "        \"\"\"\n",
    "        \n",
    "        # removing files\n",
    "        # replace path/to/directory with cache and session info for this loop\n",
    "        if (session_id not in dont_wipe_these_sessions):\n",
    "            remove_from_path_command = \"find \"+sdk_cache_dir+\"/session_\"+str(session_id)+\" -type f -name '*lfp*' -exec rm {} +\"\n",
    "            call_bash_function(remove_from_path_command)\n",
    "        \n",
    "        # At the end of the session\n",
    "        end_time_sesh = time.time()\n",
    "        elapsed_time_sesh = end_time_sesh - start_time_sesh\n",
    "        hours_sesh, rem_sesh = divmod(elapsed_time_sesh, 3600)\n",
    "        minutes_sesh, seconds_sesh = divmod(rem_sesh, 60)\n",
    "        print(f\"Elapsed time for session: {int(hours_sesh)}:{int(minutes_sesh)}:{seconds_sesh:.2f}\")\n",
    "    \n",
    "    except Exception:\n",
    "        \n",
    "        # if there is an error we want to know about it, but we dont want it to stop the loop\n",
    "        # so we will print the error to a file and continue\n",
    "        logging.error('Error in session: %s, probe id: %s', session_id, probe_id)\n",
    "        logging.error(traceback.format_exc())\n",
    "\n",
    "\n",
    "queue = Queue()\n",
    "listener = Process(target=listener_process, args=(queue,))\n",
    "listener.start()\n",
    "\n",
    "pool_size = 6\n",
    "    \n",
    "# already filterd for only brain observatory sessions\n",
    "session_list = sessions.index.values\n",
    "session_list = session_list # for testing\n",
    "\n",
    "# run the processes with the specified number of cores:\n",
    "with Pool(pool_size, initializer=init_pool, initargs=(queue,)) as p:\n",
    "    # There is no point in passing queue to this worker function\n",
    "    # since it is not used (Booboo):\n",
    "    p.map(process_session, session_list)\n",
    "\n",
    "queue.put('kill')\n",
    "listener.join()\n",
    "\n",
    "\n",
    "print(\"Done! Results in \" + swr_output_dir_path)\n",
    "\n",
    "#remove any empty directories\n",
    "# Bash command to find and remove all empty directories\n",
    "command = f\"find {swr_output_dir_path} -type d -empty -delete\"\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "\n",
    "end_time_outer = time.time()  # end timing\n",
    "elapsed_time_outer = end_time_outer - start_time_outer  # calculate elapsed time\n",
    "\n",
    "hours, rem = divmod(elapsed_time_outer, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "print(f\"Elapsed time for whole script: {int(hours)}:{int(minutes)}:{seconds:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
