{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ecephys_session_1044385384.nwb: 100%|██████████| 2.65G/2.65G [03:55<00:00, 11.3MMB/s]    \n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Session id 1044385384\n",
      "Starting Probe id 1044506933\n",
      "Probe id 1044506933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "probe_probeB_lfp.nwb: 100%|██████████| 4.58G/4.58G [04:12<00:00, 18.2MMB/s]    \n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting CA1 channel...\n",
      "lfp_ca1.shape : (12180057, 16)\n",
      "Detecting Putative Ripples\n",
      "Done\n",
      "Writing to file.\n",
      "Detecting gamma events.\n",
      "Done\n",
      "Writing to file.\n",
      "Selecting reference channel for movement artifact filtering.\n",
      "Done\n",
      "Done Probe id 1044506933\n",
      "Done\n",
      "Done Probe id 1044506933\n",
      "Elapsed time for probe: 0:14:52.94\n",
      "Starting Probe id 1044506935\n",
      "Probe id 1044506935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "probe_probeD_lfp.nwb: 100%|██████████| 4.53G/4.53G [03:15<00:00, 23.2MMB/s]    \n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting CA1 channel...\n",
      "lfp_ca1.shape : (12179998, 19)\n",
      "Detecting Putative Ripples\n",
      "Done\n",
      "Writing to file.\n",
      "Detecting gamma events.\n",
      "Done\n",
      "Writing to file.\n",
      "Selecting reference channel for movement artifact filtering.\n",
      "Done\n",
      "Done Probe id 1044506935\n",
      "Done\n",
      "Done Probe id 1044506935\n",
      "Elapsed time for probe: 0:15:8.95\n",
      "Starting Probe id 1044506936\n",
      "Probe id 1044506936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "probe_probeE_lfp.nwb: 100%|██████████| 4.78G/4.78G [03:13<00:00, 24.7MMB/s]    \n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting CA1 channel...\n",
      "lfp_ca1.shape : (12180017, 16)\n",
      "Detecting Putative Ripples\n",
      "Done\n",
      "Writing to file.\n",
      "Detecting gamma events.\n",
      "Done\n",
      "Writing to file.\n",
      "Selecting reference channel for movement artifact filtering.\n",
      "Done\n",
      "Done Probe id 1044506936\n",
      "Done\n",
      "Done Probe id 1044506936\n",
      "Elapsed time for probe: 0:14:6.98\n",
      "Starting Probe id 1044506937\n",
      "Probe id 1044506937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "probe_probeF_lfp.nwb: 100%|██████████| 4.73G/4.73G [03:21<00:00, 23.4MMB/s]    \n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting CA1 channel...\n",
      "lfp_ca1.shape : (12179970, 16)\n",
      "Detecting Putative Ripples\n",
      "Done\n",
      "Writing to file.\n",
      "Detecting gamma events.\n",
      "Done\n",
      "Writing to file.\n",
      "Selecting reference channel for movement artifact filtering.\n",
      "Done\n",
      "Done Probe id 1044506937\n",
      "Done\n",
      "Done Probe id 1044506937\n",
      "Elapsed time for probe: 0:14:56.62\n",
      "Done Session id 1044385384\n",
      "Bash function executed successfully.\n",
      "Output: \n",
      "Elapsed time for session: 0:59:6.91\n",
      "Done! Results in /space/scratch/allen_visbehave_swr_data/allen_visbehave_swr_murphylab2024\n",
      "Elapsed time for whole script: 1:3:17.78\n"
     ]
    }
   ],
   "source": [
    "# Allen Visual Behaviour SWR detection script\n",
    "\n",
    "# libraries\n",
    "import os\n",
    "import subprocess \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import ripple_detection\n",
    "from ripple_detection import filter_ripple_band\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorNeuropixelsProjectCache\n",
    "from scipy import interpolate\n",
    "from scipy.signal import firwin, lfilter\n",
    "from multiprocessing import Pool, Queue, Process\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "import yaml\n",
    "\n",
    "\n",
    "# start timing\n",
    "start_time_outer = time.time()  # start timing\n",
    "\n",
    "\n",
    "# Load the configuration from a YAML file\n",
    "with open('abi_visbehave_swr_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Get the values from the configuration\n",
    "pool_size = config['pool_size']\n",
    "sdk_cache_dir = config['sdk_cache_dir']\n",
    "output_dir = config['output_dir']\n",
    "swr_output_dir = config['swr_output_dir']\n",
    "run_name = config['run_name']\n",
    "select_these_sessions = config['select_these_sessions']\n",
    "only_brain_observatory_sessions = config['only_brain_observatory_sessions']\n",
    "dont_wipe_these_sessions = config['dont_wipe_these_sessions']\n",
    "gamma_event_thresh = config['gamma_event_thresh']\n",
    "gamma_filter_path = config['gamma_filter_path']\n",
    "theta_filter_path = config['theta_filter_path']\n",
    "ripple_band_threshold = config['ripple_band_threshold']\n",
    "movement_artifact_ripple_band_threshold = config['movement_artifact_ripple_band_threshold']\n",
    "save_lfp = config['save_lfp']\n",
    "\n",
    "\n",
    "# Functions\n",
    "def call_bash_function(bash_command = \"\"):\n",
    "    process = subprocess.Popen(bash_command, stdout=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        print(\"Bash function executed successfully.\")\n",
    "        print(\"Output:\", output.decode('utf-8'))\n",
    "    else:\n",
    "        print(\"Error:\", error.decode('utf-8'))\n",
    "\n",
    "def finitimpresp_filter_for_LFP(LFP_array, samplingfreq, lowcut = 1, highcut = 250,\n",
    "                    filter_order = 101):\n",
    "    \"\"\"\n",
    "    Filter the LFP array using a finite impulse response filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    LFP_array : np.array\n",
    "        The LFP array.\n",
    "    samplingfreq : float\n",
    "        The sampling frequency of the LFP array.\n",
    "    lowcut : float\n",
    "        The lowcut frequency.\n",
    "    highcut : float\n",
    "        The highcut frequency.\n",
    "    filter_order : int\n",
    "        The filter order.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The filtered LFP array.    \n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * samplingfreq\n",
    "\n",
    "    # Design the FIR bandpass filter using scipy.signal.firwin\n",
    "    fir_coeff = signal.firwin(filter_order, [lowcut / nyquist, highcut / nyquist],\n",
    "                              pass_zero=False, fs=samplingfreq)\n",
    "\n",
    "    # Apply the FIR filter to your signal_array\n",
    "    #filtered_signal = signal.convolve(LFP_array, fir_coeff, mode='same', method='auto')\n",
    "    filtered_signal = signal.lfilter(fir_coeff, 1.0, LFP_array, axis=0)\n",
    "    return(filtered_signal)\n",
    "\n",
    "\n",
    "def event_boundary_detector(time, five_to_fourty_band_power_df, envelope=True, minimum_duration = 0.02, maximum_duration = 0.4,\n",
    "                       threshold_sd=2.5, envelope_threshold_sd=1):\n",
    "    \"\"\"\n",
    "    For detecting gamma events.\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    five_to_fourty_band_power_df : np.array\n",
    "        The power of the signal in the 5-40 Hz band.\n",
    "    envelope : bool\n",
    "        Whether to use the envelope threshold.\n",
    "    minimum_duration : float\n",
    "        The minimum duration of an event.\n",
    "    maximum_duration : float\n",
    "        The maximum duration of an event.\n",
    "    threshold_sd : float\n",
    "        The threshold in standard deviations.\n",
    "    envelope_threshold_sd : float\n",
    "        The envelope threshold in standard deviations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # make df to fill\n",
    "    row_of_info =  {\n",
    "        'start_time': [],\n",
    "        'end_time': [],\n",
    "        'duration': [],\n",
    "        }\n",
    "\n",
    "    #sharp_wave_events_df = pd.DataFrame()\n",
    "    #scored_wave_power = stats.zscore(five_to_fourty_band_df)\n",
    "    \n",
    "    # compute our power threshold\n",
    "    #wave_band_sd_thresh = np.std(five_to_fourty_band_df)*threshold_sd\n",
    "    five_to_fourty_band_power_df = stats.zscore(five_to_fourty_band_power_df)\n",
    "    past_thresh = five_to_fourty_band_power_df>=threshold_sd\n",
    "    \n",
    "    # now we expand the sections that are past thresh up to the points that \n",
    "    # are past the envelope thresh, so not all sections above envelope thresh are true\n",
    "    # but those sections which alse contain a region past the detection threshold are included\n",
    "    def expand_sections(z_scores, boolean_array, thresh):\n",
    "        # Find indices where boolean_array is True\n",
    "        true_indices = np.where(boolean_array)[0]\n",
    "\n",
    "        # Initialize an array to keep track of expanded sections\n",
    "        expanded_sections = np.zeros_like(z_scores, dtype=bool)\n",
    "\n",
    "        # Iterate over true_indices and expand sections\n",
    "        for index in true_indices:\n",
    "            # Find the start and end of the current section\n",
    "            start = index\n",
    "            end = index\n",
    "\n",
    "            # Expand section to the left (while meeting conditions)\n",
    "            while start > 0 and z_scores[start - 1] >  thresh:\n",
    "                start -= 1\n",
    "\n",
    "            # Expand section to the right (while meeting conditions)\n",
    "            while end < len(z_scores) - 1 and z_scores[end + 1] >  thresh:\n",
    "                end += 1\n",
    "\n",
    "            # Check if the expanded section contains a point above envelope_threshold_sd in z_scores\n",
    "            if any(z_scores[start:end + 1] >  thresh):\n",
    "                expanded_sections[start:end + 1] = True\n",
    "\n",
    "        # Update the boolean_array based on expanded_sections\n",
    "        boolean_array = boolean_array | expanded_sections\n",
    "\n",
    "        return boolean_array\n",
    "    \n",
    "    if envelope==True:\n",
    "        past_thresh = expand_sections(z_scores=five_to_fourty_band_power_df,\n",
    "                                  boolean_array= past_thresh,\n",
    "                                  thresh = envelope_threshold_sd)\n",
    "    \n",
    "    \n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info['start_time'] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info['end_time'] = time[ends]\n",
    "    \n",
    "    row_of_info['duration'] = [row_of_info['end_time'][i]-row_of_info['start_time'][i] for i in range(0,len(row_of_info['start_time']))]\n",
    "    \n",
    "    #turn the dictionary into adataframe\n",
    "    sharp_wave_events_df = pd.DataFrame(row_of_info)\n",
    "    \n",
    "    # filter for the duration range we want\n",
    "    in_duration_range = (sharp_wave_events_df.duration>minimum_duration)&(sharp_wave_events_df.duration<maximum_duration)\n",
    "    sharp_wave_events_df = sharp_wave_events_df[in_duration_range]\n",
    "    \n",
    "    return sharp_wave_events_df\n",
    "\n",
    "def event_boundary_times(time, past_thresh):\n",
    "    \"\"\"\n",
    "    Finds the times of a vector of true statements and returns values from another\n",
    "    array representing the times\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    past_thresh : np.array\n",
    "        The boolean array of the signal.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.    \n",
    "    \"\"\"\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info['start_time'] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info['end_time'] = time[ends]\n",
    "    \n",
    "    row_of_info['duration'] = [row_of_info['end_time'][i]-row_of_info['start_time'][i] for i in range(0,len(row_of_info['start_time']))]\n",
    "    \n",
    "    #turn the dictionary into adataframe\n",
    "    events_df = pd.DataFrame(row_of_info)\n",
    "      \n",
    "    return events_df\n",
    "\n",
    "def peaks_time_of_events(events, time_values, signal_values):\n",
    "    \"\"\"\n",
    "    Computes the times when ripple power peaks in the events\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        The events dataframe.\n",
    "    time_values : np.array\n",
    "        The time values for the signal.\n",
    "    signal_values : np.array\n",
    "        The signal values for the signal.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The times of the peaks in the ripple power signal.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # looks for the peaks in the ripple power signal, value of zscored raw lfp peak and returns time of peak\n",
    "    signal_values_zscore = stats.zscore(signal_values)\n",
    "    peak_times = []\n",
    "    for start, end in zip(events['start_time'], events['end_time']):\n",
    "        window_idx = (time_values >= start) & (time_values <= end)\n",
    "        ripple_lfp_zscore_signal = signal_values_zscore[window_idx]\n",
    "        maxpoint = np.argmax(ripple_lfp_zscore_signal)\n",
    "        rippletimepoints = time_values[window_idx]\n",
    "        peak_times.append(rippletimepoints[maxpoint])\n",
    "    return np.array(peak_times)\n",
    "\n",
    "\n",
    "def resample_signal(signal, times, new_rate):\n",
    "    \"\"\"\n",
    "    Resample a 2D signal array to a new sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    signal (np.array): 2D array where each column is a source and each row is a time point.\n",
    "    times (np.array): 1D array of times corresponding to the rows of the signal array.\n",
    "    new_rate (float): The new sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "    new_signal (np.array): The resampled signal array.\n",
    "    new_times (np.array): The times corresponding to the rows of the new signal array.\n",
    "    \"\"\"\n",
    "    nsamples_new = int(len(times) * new_rate / (len(times) / times[-1]))\n",
    "    new_times = np.linspace(times[0], times[-1], nsamples_new)\n",
    "    new_signal = np.zeros((nsamples_new, signal.shape[1]))\n",
    "\n",
    "    for i in range(signal.shape[1]):\n",
    "        interp_func = interpolate.interp1d(times, signal[:, i], bounds_error=False, fill_value=\"extrapolate\")\n",
    "        new_signal[:, i] = interp_func(new_times)\n",
    "\n",
    "    return new_signal, new_times\n",
    "\n",
    "# Set up error logging \n",
    "MESSAGE = 25  # Define a custom logging level, between INFO (20) and WARNING (30)\n",
    "\n",
    "def listener_process(queue):\n",
    "    \"\"\"\n",
    "    This function listens for messages from the logging module and writes them to a log file.\n",
    "    It sets the logging level to MESSAGE so that only messages with level MESSAGE or higher are written to the log file.\n",
    "    This is a level we created to be between INFO and WARNING, so to see messages from this code and errors  but not other\n",
    "    messages that are mostly irrelevant and make the log file too large and uninterpretable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queue : multiprocessing.Queue\n",
    "        The queue to get messages from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    root = logging.getLogger()\n",
    "    h = logging.FileHandler(f'abi_detector_{swr_output_dir}_{run_name}_app.log', mode='w')\n",
    "    f = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "    h.setFormatter(f)\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "\n",
    "    while True:\n",
    "        message = queue.get()\n",
    "        if message == 'kill':\n",
    "            break\n",
    "        logger = logging.getLogger(message.name)\n",
    "        logger.handle(message)\n",
    "\n",
    "def init_pool(*args):\n",
    "    h = logging.handlers.QueueHandler(queue)\n",
    "    root = logging.getLogger()\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "\n",
    "\n",
    "# loading filters (craetes artifacts in first and last ~ 3.5 seconds of recordings, remember to clip these off)\n",
    "gamma_filter = np.load(gamma_filter_path)\n",
    "gamma_filter = gamma_filter['arr_0']\n",
    "\n",
    "theta_filter = np.load(theta_filter_path)\n",
    "theta_filter = theta_filter['arr_0']\n",
    "\n",
    "# Setting up the ABI Cache (where data is held, what is present or absent)\n",
    "manifest_path = os.path.join(sdk_cache_dir, \"manifest.json\")\n",
    "\n",
    "cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=sdk_cache_dir)\n",
    "\n",
    "\n",
    "# we start by calling and filtering our dataframe of the sessions we will be working with\n",
    "sessions = cache.get_ecephys_session_table()\n",
    "\n",
    "# Create main folder\n",
    "swr_output_dir_path = os.path.join(output_dir, swr_output_dir)\n",
    "os.makedirs(swr_output_dir_path, exist_ok=True)\n",
    "\n",
    "if save_lfp == True:\n",
    "    lfp_output_dir_path = os.path.join(output_dir, swr_output_dir+'_lfp_data')\n",
    "    os.makedirs(lfp_output_dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# this is the process we will run for each session over multiple cores\n",
    "def process_session(session_id):\n",
    "    \"\"\"\n",
    "    This function takes in a session id and loops through the probes in that session,\n",
    "    for each probe it finds the CA1 channel with the highest ripple power and uses that\n",
    "    channel to detect SWR events.  It also detects gamma events and movement artifacts\n",
    "    on two channels outside of the brain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session_id : int\n",
    "        The session id for the session to be processed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    but...\n",
    "    Saves the following files to the folder specified by swr_output_dir_path:\n",
    "        - a csv file for each probe with the SWR events detected on the CA1 channel with the highest ripple power\n",
    "        - a csv file for each probe with the gamma events detected on the CA1 channel with the highest ripple power\n",
    "        - a csv file for each probe with the movement artifacts detected on the two channels outside of the brain\n",
    "        - a numpy array of the LFP from the CA1 channels used for SWR detection\n",
    "        - a numpy array of the LFP from the two channels outside of the brain used for movement artifact detection\n",
    "        - a numpy array of the times of samples in the interpolated LFP (from original rate to 1500 Hz) for all channels used\n",
    "        \n",
    "    Notes:\n",
    "    - The LFP is interpolated to 1500 Hz for all channels used.\n",
    "    - The SWR detector used is the Karlsson ripple detector from the ripple_detection module.\n",
    "    - The folders are titled by session and all files contain the name of the probe and the channel they originated from    \n",
    "    \"\"\"\n",
    "    session = cache.get_ecephys_session(ecephys_session_id=session_id)\n",
    "    session.channels  = session.get_channels()\n",
    "    print(\"Starting Session id \" + str(session_id))\n",
    "    probe_id = 'Not selected yet'\n",
    "    # check if this session even has CA1 channels in it, if not skip this iteration and add the name to the list\n",
    "    sesh_has_ca1 = np.isin('CA1', list(session.channels.structure_acronym.unique()))\n",
    "    if not sesh_has_ca1:\n",
    "        print(\"Session id \" + str(session_id) + \"Does not have CA1\")\n",
    "        return  # end the process\n",
    "    try:\n",
    "        #timing the probe run\n",
    "        start_time_sesh = time.time() \n",
    "\n",
    "        # Create subfolder for session, will contain all csvs for events detected and .npy of ca1 channels and control channels \n",
    "        session_subfolder = \"swrs_session_\" + str(session_id)\n",
    "        session_subfolder = os.path.join(swr_output_dir_path, session_subfolder)\n",
    "        if os.path.exists(session_subfolder):\n",
    "            raise FileExistsError(f\"The directory {session_subfolder} already exists.\")\n",
    "        else:\n",
    "            os.makedirs(session_subfolder)\n",
    "\n",
    "        if save_lfp == True:\n",
    "            # Create subfolder for lfp data, will contain all npz files for lfp data\n",
    "            session_lfp_subfolder = \"lfp_session_\" + str(session_id)\n",
    "            session_lfp_subfolder = os.path.join(lfp_output_dir_path, session_lfp_subfolder)\n",
    "            os.makedirs(session_lfp_subfolder, exist_ok=True)\n",
    "\n",
    "        # get probes with CA1 recordings out of recording\n",
    "        probes_table_df = cache.get_probe_table()\n",
    "        valid_lfp = probes_table_df[probes_table_df['has_lfp_data']]\n",
    "\n",
    "        probe_id_list = list(valid_lfp[valid_lfp.ecephys_session_id==session_id].index)\n",
    "        del probes_table_df\n",
    "        del valid_lfp\n",
    "        probes_of_interest = []\n",
    "\n",
    "        # find probes which contain channels from CA1\n",
    "\n",
    "        for probe_id in probe_id_list:\n",
    "            has_ca1_and_exists = np.isin('CA1', list(session.channels[session.channels.probe_id == probe_id].structure_acronym.unique()))\n",
    "            if has_ca1_and_exists:\n",
    "                probes_of_interest.append(probe_id)\n",
    "        # create an arraey to be filled with channel ids fro ca1\n",
    "        ca1_chans_arr = np.array([], dtype=int)\n",
    "\n",
    "        # get lfp for each probe\n",
    "        for probe_id in probes_of_interest:    \n",
    "            \n",
    "            #timing the probe...\n",
    "            start_time_probe = time.time() \n",
    "            \n",
    "            # pull or laod the lfp for this probe\n",
    "            print(\"Probe id \" + str(probe_id))\n",
    "            lfp = session.get_lfp(probe_id)\n",
    "\n",
    "            print(\"Selecting CA1 channel...\")\n",
    "            # fetching channels in ca1 on this probe for this recording\n",
    "            ca1_chans = session.channels.probe_channel_number[(session.channels.probe_id==probe_id)&(session.channels.structure_acronym=='CA1')]\n",
    "            ca1_idx = np.isin(lfp.channel.values, ca1_chans.index.values)\n",
    "            ca1_idx = lfp.channel.values[ca1_idx]\n",
    "            \n",
    "            # select ca1 channels \n",
    "            lfp_ca1  = lfp.sel(channel=ca1_idx)\n",
    "            lfp_ca1  = lfp_ca1.to_pandas()\n",
    "            lfp_ca1_chans  = lfp_ca1.columns\n",
    "            lfp_ca1  = lfp_ca1.to_numpy()\n",
    "            \n",
    "            # check for nans indicating this is a bad probe\n",
    "            try:\n",
    "                if np.isnan(lfp_ca1).any():  # Check if there is any NaN in lfp_ca1\n",
    "                    del lfp_ca1, lfp  # Delete lfp_ca1 and lfp from memory\n",
    "                    raise ValueError(f\"During session {session_id} processing : Nan in lfp of Probe {probe_id}, probe skipped.\")  # Raise error\n",
    "            except ValueError as e:\n",
    "                logging.error(e)  # Log the error message, skip to next probe\n",
    "                del lfp  # Delete lfp from memory to save RAM\n",
    "                continue\n",
    "            \n",
    "            # get the timestamps for this lfp recording\n",
    "            #lfp_time_index = lfp_ca1.index.values \n",
    "            lfp_ca1, lfp_time_index = resample_signal(lfp_ca1, lfp.time.values, 1500.0) # note the original samplig rate is infered from the times object\n",
    "\n",
    "            #identify channel on probe with highest ripple power\n",
    "            #lfp_ca1_ripppleband = finitimpresp_filter_for_LFP(lfp_ca1, samplingfreq = sampling_rate_this_probe,  lowcut = 120, highcut = 250)\n",
    "            lfp_ca1_ripppleband = filter_ripple_band(lfp_ca1)\n",
    "            highest_rip_power = np.abs(signal.hilbert(lfp_ca1_ripppleband))**2\n",
    "            highest_rip_power = highest_rip_power.max(axis=0)\n",
    "            \n",
    "            # store channel identity in ca1_chans_arr and pull it for analysis of that channel\n",
    "            this_chan_id = int(lfp_ca1_chans[highest_rip_power.argmax()])\n",
    "            \n",
    "            # ideally we would store the channels for later use, but each lfp has it's own time and sampling rate that it goes through\n",
    "            #used_channels_xarray_dict[this_chan_id] = lfp.channel.values[this_chan_id]\n",
    "            \n",
    "            #GET LFP FOR ALL CHANNELS NEEDED, DELETE ARRAYS TO CLEAN UP MEMORY\n",
    "            peakripchan_lfp_ca1 = lfp_ca1[:,lfp_ca1_chans == this_chan_id]\n",
    "            \n",
    "            # as detailed in supplementry methods in Nitzan et al., (2022) on page 2 under Event Detection\n",
    "            # we will take two control channels from the same probe rather than just one\n",
    "            # Bute we will take two control channels from the same probe rather than just one\n",
    "            idx = session.channels.probe_id == probe_id\n",
    "            organisedprobechans = session.channels[idx].sort_values(by='probe_vertical_position')\n",
    "            organisedprobechans = organisedprobechans[np.isin(organisedprobechans.index.values, lfp.channel.values) ]\n",
    "\n",
    "            # code for identifying first  and last ca1 channel, not used now but can be later to pick channels above or below ca1\n",
    "            # first_ca1 = organisedprobechans.probe_vertical_position[organisedprobechans.ecephys_structure_acronym == 'CA1'].tolist()[-1]\n",
    "            # last_ca1 = organisedprobechans.probe_vertical_position[organisedprobechans.ecephys_structure_acronym == 'CA1'].tolist()[0]\n",
    "            \n",
    "            not_a_ca1_chan = np.logical_not(np.isin(organisedprobechans.structure_acronym,[ \"CA3\", \"CA2\", \"CA1\", \"HPF\", \"EC\", \"DG\"]))\n",
    "\n",
    "            # Find the indices of the blocks of False i.e. the channels that are ca1\n",
    "            take_two = np.random.choice(organisedprobechans.index[not_a_ca1_chan], 2, replace=False)\n",
    "            control_channels = []\n",
    "            \n",
    "            # movement control\n",
    "            for channel_outside_hp in take_two:\n",
    "                movement_control_channel = lfp.sel(channel=channel_outside_hp)\n",
    "                movement_control_channel = movement_control_channel.to_numpy()\n",
    "                # select ca1 channels\n",
    "                interp_func = interpolate.interp1d(lfp.time.values, movement_control_channel)\n",
    "                movement_control_channel = interp_func(lfp_time_index)\n",
    "                control_channels.append(movement_control_channel)\n",
    "            \n",
    "            # Saving LFP for all channels used\n",
    "            if save_lfp == True:\n",
    "                np.savez(os.path.join(session_lfp_subfolder, f\"probe_{probe_id}_channel_{this_chan_id}_lfp_ca1_peakripplepower.npz\"), lfp_ca1 = peakripchan_lfp_ca1)\n",
    "                np.savez(os.path.join(session_lfp_subfolder, f\"probe_{probe_id}_channel_{this_chan_id}_lfp_time_index_1500hz.npz\"), lfp_time_index = lfp_time_index)\n",
    "                for i in [0,1]:\n",
    "                    channel_outside_hp = str(take_two[i])\n",
    "                    np.savez(os.path.join(session_lfp_subfolder, f\"probe_{probe_id}_channel_{channel_outside_hp}_lfp_control_channel.npz\"), lfp_control_channel = control_channels[i])\n",
    "                \n",
    "            # clean memory...\n",
    "            del lfp\n",
    "            del lfp_ca1\n",
    "            \n",
    "            # COMPUTING LFP EVENTS\n",
    "            ca1_chans_arr = np.append(ca1_chans_arr, this_chan_id)\n",
    "            peakrippleband = lfp_ca1_ripppleband[:,highest_rip_power.argmax()]\n",
    "            # make fake speed variable, we can use this for now and fix it later              \n",
    "            dummy_speed = np.zeros_like(peakrippleband)\n",
    "            print(\"Detecting Putative Ripples\")\n",
    "            # we add a dimension to peakrippleband because the ripple detector needs it\n",
    "            Karlsson_ripple_times = ripple_detection.Karlsson_ripple_detector(\n",
    "                time = lfp_time_index, \n",
    "                zscore_threshold= ripple_band_threshold,\n",
    "                filtered_lfps = peakrippleband[:,None], \n",
    "                speed = dummy_speed, \n",
    "                sampling_frequency = 1500.0 # reinterploate to 1500 Hz, for edeno code\n",
    "            )\n",
    "            # there is no need for this criteria (Karlsson_ripple_times.duration>0.015)&(Karlsson_ripple_times.duration<0.25)\n",
    "            # because they are already filtered for minimum duration\n",
    "            # but we need to do it for maximum duration\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times[Karlsson_ripple_times.duration<0.25]\n",
    "            print(\"Done\")\n",
    "            # adds some stuff we want to the file\n",
    "            peakrippleband_power = np.abs(signal.hilbert(peakrippleband))**2\n",
    "            Karlsson_ripple_times['Peak_time'] = peaks_time_of_events(events=Karlsson_ripple_times, \n",
    "                                                                    time_values=lfp_time_index, \n",
    "                                                                signal_values=peakrippleband_power)\n",
    "            speed_cols = [col for col in Karlsson_ripple_times.columns if 'speed' in col]\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times.drop(columns=speed_cols)\n",
    "            csv_filename = f\"probe_{probe_id}_channel_{this_chan_id}_karlsson_detector_events.csv\"\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            Karlsson_ripple_times.to_csv(csv_path, index=True, compression='gzip')\n",
    "            print(\"Writing to file.\")\n",
    "            print(\"Detecting gamma events.\")\n",
    "            # gamma power\n",
    "            # compute this later, I will have a seperate script called SWR filtering which will do this\n",
    "            #gamma_band = finitimpresp_filter_for_LFP(lfp_ca1[:,lfp_ca1_chans == this_chan_id], samplingfreq =  1500.0, lowcut = 20, highcut = 80)\n",
    "            #gamma_band = gamma_band_1500hzsig_filter(interpolated_1500hz_signal = peakripchan_lfp_ca1, filters_path = gamma_filters_paths)\n",
    "            gamma_band = np.convolve(peakripchan_lfp_ca1.reshape(-1), gamma_filter, mode='same') # reshape is needed to prevent \"to deep\" error\n",
    "            gamma_power = np.abs(signal.hilbert(gamma_band))**2\n",
    "            gamma_times = event_boundary_detector(time = lfp_time_index, threshold_sd = gamma_event_thresh, envelope=False, \n",
    "                                        minimum_duration = 0.015, maximum_duration = float('inf'),\n",
    "                                    five_to_fourty_band_power_df = gamma_power)\n",
    "            print(\"Done\")\n",
    "            csv_filename = f\"probe_{probe_id}_channel_{this_chan_id}_gamma_band_events.csv\"\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            gamma_times.to_csv(csv_path, index=True, compression='gzip')\n",
    "            print(\"Writing to file.\")\n",
    "            print(\"Selecting reference channel for movement artifact filtering.\")\n",
    "            \n",
    "\n",
    "            # as detailed in supplementry methods in Nitzan et al., (2022) on page 2 under Event Detection\n",
    "            \"\"\"\"\n",
    "            An additional ‘noise’ signal from a channel outside of the hippocampus was provided to exclude\n",
    "            simultaneously occurring high frequency events. \n",
    "            \"\"\"\n",
    "                \n",
    "            # movement control\n",
    "            for i in [0,1]:\n",
    "                channel_outside_hp = take_two[i]\n",
    "                movement_control_channel = control_channels[i]\n",
    "                movement_control_channel = filter_ripple_band(movement_control_channel[:,None])\n",
    "\n",
    "                dummy_speed = np.zeros_like(movement_control_channel)\n",
    "\n",
    "                movement_controls = ripple_detection.Karlsson_ripple_detector(\n",
    "                    time = lfp_time_index.reshape(-1),\n",
    "                    filtered_lfps = movement_control_channel,\n",
    "                    speed = dummy_speed.reshape(-1),\n",
    "                    zscore_threshold= movement_artifact_ripple_band_threshold,\n",
    "                    sampling_frequency = 1500.0\n",
    "                )\n",
    "                print(\"Done\")\n",
    "                speed_cols = [col for col in movement_controls.columns if 'speed' in col]\n",
    "                movement_controls = movement_controls.drop(columns=speed_cols)\n",
    "                csv_filename = f\"probe_{probe_id}_channel_{channel_outside_hp}_movement_artifacts.csv\"\n",
    "                csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "                movement_controls.to_csv(csv_path, index=True, compression='gzip')\n",
    "                print(\"Done Probe id \" + str(probe_id))\n",
    "            \n",
    "            #write these two to a numpy array, finish loop\n",
    "            # write channel number and session id to a pandas array tracking where each channel came from\n",
    "            \n",
    "            # At the end of the probe\n",
    "            end_time_probe = time.time()\n",
    "            elapsed_time_probe = end_time_probe - start_time_probe\n",
    "            hours_probe, rem_probe = divmod(elapsed_time_probe, 3600)\n",
    "            minutes_probe, seconds_probe = divmod(rem_probe, 60)\n",
    "            print(f\"Elapsed time for probe: {int(hours_probe)}:{int(minutes_probe)}:{seconds_probe:.2f}\")\n",
    "\n",
    "        print(\"Done Session id \" + str(session_id))\n",
    "        #loop over global channels\n",
    "\n",
    "        # removing files\n",
    "        # replace path/to/directory with cache and session info for this loop\n",
    "        if (session_id not in dont_wipe_these_sessions):\n",
    "            path_to_directory = os.path.join(sdk_cache_dir, \"visual-behavior-neuropixels-0.5.0\",\"behavior_ecephys_sessions\", str(session_id))\n",
    "            remove_from_path_command = \"rm -r \" + path_to_directory\n",
    "            call_bash_function(remove_from_path_command)\n",
    "\n",
    "        # At the end of the session\n",
    "        end_time_sesh = time.time()\n",
    "        elapsed_time_sesh = end_time_sesh - start_time_sesh\n",
    "        hours_sesh, rem_sesh = divmod(elapsed_time_sesh, 3600)\n",
    "        minutes_sesh, seconds_sesh = divmod(rem_sesh, 60)\n",
    "        print(f\"Elapsed time for session: {int(hours_sesh)}:{int(minutes_sesh)}:{seconds_sesh:.2f}\")\n",
    "    \n",
    "    except Exception:\n",
    "        \n",
    "        # if there is an error we want to know about it, but we dont want it to stop the loop\n",
    "        # so we will print the error to a file and continue\n",
    "        logging.error('Error in session: %s, probe id: %s', session_id, probe_id)\n",
    "        logging.error(traceback.format_exc())\n",
    "\n",
    "# Initialize an empty list to store session_ids with 'CA1'\n",
    "sessions_with_CA1 = []\n",
    "\n",
    "# Iterate over each session_id in the DataFrame's index\n",
    "for session_id in sessions.index:\n",
    "    # Check if 'CA1' is in the structure_acronyms list for the current session_id\n",
    "    if 'CA1' in sessions.loc[session_id, 'structure_acronyms']:\n",
    "        # If 'CA1' is found, append the session_id to the list\n",
    "        sessions_with_CA1.append(session_id)\n",
    "\n",
    "queue = Queue()\n",
    "listener = Process(target=listener_process, args=(queue,))\n",
    "listener.start()\n",
    "\n",
    "pool_size = 6\n",
    "    \n",
    "# already filterd for only brain observatory sessions\n",
    "# Initialize an empty list to store session_ids with 'CA1'\n",
    "sessions_with_CA1 = []\n",
    "\n",
    "# Iterate over each session_id in the DataFrame's index\n",
    "for session_id in sessions.index:\n",
    "    # Check if 'CA1' is in the structure_acronyms list for the current session_id\n",
    "    if 'CA1' in sessions.loc[session_id, 'structure_acronyms']:\n",
    "        # If 'CA1' is found, append the session_id to the list\n",
    "        sessions_with_CA1.append(session_id)\n",
    "\n",
    "# run the processes with the specified number of cores:\n",
    "with Pool(pool_size, initializer=init_pool, initargs=(queue,)) as p:\n",
    "    # There is no point in passing queue to this worker function\n",
    "    # since it is not used (Booboo):\n",
    "    p.map(process_session, sessions_with_CA1[0:1])\n",
    "\n",
    "queue.put('kill')\n",
    "listener.join()\n",
    "\n",
    "print(\"Done! Results in \" + swr_output_dir_path)\n",
    "\n",
    "#remove any empty directories\n",
    "# Bash command to find and remove all empty directories\n",
    "command_empty_swrdir_clear = f\"find {swr_output_dir_path} -mindepth 1 -type d -empty -delete\"\n",
    "if save_lfp == True:\n",
    "    command_empty_lfpdir_clear = f\"find {lfp_output_dir_path} -mindepth 1 -type d -empty -delete\"\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command_empty_swrdir_clear, shell=True, check=True)\n",
    "# Run the command\n",
    "if save_lfp == True:\n",
    "    subprocess.run(command_empty_lfpdir_clear , shell=True, check=True)\n",
    "\n",
    "end_time_outer = time.time()  # end timing\n",
    "elapsed_time_outer = end_time_outer - start_time_outer  # calculate elapsed time\n",
    "\n",
    "hours, rem = divmod(elapsed_time_outer, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "print(f\"Elapsed time for whole script: {int(hours)}:{int(minutes)}:{seconds:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
