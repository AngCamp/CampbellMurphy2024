{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing new pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acampbell/miniconda3/envs/ONE_ibl_env/lib/python3.10/site-packages/neurodsp/__init__.py:5: FutureWarning: neurodsp has been renamed to ibldsp and the old name will be deprecated on 01-Sep-2024.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# pipeline that works on the 1500 Hz data from all the \n",
    "# probes\n",
    "\n",
    "# IBL SWR detector\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io, signal, stats\n",
    "from scipy.signal import lfilter\n",
    "import scipy.ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import ripple_detection\n",
    "from ripple_detection import filter_ripple_band\n",
    "import ripple_detection.simulate as ripsim  # for making our time vectors\n",
    "from tqdm import tqdm\n",
    "from iblatlas.atlas import AllenAtlas\n",
    "from iblatlas.regions import BrainRegions\n",
    "from one.api import ONE\n",
    "import spikeglx\n",
    "from brainbox.io.one import load_channel_locations\n",
    "from brainbox.io.spikeglx import Streamer\n",
    "from brainbox.io.one import SpikeSortingLoader\n",
    "from neurodsp.voltage import destripe_lfp\n",
    "from ibllib.plots import Density\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "import sys\n",
    "from multiprocessing import Pool, Process, Queue, Manager, set_start_method\n",
    "import yaml\n",
    "\n",
    "# Load the configuration from a YAML file\n",
    "with open(\"united_detector_config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Get the values from the configuration\n",
    "pool_size = config[\"pool_size\"]\n",
    "gamma_filters_path = config[\"gamma_filters_path\"]\n",
    "run_name = config[\"run_name\"]\n",
    "oneapi_cache_dir = config[\"oneapi_cache_dir\"]\n",
    "output_dir = config[\"output_dir\"]\n",
    "swr_output_dir = config[\"swr_output_dir\"]\n",
    "gamma_event_thresh = config[\"gamma_event_thresh\"]\n",
    "ripple_band_threshold = config[\"ripple_band_threshold\"]\n",
    "movement_artifact_ripple_band_threshold = config[\n",
    "    \"movement_artifact_ripple_band_threshold\"\n",
    "]\n",
    "dont_wipe_these_sessions = config[\"dont_wipe_these_sessions\"]\n",
    "session_npz_filepath = config[\"session_npz_filepath\"]\n",
    "save_lfp = config[\"save_lfp\"]\n",
    "\n",
    "\n",
    "# FUNCTIONS\n",
    "# subprocess is a default module\n",
    "def call_bash_function(bash_command=\"\"):\n",
    "    # example bash comand:\n",
    "    # bash_command = \"source /path/to/your/bash_script.sh && your_bash_function\"\n",
    "    process = subprocess.Popen(bash_command, stdout=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        print(\"Bash function executed successfully.\")\n",
    "        print(\"Output:\", output.decode(\"utf-8\"))\n",
    "    else:\n",
    "        print(\"Error:\", error.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# Assuming you have your signal_array, b, and a defined as before\n",
    "def finitimpresp_filter_for_LFP(\n",
    "    LFP_array, samplingfreq, lowcut=1, highcut=250, filter_order=101\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter the LFP array using a finite impulse response filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    LFP_array : np.array\n",
    "        The LFP array.\n",
    "    samplingfreq : float\n",
    "        The sampling frequency of the LFP array.\n",
    "    lowcut : float\n",
    "        The lowcut frequency.\n",
    "    highcut : float\n",
    "        The highcut frequency.\n",
    "    filter_order : int\n",
    "        The filter order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The filtered LFP array.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * samplingfreq\n",
    "\n",
    "    # Design the FIR bandpass filter using scipy.signal.firwin\n",
    "    fir_coeff = signal.firwin(\n",
    "        filter_order,\n",
    "        [lowcut / nyquist, highcut / nyquist],\n",
    "        pass_zero=False,\n",
    "        fs=samplingfreq,\n",
    "    )\n",
    "\n",
    "    # Apply the FIR filter to your signal_array\n",
    "    # filtered_signal = signal.convolve(LFP_array, fir_coeff, mode='same', method='auto')\n",
    "    filtered_signal = signal.lfilter(fir_coeff, 1.0, LFP_array, axis=0)\n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def event_boundary_detector(\n",
    "    time,\n",
    "    five_to_fourty_band_power_df,\n",
    "    envelope=True,\n",
    "    minimum_duration=0.02,\n",
    "    maximum_duration=0.4,\n",
    "    threshold_sd=2.5,\n",
    "    envelope_threshold_sd=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    For detecting gamma events.\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    five_to_fourty_band_power_df : np.array\n",
    "        The power of the signal in the 5-40 Hz band.\n",
    "    envelope : bool\n",
    "        Whether to use the envelope threshold.\n",
    "    minimum_duration : float\n",
    "        The minimum duration of an event.\n",
    "    maximum_duration : float\n",
    "        The maximum duration of an event.\n",
    "    threshold_sd : float\n",
    "        The threshold in standard deviations.\n",
    "    envelope_threshold_sd : float\n",
    "        The envelope threshold in standard deviations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.\n",
    "\n",
    "    \"\"\"\n",
    "    # make df to fill\n",
    "    row_of_info = {\n",
    "        \"start_time\": [],\n",
    "        \"end_time\": [],\n",
    "        \"duration\": [],\n",
    "    }\n",
    "\n",
    "    # sharp_wave_events_df = pd.DataFrame()\n",
    "    # scored_wave_power = stats.zscore(five_to_fourty_band_df)\n",
    "\n",
    "    # compute our power threshold\n",
    "    # wave_band_sd_thresh = np.std(five_to_fourty_band_df)*threshold_sd\n",
    "    five_to_fourty_band_power_df = stats.zscore(five_to_fourty_band_power_df)\n",
    "    past_thresh = five_to_fourty_band_power_df >= threshold_sd\n",
    "\n",
    "    # now we expand the sections that are past thresh up to the points that\n",
    "    # are past the envelope thresh, so not all sections above envelope thresh are true\n",
    "    # but those sections which alse contain a region past the detection threshold are included\n",
    "    def expand_sections(z_scores, boolean_array, thresh):\n",
    "        # Find indices where boolean_array is True\n",
    "        true_indices = np.where(boolean_array)[0]\n",
    "\n",
    "        # Initialize an array to keep track of expanded sections\n",
    "        expanded_sections = np.zeros_like(z_scores, dtype=bool)\n",
    "\n",
    "        # Iterate over true_indices and expand sections\n",
    "        for index in true_indices:\n",
    "            # Find the start and end of the current section\n",
    "            start = index\n",
    "            end = index\n",
    "\n",
    "            # Expand section to the left (while meeting conditions)\n",
    "            while start > 0 and z_scores[start - 1] > thresh:\n",
    "                start -= 1\n",
    "\n",
    "            # Expand section to the right (while meeting conditions)\n",
    "            while end < len(z_scores) - 1 and z_scores[end + 1] > thresh:\n",
    "                end += 1\n",
    "\n",
    "            # Check if the expanded section contains a point above envelope_threshold_sd in z_scores\n",
    "            if any(z_scores[start : end + 1] > thresh):\n",
    "                expanded_sections[start : end + 1] = True\n",
    "\n",
    "        # Update the boolean_array based on expanded_sections\n",
    "        boolean_array = boolean_array | expanded_sections\n",
    "\n",
    "        return boolean_array\n",
    "\n",
    "    if envelope == True:\n",
    "        past_thresh = expand_sections(\n",
    "            z_scores=five_to_fourty_band_power_df,\n",
    "            boolean_array=past_thresh,\n",
    "            thresh=envelope_threshold_sd,\n",
    "        )\n",
    "\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info[\"start_time\"] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info[\"end_time\"] = time[ends]\n",
    "\n",
    "    row_of_info[\"duration\"] = [\n",
    "        row_of_info[\"end_time\"][i] - row_of_info[\"start_time\"][i]\n",
    "        for i in range(0, len(row_of_info[\"start_time\"]))\n",
    "    ]\n",
    "\n",
    "    # turn the dictionary into adataframe\n",
    "    sharp_wave_events_df = pd.DataFrame(row_of_info)\n",
    "\n",
    "    # filter for the duration range we want\n",
    "    in_duration_range = (sharp_wave_events_df.duration > minimum_duration) & (\n",
    "        sharp_wave_events_df.duration < maximum_duration\n",
    "    )\n",
    "    sharp_wave_events_df = sharp_wave_events_df[in_duration_range]\n",
    "\n",
    "    return sharp_wave_events_df\n",
    "\n",
    "\n",
    "def event_boundary_times(time, past_thresh):\n",
    "    \"\"\"\n",
    "    Finds the times of a vector of true statements and returns values from another\n",
    "    array representing the times\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    past_thresh : np.array\n",
    "        The boolean array of the signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.\n",
    "    \"\"\"\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info[\"start_time\"] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info[\"end_time\"] = time[ends]\n",
    "\n",
    "    row_of_info[\"duration\"] = [\n",
    "        row_of_info[\"end_time\"][i] - row_of_info[\"start_time\"][i]\n",
    "        for i in range(0, len(row_of_info[\"start_time\"]))\n",
    "    ]\n",
    "\n",
    "    # turn the dictionary into adataframe\n",
    "    events_df = pd.DataFrame(row_of_info)\n",
    "\n",
    "    return events_df\n",
    "\n",
    "\n",
    "def peaks_time_of_events(events, time_values, signal_values):\n",
    "    \"\"\"\n",
    "    Computes the times when ripple power peaks in the events\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        The events dataframe.\n",
    "    time_values : np.array\n",
    "        The time values for the signal.\n",
    "    signal_values : np.array\n",
    "        The signal values for the signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The times of the peaks in the ripple power signal.\n",
    "    \"\"\"\n",
    "\n",
    "    # looks for the peaks in the ripple power signal, value of zscored raw lfp peak and returns time of peak\n",
    "    signal_values_zscore = stats.zscore(signal_values)\n",
    "    peak_times = []\n",
    "    for start, end in zip(events[\"start_time\"], events[\"end_time\"]):\n",
    "        window_idx = (time_values >= start) & (time_values <= end)\n",
    "        ripple_lfp_zscore_signal = signal_values_zscore[window_idx]\n",
    "        maxpoint = np.argmax(ripple_lfp_zscore_signal)\n",
    "        rippletimepoints = time_values[window_idx]\n",
    "        peak_times.append(rippletimepoints[maxpoint])\n",
    "    return np.array(peak_times)\n",
    "\n",
    "\n",
    "def resample_signal(signal, times, new_rate):\n",
    "    \"\"\"\n",
    "    Resample a 2D signal array to a new sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    signal (np.array): 2D array where each column is a source and each row is a time point.\n",
    "    times (np.array): 1D array of times corresponding to the rows of the signal array.\n",
    "    new_rate (float): The new sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "    new_signal (np.array): The resampled signal array.\n",
    "    new_times (np.array): The times corresponding to the rows of the new signal array.\n",
    "    \"\"\"\n",
    "    nsamples_new = int(len(times) * new_rate / (len(times) / times[-1]))\n",
    "    new_times = np.linspace(times[0], times[-1], nsamples_new)\n",
    "    new_signal = np.zeros((signal.shape[0], nsamples_new))\n",
    "\n",
    "    for i in range(signal.shape[0]):\n",
    "        interp_func = interpolate.interp1d(\n",
    "            times, signal[i, :], bounds_error=False, fill_value=\"extrapolate\"\n",
    "        )\n",
    "        new_signal[i, :] = interp_func(new_times)\n",
    "\n",
    "    return new_signal, new_times\n",
    "\n",
    "def listener_process(queue):\n",
    "    \"\"\"\n",
    "    This function listens for messages from the logging module and writes them to a log file.\n",
    "    It sets the logging level to MESSAGE so that only messages with level MESSAGE or higher are written to the log file.\n",
    "    This is a level we created to be between INFO and WARNING, so to see messages from this code and errors  but not other\n",
    "    messages that are mostly irrelevant and make the log file too large and uninterpretable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queue : multiprocessing.Queue\n",
    "        The queue to get messages from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    root = logging.getLogger()\n",
    "    h = logging.FileHandler(\n",
    "        f\"ibl_detector_{swr_output_dir}_{run_name}_app.log\", mode=\"w\"\n",
    "    )\n",
    "    f = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "    h.setFormatter(f)\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "\n",
    "    while True:\n",
    "        message = queue.get()\n",
    "        if message == \"kill\":\n",
    "            break\n",
    "        logger = logging.getLogger(message.name)\n",
    "        logger.handle(message)\n",
    "\n",
    "def init_pool(*args):\n",
    "    h = logging.handlers.QueueHandler(queue)\n",
    "    root = logging.getLogger()\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ibl_loader:\n",
    "    def __init__(self, session_id, br=None):\n",
    "        \"\"\"\n",
    "        Initialize the IBL loader with a session ID.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        session_id : str\n",
    "            The IBL session ID\n",
    "        br : BrainRegions, optional\n",
    "            An existing BrainRegions object to use. If None, brain regions\n",
    "            will need to be added externally.\n",
    "        \"\"\"\n",
    "        self.session_id = session_id\n",
    "        self.probe_id = \"Not Loaded Yet\"\n",
    "        self.one_exists = False\n",
    "        self.probelist = None\n",
    "        self.probenames = None\n",
    "        self.one = None\n",
    "        self.data_files = None\n",
    "        self.br = br\n",
    "        \n",
    "    def set_up(self):\n",
    "        \"\"\"\n",
    "        Sets up the ONE API connection.\n",
    "        \"\"\"\n",
    "        ONE.setup(base_url=\"https://openalyx.internationalbrainlab.org\", silent=True)\n",
    "        self.one = ONE(password=\"international\")\n",
    "        self.one_exists = True\n",
    "        return self\n",
    "        \n",
    "    def get_probe_ids_and_names(self):\n",
    "        \"\"\"\n",
    "        Gets the probe IDs and names for the session.        \n",
    "        \"\"\"\n",
    "        if not self.one_exists:\n",
    "            self.set_up()\n",
    "        self.probelist, self.probenames = self.one.eid2pid(self.session_id)\n",
    "        print(f\"Probe IDs: {self.probelist}, Probe names: {self.probenames}\")\n",
    "        return self.probelist, self.probenames\n",
    "    \n",
    "    def load_channels(self, probe_idx):\n",
    "        \"\"\"\n",
    "        Loads channel data for a specific probe.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        probe_idx : int\n",
    "            Index of the probe in the probelist.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (channels, probe_name, probe_id)\n",
    "        \"\"\"\n",
    "        if self.probelist is None:\n",
    "            self.get_probe_ids_and_names()\n",
    "            \n",
    "        probe_name = self.probenames[probe_idx]\n",
    "        probe_id = self.probelist[probe_idx]\n",
    "        print(f\"Loading channel data for probe: {probe_id}\")\n",
    "        \n",
    "        # Get channels data\n",
    "        collectionname = f\"alf/{probe_name}/pykilosort\"\n",
    "        channels = self.one.load_object(self.session_id, \"channels\", collection=collectionname)\n",
    "        \n",
    "        return channels, probe_name, probe_id\n",
    "    \n",
    "    def has_ca1_channels(self, channels):\n",
    "        \"\"\"\n",
    "        Checks if the channels include CA1.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        channels : object\n",
    "            Channel information object\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if CA1 channels exist, False otherwise\n",
    "        \"\"\"\n",
    "        if self.br is None:\n",
    "            raise ValueError(\"BrainRegions object (br) must be set to check for CA1 channels\")\n",
    "            \n",
    "        channels.allen2017_25um_acronym = self.br.id2acronym(\n",
    "            channels[\"brainLocationIds_ccf_2017\"]\n",
    "        )\n",
    "        \n",
    "        regions_on_probe = np.unique(channels.allen2017_25um_acronym)\n",
    "        has_ca1 = \"CA1\" in regions_on_probe\n",
    "        \n",
    "        if not has_ca1:\n",
    "            print(f\"No CA1 channels on probe, skipping...\")\n",
    "            \n",
    "        return has_ca1\n",
    "    \n",
    "    def load_bin_file(self, probe_name):\n",
    "        \"\"\"\n",
    "        Loads the binary file for a probe.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        probe_name : str\n",
    "            Name of the probe\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pathlib.Path or None\n",
    "            Path to the binary file\n",
    "        \"\"\"\n",
    "        # Find the relevant datasets and download them\n",
    "        dsets = self.one.list_datasets(\n",
    "            self.session_id, collection=f\"raw_ephys_data/{probe_name}\", filename=\"*.lf.*\"\n",
    "        )\n",
    "        print(f\"Found {len(dsets)} datasets\")\n",
    "        \n",
    "        self.data_files, _ = self.one.load_datasets(self.session_id, dsets, download_only=False)\n",
    "        bin_file = next((df for df in self.data_files if df.suffix == \".cbin\"), None)\n",
    "        \n",
    "        if bin_file is None:\n",
    "            print(f\"No .cbin file found for probe {probe_name}, skipping...\")\n",
    "            \n",
    "        return bin_file\n",
    "    \n",
    "    def create_time_index(self, sr, probe_id):\n",
    "        \"\"\"\n",
    "        Creates a time index for the LFP data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sr : spikeglx.Reader\n",
    "            SpikeGLX reader object\n",
    "        probe_id : str\n",
    "            Probe ID\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Time index for the LFP data\n",
    "        \"\"\"\n",
    "        # Make time index\n",
    "        start_time = time.time()\n",
    "        ssl = SpikeSortingLoader(pid=probe_id, one=self.one)\n",
    "        t0 = ssl.samples2times(0, direction=\"forward\")\n",
    "        dt = (ssl.samples2times(1, direction=\"forward\") - t0) * 12\n",
    "        lfp_time_index_og = np.arange(0, sr.shape[0]) * dt + t0\n",
    "        del ssl\n",
    "        print(f\"Time index created, time elapsed: {time.time() - start_time}\")\n",
    "        \n",
    "        return lfp_time_index_og\n",
    "    \n",
    "    def extract_raw_data(self, sr):\n",
    "        \"\"\"\n",
    "        Extracts raw data from the SpikeGLX reader.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sr : spikeglx.Reader\n",
    "            SpikeGLX reader object\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (raw_data, sampling_rate)\n",
    "        \"\"\"\n",
    "        # Extract raw data\n",
    "        start_time = time.time()\n",
    "        raw = sr[:, : -sr.nsync].T\n",
    "        fs_from_sr = sr.fs\n",
    "        print(f\"Raw data extracted, time elapsed: {time.time() - start_time}\")\n",
    "        \n",
    "        return raw, fs_from_sr\n",
    "    \n",
    "    def destripe_data(self, raw, fs):\n",
    "        \"\"\"\n",
    "        Applies destriping to the raw data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        raw : numpy.ndarray\n",
    "            Raw data\n",
    "        fs : float\n",
    "            Sampling rate\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Destriped data\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        destriped = destripe_lfp(raw, fs=fs)\n",
    "        print(f\"Destriped shape: {destriped.shape}\")\n",
    "        print(f\"Destriping done, time elapsed: {time.time() - start_time}\")\n",
    "        \n",
    "        return destriped\n",
    "    \n",
    "    def get_ca1_channels(self, channels, destriped):\n",
    "        \"\"\"\n",
    "        Gets the CA1 channels from the destriped data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        channels : object\n",
    "            Channel information object\n",
    "        destriped : numpy.ndarray\n",
    "            Destriped data\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (ca1_lfp, ca1_channel_indices)\n",
    "        \"\"\"\n",
    "        ca1_chans = channels.rawInd[channels.allen2017_25um_acronym == \"CA1\"]\n",
    "        lfp_ca1 = destriped[ca1_chans, :]\n",
    "        \n",
    "        return lfp_ca1, ca1_chans\n",
    "    \n",
    "    def get_non_hippocampal_channels(self, channels, destriped):\n",
    "        \"\"\"\n",
    "        Gets two non-hippocampal channels for artifact detection.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        channels : object\n",
    "            Channel information object\n",
    "        destriped : numpy.ndarray\n",
    "            Destriped data\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (non_hippocampal_lfp_list, non_hippocampal_channel_indices)\n",
    "        \"\"\"\n",
    "        # Find channels outside the hippocampal formation\n",
    "        not_a_hp_chan = np.logical_not(\n",
    "            np.isin(\n",
    "                channels.allen2017_25um_acronym,\n",
    "                [\"CA3\", \"CA2\", \"CA1\", \"HPF\", \"EC\", \"DG\"],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Select two random non-hippocampal channels\n",
    "        control_channels = np.random.choice(\n",
    "            channels.rawInd[not_a_hp_chan], 2, replace=False\n",
    "        )\n",
    "        \n",
    "        # Extract data for these channels\n",
    "        control_data = []\n",
    "        for channel_idx in control_channels:\n",
    "            control_data.append(destriped[channel_idx, :])\n",
    "            \n",
    "        return control_data, control_channels\n",
    "    \n",
    "    def resample_signal(self, lfp_data, time_index_og, target_fs=1500.0):\n",
    "        \"\"\"\n",
    "        Resamples the signal to a target frequency.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lfp_data : numpy.ndarray\n",
    "            LFP data\n",
    "        time_index_og : numpy.ndarray\n",
    "            Original time index\n",
    "        target_fs : float, optional\n",
    "            Target sampling frequency\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (resampled_data, new_time_index)\n",
    "        \"\"\"\n",
    "        # Create new time index at target sampling rate\n",
    "        t_start = time_index_og[0]\n",
    "        t_end = time_index_og[-1]\n",
    "        dt_new = 1.0 / target_fs\n",
    "        n_samples = int(np.ceil((t_end - t_start) / dt_new))\n",
    "        new_time_index = t_start + np.arange(n_samples) * dt_new\n",
    "        \n",
    "        # Check if lfp_data is 1D or 2D\n",
    "        if lfp_data.ndim == 1:\n",
    "            # For 1D array\n",
    "            interp_func = interpolate.interp1d(\n",
    "                time_index_og,\n",
    "                lfp_data,\n",
    "                bounds_error=False,\n",
    "                fill_value=\"extrapolate\",\n",
    "            )\n",
    "            resampled = interp_func(new_time_index)\n",
    "            resampled = resampled.T  # Transpose for standard orientation\n",
    "        else:\n",
    "            # For 2D array (multiple channels)\n",
    "            resampled = np.zeros((lfp_data.shape[0], len(new_time_index)))\n",
    "            for i in range(lfp_data.shape[0]):\n",
    "                interp_func = interpolate.interp1d(\n",
    "                    time_index_og,\n",
    "                    lfp_data[i, :],\n",
    "                    bounds_error=False,\n",
    "                    fill_value=\"extrapolate\",\n",
    "                )\n",
    "                resampled[i, :] = interp_func(new_time_index)\n",
    "            \n",
    "            resampled = resampled.T  # Transpose for standard orientation\n",
    "        \n",
    "        return resampled, new_time_index\n",
    "    \n",
    "    def find_peak_ripple_channel(self, lfp_ca1, ca1_chans, filter_ripple_band_func):\n",
    "        \"\"\"\n",
    "        Finds the CA1 channel with the highest ripple power.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lfp_ca1 : numpy.ndarray\n",
    "            LFP data from CA1 channels\n",
    "        ca1_chans : numpy.ndarray\n",
    "            Indices of CA1 channels\n",
    "        filter_ripple_band_func : function\n",
    "            Function to filter signal to ripple band\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (peak_channel_index, peak_channel_id, peak_channel_raw_lfp)\n",
    "        \"\"\"\n",
    "        lfp_ca1_rippleband = filter_ripple_band_func(lfp_ca1)\n",
    "        highest_rip_power = np.abs(signal.hilbert(lfp_ca1_rippleband)) ** 2\n",
    "        highest_rip_power = highest_rip_power.max(axis=0)\n",
    "        \n",
    "        peak_channel_idx = highest_rip_power.argmax()\n",
    "        peak_channel_id = ca1_chans[peak_channel_idx]\n",
    "        peak_channel_raw_lfp = lfp_ca1[:, peak_channel_idx]\n",
    "        \n",
    "        return peak_channel_idx, peak_channel_id, peak_channel_raw_lfp\n",
    "    \n",
    "    def process_probe(self, probe_idx, filter_ripple_band_func=None):\n",
    "        \"\"\"\n",
    "        Processes a single probe completely.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        probe_idx : int\n",
    "            Index of the probe in the probelist\n",
    "        filter_ripple_band_func : function, optional\n",
    "            Function to filter for ripple band\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with processing results\n",
    "        \"\"\"\n",
    "        if self.probelist is None:\n",
    "            self.get_probe_ids_and_names()\n",
    "            \n",
    "        # Step 1: Load channels\n",
    "        channels, probe_name, probe_id = self.load_channels(probe_idx)\n",
    "        \n",
    "        # Step 2: Check for CA1 channels\n",
    "        if self.br is not None and not self.has_ca1_channels(channels):\n",
    "            return None\n",
    "            \n",
    "        # Step 3: Load bin file\n",
    "        bin_file = self.load_bin_file(probe_name)\n",
    "        if bin_file is None:\n",
    "            return None\n",
    "            \n",
    "        # Step 4: Read the data\n",
    "        print(f\"Reading LFP data for probe {probe_id}...\")\n",
    "        sr = spikeglx.Reader(bin_file)\n",
    "        \n",
    "        # Step 5: Create time index\n",
    "        lfp_time_index_og = self.create_time_index(sr, probe_id)\n",
    "        \n",
    "        # Step 6: Extract raw data\n",
    "        raw, fs_from_sr = self.extract_raw_data(sr)\n",
    "        del sr  # Free memory\n",
    "        \n",
    "        # Step 7: Destripe data\n",
    "        destriped = self.destripe_data(raw, fs_from_sr)\n",
    "        del raw  # Free memory\n",
    "        \n",
    "        # Step 8: Get CA1 channels\n",
    "        lfp_ca1, ca1_chans = self.get_ca1_channels(channels, destriped)\n",
    "        \n",
    "        # Step 9: Get non-hippocampal control channels for artifact detection\n",
    "        print(f\"Getting control channels for artifact detection...\")\n",
    "        control_data, control_channels = self.get_non_hippocampal_channels(channels, destriped)\n",
    "        \n",
    "        # Step 10: Resample CA1 channels to 1500 Hz\n",
    "        print(f\"Resampling CA1 channels to 1.5kHz...\")\n",
    "        lfp_ca1, lfp_time_index = self.resample_signal(lfp_ca1, lfp_time_index_og, 1500.0)\n",
    "        \n",
    "        # Step 11: Resample control channels\n",
    "        outof_hp_chans_lfp = []\n",
    "        for channel_data in control_data:\n",
    "            # Reshape to 2D array with shape (1, n_samples)\n",
    "            channel_data = channel_data.reshape(1, -1)\n",
    "            \n",
    "            # Resample\n",
    "            lfp_control, _ = self.resample_signal(channel_data, lfp_time_index_og, 1500.0)\n",
    "            \n",
    "            # Append to list, ensuring correct shape\n",
    "            outof_hp_chans_lfp.append(lfp_control[:, None])\n",
    "            del lfp_control  # Free memory\n",
    "        \n",
    "        del destriped  # Free memory for large array\n",
    "        del control_data  # Free memory\n",
    "        \n",
    "        # Step 12: Find channel with highest ripple power if function provided\n",
    "        if filter_ripple_band_func is not None:\n",
    "            peak_idx, peak_id, peak_lfp = self.find_peak_ripple_channel(\n",
    "                lfp_ca1, ca1_chans, filter_ripple_band_func\n",
    "            )\n",
    "            \n",
    "            # Create a channel ID string for naming files\n",
    "            this_chan_id = f\"channelsrawInd_{peak_id}\"\n",
    "        else:\n",
    "            peak_idx = None\n",
    "            peak_id = None\n",
    "            peak_lfp = None\n",
    "            this_chan_id = None\n",
    "            \n",
    "        # Collect results\n",
    "        results = {\n",
    "            'probe_id': probe_id,\n",
    "            'probe_name': probe_name,\n",
    "            'lfp_ca1': lfp_ca1,\n",
    "            'lfp_time_index': lfp_time_index,\n",
    "            'channels': channels,\n",
    "            'ca1_chans': ca1_chans,\n",
    "            'control_lfps': outof_hp_chans_lfp,\n",
    "            'control_channels': control_channels,\n",
    "            'peak_ripple_chan_idx': peak_idx,\n",
    "            'peak_ripple_chan_id': peak_id,\n",
    "            'peak_ripple_chan_raw_lfp': peak_lfp,\n",
    "            'chan_id_string': this_chan_id\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Cleans up resources to free memory.\n",
    "        \"\"\"\n",
    "        self.data_files = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_session(session_id):\n",
    "    \"\"\"\n",
    "    This function takes in a session_id (eid in the IBL) and loops through the probes in that session,\n",
    "    for each probe it finds the CA1 channel with the highest ripple power and uses that\n",
    "    channel to detect SWR events.  It also detects gamma events and movement artifacts\n",
    "    on two channels outside of the brain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session_id : int\n",
    "        The session id for the session to be processed.\n",
    "    queue : multiprocessing.Queue\n",
    "        The queue to send messages to the listener process for recording errors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    but...\n",
    "    Saves the following files to the folder specified by swr_output_dir_path.\n",
    "    \n",
    "    Notes:\n",
    "    - The LFP is interpolated to 1500 Hz for all channels used.\n",
    "    - The SWR detector used is the Karlsson ripple detector from the ripple_detection module.\n",
    "    - The folders are titled by session and all files contain the name of the probe and the channel they originated from\n",
    "    \"\"\"\n",
    "    \n",
    "    process_stage = \"Starting the process\"  # for debugging\n",
    "    probe_id = \"Not Loaded Yet\"\n",
    "    one_exists = False\n",
    "    \n",
    "    # Create session subfolder\n",
    "    session_subfolder = \"swrs_session_\" + str(session_id)\n",
    "    session_subfolder = os.path.join(swr_output_dir_path, session_subfolder)\n",
    "    \n",
    "    try:\n",
    "        # Set up brain atlas\n",
    "        process_stage = \"Setting up brain atlas\"\n",
    "        ba = AllenAtlas()\n",
    "        br = BrainRegions()\n",
    "        \n",
    "        process_stage = \"Session loaded, checking if directory exists\"\n",
    "        # Check if directory already exists\n",
    "        if os.path.exists(session_subfolder):\n",
    "            raise FileExistsError(f\"The directory {session_subfolder} already exists.\")\n",
    "        else:\n",
    "            os.makedirs(session_subfolder)\n",
    "            \n",
    "        if save_lfp == True:\n",
    "            # Create subfolder for lfp data\n",
    "            session_lfp_subfolder = \"lfp_session_\" + str(session_id)\n",
    "            session_lfp_subfolder = os.path.join(lfp_output_dir_path, session_lfp_subfolder)\n",
    "            os.makedirs(session_lfp_subfolder, exist_ok=True)\n",
    "        \n",
    "        # Initialize and set up the IBL loader\n",
    "        process_stage = \"Setting up IBL loader\"\n",
    "        loader = ibl_loader(session_id, br=br)\n",
    "        loader.set_up()\n",
    "        one_exists = True  # Mark that we have a connection for error handling\n",
    "        \n",
    "        # Get probe IDs and names\n",
    "        process_stage = \"Getting probe IDs and names\"\n",
    "        probelist, probenames = loader.get_probe_ids_and_names()\n",
    "        \n",
    "        process_stage = \"Running through the probes in the session\"\n",
    "        # Process each probe\n",
    "        for this_probe in range(len(probelist)):\n",
    "            probe_name = probenames[this_probe]\n",
    "            probe_id = probelist[this_probe]\n",
    "            print(f\"Processing probe: {probe_id}\")\n",
    "            \n",
    "            # Create a function for ripple band filtering\n",
    "            def filter_ripple_band(signal_data):\n",
    "                # Your existing filter_ripple_band implementation\n",
    "                return filter_ripple_band(signal_data)\n",
    "            \n",
    "            # Process the probe and get results\n",
    "            process_stage = f\"Processing probe {probe_name} with id {probe_id}\"\n",
    "            results = loader.process_probe(this_probe, filter_ripple_band)\n",
    "            \n",
    "            # Skip if no results (no CA1 channels or no bin file)\n",
    "            if results is None:\n",
    "                print(f\"No results for probe {probe_id}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Extract results\n",
    "            lfp_ca1 = results['lfp_ca1']\n",
    "            lfp_time_index = results['lfp_time_index']\n",
    "            ca1_chans = results['ca1_chans']\n",
    "            outof_hp_chans_lfp = results['control_lfps']\n",
    "            take_two = results['control_channels']\n",
    "            \n",
    "            # From here, continue with your existing processing steps:\n",
    "            # - Filter for ripple band\n",
    "            lfp_ca1_ripppleband = filter_ripple_band(lfp_ca1)\n",
    "            highest_rip_power = np.abs(signal.hilbert(lfp_ca1_ripppleband)) ** 2\n",
    "            highest_rip_power = highest_rip_power.max(axis=0)\n",
    "            \n",
    "            # Get channel ID and peak ripple data\n",
    "            this_chan_id = \"channelsrawInd_\" + str(ca1_chans[highest_rip_power.argmax()])\n",
    "            peakrippleband = lfp_ca1_ripppleband[:, highest_rip_power.argmax()]\n",
    "            peakripple_chan_raw_lfp = lfp_ca1[:, highest_rip_power.argmax()]\n",
    "            \n",
    "            # Filter to gamma band\n",
    "            gamma_band_ca1 = np.convolve(\n",
    "                peakripple_chan_raw_lfp.reshape(-1), gamma_filter, mode=\"same\"\n",
    "            )\n",
    "\n",
    "            # write our lfp to file\n",
    "            np.savez(\n",
    "                os.path.join(\n",
    "                    session_lfp_subfolder,\n",
    "                    f\"probe_{probe_id}_channel_{this_chan_id}_lfp_ca1_peakripplepower.npz\",\n",
    "                ),\n",
    "                lfp_ca1=peakripple_chan_raw_lfp,\n",
    "            )\n",
    "            np.savez(\n",
    "                os.path.join(\n",
    "                    session_lfp_subfolder,\n",
    "                    f\"probe_{probe_id}_channel_{this_chan_id}_lfp_time_index_1500hz.npz\",\n",
    "                ),\n",
    "                lfp_time_index=lfp_time_index,\n",
    "            )\n",
    "            for i in range(2):\n",
    "                channel_outside_hp = take_two[i]\n",
    "                channel_outside_hp = \"channelsrawInd_\" + str(channel_outside_hp)\n",
    "                np.savez(\n",
    "                    os.path.join(\n",
    "                        session_lfp_subfolder,\n",
    "                        f\"probe_{probe_id}_channel_{channel_outside_hp}_lfp_control_channel.npz\",\n",
    "                    ),\n",
    "                    lfp_control_channel=outof_hp_chans_lfp[i],\n",
    "                )\n",
    "\n",
    "            del lfp_ca1  # clear up some memory\n",
    "\n",
    "            # create a dummy speed vector\n",
    "            dummy_speed = np.zeros_like(peakrippleband)\n",
    "            print(\"Detecting Putative Ripples\")\n",
    "            # we add a dimension to peakrippleband because the ripple detector needs it\n",
    "            process_stage = (\n",
    "                f\"Detecting Putative Ripples on probe {probe_name} with id {probe_id}\"\n",
    "            )\n",
    "            Karlsson_ripple_times = ripple_detection.Karlsson_ripple_detector(\n",
    "                time=lfp_time_index,\n",
    "                zscore_threshold=ripple_band_threshold,\n",
    "                filtered_lfps=peakrippleband[:, None],\n",
    "                speed=dummy_speed,\n",
    "                sampling_frequency=1500.0,\n",
    "            )\n",
    "\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times[\n",
    "                Karlsson_ripple_times.duration < 0.25\n",
    "            ]\n",
    "            print(\"Done\")\n",
    "            # adds some stuff we want to the file\n",
    "\n",
    "            # ripple band power\n",
    "            peakrippleband_power = np.abs(signal.hilbert(peakrippleband)) ** 2\n",
    "            Karlsson_ripple_times[\"Peak_time\"] = peaks_time_of_events(\n",
    "                events=Karlsson_ripple_times,\n",
    "                time_values=lfp_time_index,\n",
    "                signal_values=peakrippleband_power,\n",
    "            )\n",
    "            speed_cols = [\n",
    "                col for col in Karlsson_ripple_times.columns if \"speed\" in col\n",
    "            ]\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times.drop(columns=speed_cols)\n",
    "            csv_filename = (\n",
    "                f\"probe_{probe_id}_channel_{this_chan_id}_karlsson_detector_events.csv\"\n",
    "            )\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            Karlsson_ripple_times.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "            print(\"Writing to file.\")\n",
    "            print(\"Detecting gamma events.\")\n",
    "\n",
    "            # compute this later, I will have a seperate script called SWR filtering which will do this\n",
    "            process_stage = (\n",
    "                f\"Detecting Gamma Events on probe {probe_name} with id {probe_id}\"\n",
    "            )\n",
    "            gamma_power = np.abs(signal.hilbert(gamma_band_ca1)) ** 2\n",
    "            gamma_times = event_boundary_detector(\n",
    "                time=lfp_time_index,\n",
    "                threshold_sd=gamma_event_thresh,\n",
    "                envelope=False,\n",
    "                minimum_duration=0.015,\n",
    "                maximum_duration=float(\"inf\"),\n",
    "                five_to_fourty_band_power_df=gamma_power,\n",
    "            )\n",
    "            print(\"Done\")\n",
    "            csv_filename = (\n",
    "                f\"probe_{probe_id}_channel_{this_chan_id}_gamma_band_events.csv\"\n",
    "            )\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            gamma_times.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "\n",
    "            # movement artifact detection\n",
    "            process_stage = (\n",
    "                f\"Detecting Movement Artifacts on probe {probe_name} with id {probe_id}\"\n",
    "            )\n",
    "            for i in [0, 1]:\n",
    "                channel_outside_hp = take_two[i]\n",
    "                process_stage = f\"Detecting Movement Artifacts on control channel {channel_outside_hp} on probe {probe_name} with id {probe_id}\"\n",
    "                # process control channel ripple times\n",
    "                ripple_band_control = outof_hp_chans_lfp[i]\n",
    "\n",
    "                ripple_band_control = filter_ripple_band(ripple_band_control)\n",
    "                rip_power_controlchan = np.abs(signal.hilbert(ripple_band_control)) ** 2\n",
    "                movement_controls = ripple_detection.Karlsson_ripple_detector(\n",
    "                    time=lfp_time_index,  # if this doesnt work try adding .reshape(-1)\n",
    "                    filtered_lfps=rip_power_controlchan,  # indexing [:,None] is not needed here, rip_power_controlchan is already 2d (nsamples, 1)\n",
    "                    speed=dummy_speed,  # if this doesnt work try adding .reshape(-1)\n",
    "                    zscore_threshold=movement_artifact_ripple_band_threshold,\n",
    "                    sampling_frequency=1500.0,\n",
    "                )\n",
    "                speed_cols = [\n",
    "                    col for col in movement_controls.columns if \"speed\" in col\n",
    "                ]\n",
    "                movement_controls = movement_controls.drop(columns=speed_cols)\n",
    "                # write to file name\n",
    "                channel_outside_hp = \"channelsrawInd_\" + str(\n",
    "                    channel_outside_hp\n",
    "                )  # no cjannel id in IBL dataset, so this will do instead\n",
    "                csv_filename = f\"probe_{probe_id}_channel_{channel_outside_hp}_movement_artifacts.csv\"\n",
    "                csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "                movement_controls.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "                print(\"Done Probe id \" + str(probe_id))\n",
    "\n",
    "        # deleting the session folder\n",
    "        del one  # so that we can delete the session folder, note sr and ssl need to be deleted as well, already done earlier\n",
    "        process_stage = \"All processing done, Deleting the session folder\"\n",
    "        one_exists = False\n",
    "        # Get the file path of the session folder trim it and then use it to delete the folder\n",
    "        s = str(data_files[0])\n",
    "\n",
    "        # Find the index of \"raw_ephys_data\" in the string\n",
    "        index = s.find(\"raw_ephys_data\")\n",
    "\n",
    "        # Remove everything after \"raw_ephys_data\" including that string\n",
    "        s = s[:index]\n",
    "\n",
    "        # Remove the substring \"PosixPath('\"\n",
    "        s = s.replace(\"PosixPath('\", \"\")\n",
    "\n",
    "        # Remove the trailing slash\n",
    "        s = s.rstrip(\"/\")\n",
    "\n",
    "        # Define the bash command to delete the folder\n",
    "        cmd = f\"rm -r {s}\"\n",
    "\n",
    "        # Execute the bash command\n",
    "        os.system(cmd)\n",
    "\n",
    "        # in the session\n",
    "        logging.log(MESSAGE, f\"Processing complete for id {session_id}.\")\n",
    "    except Exception:\n",
    "\n",
    "        # we still need to clear the session\n",
    "        # if an error occured in deleting the session folder one will have already been deleted\n",
    "        if one_exists:\n",
    "            del one  # so that we can delete the session folder, note sr and ssl need to be deleted as well, already done earlier\n",
    "\n",
    "        if data_files is not None:\n",
    "            # to avoid an exception where data files have not been created yet\n",
    "            # Get the file path of the session folder trim it and then use it to delete the folder\n",
    "            s = str(data_files[0])\n",
    "\n",
    "            # Find the index of \"raw_ephys_data\" in the string\n",
    "            index = s.find(\"raw_ephys_data\")\n",
    "\n",
    "            # Remove everything after \"raw_ephys_data\" including that string\n",
    "            s = s[:index]\n",
    "\n",
    "            # Remove the substring \"PosixPath('\"\n",
    "            s = s.replace(\"PosixPath('\", \"\")\n",
    "\n",
    "            # Remove the trailing slash\n",
    "            s = s.rstrip(\"/\")\n",
    "\n",
    "            # Define the bash command to delete the folder\n",
    "            cmd = f\"rm -rf {s}\"\n",
    "\n",
    "            # Execute the bash command\n",
    "            os.system(cmd)\n",
    "        # Check if the session subfolder is empty\n",
    "        if os.path.exists(session_subfolder) and not os.listdir(session_subfolder):\n",
    "            # If it is, delete it\n",
    "            os.rmdir(session_subfolder)\n",
    "            logging.log(\n",
    "                MESSAGE,\n",
    "                \"PROCESSING FAILED REMOVING EMPTY SESSION SWR DIR :  session id %s \",\n",
    "                session_id,\n",
    "            )\n",
    "        # if there is an error we want to know about it, but we dont want it to stop the loop\n",
    "        # so we will print the error to a file and continue\n",
    "        logging.error(\n",
    "            \"Error in session: %s, probe id: %s, Process Error at : \",\n",
    "            session_id,\n",
    "            probe_id,\n",
    "            process_stage,\n",
    "        )\n",
    "        logging.error(traceback.format_exc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# run the processes with the specified number of cores:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(pool_size, initializer\u001b[38;5;241m=\u001b[39minit_pool, initargs\u001b[38;5;241m=\u001b[39m(queue,)) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sesh_with_ca1_eid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m queue\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkill\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m listener\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/miniconda3/envs/ONE_ibl_env/lib/python3.10/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ONE_ibl_env/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ONE_ibl_env/lib/python3.10/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ONE_ibl_env/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/ONE_ibl_env/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# set up the logging\n",
    "MESSAGE = 25  # Define a custom logging level, between INFO (20) and WARNING (30)\n",
    "\n",
    "# loading filters (crates artifacts in first and last ~ 3.5 seconds of recordings, remember to clip these off)\n",
    "# I don't think I need this it's at the start of my files\n",
    "gamma_filter = np.load(gamma_filters_path)\n",
    "gamma_filter = gamma_filter[\"arr_0\"]\n",
    "\n",
    "# load in the brain atlas and the brain region object for working with the ccf and ABI region id's in channels objects\n",
    "ba = AllenAtlas()\n",
    "br = BrainRegions()  # br is also an attribute of ba so could to br = ba.regions\n",
    "\n",
    "# Searching for datasets\n",
    "brain_acronym = \"CA1\"\n",
    "# query sessions endpoint\n",
    "# sessions, sess_details = one.search(atlas_acronym=brain_acronym, query_type='remote', details=True)\n",
    "\n",
    "swr_output_dir_path = os.path.join(output_dir, swr_output_dir)\n",
    "os.makedirs(swr_output_dir_path, exist_ok=True)\n",
    "sessions_without_ca1 = np.array([])\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "if save_lfp == True:\n",
    "    lfp_output_dir_path = os.path.join(output_dir, swr_output_dir + \"_lfp_data\")\n",
    "    os.makedirs(lfp_output_dir_path, exist_ok=True)\n",
    "\n",
    "queue = Queue()\n",
    "listener = Process(target=listener_process, args=(queue,))\n",
    "listener.start()\n",
    "\n",
    "data = np.load(session_npz_filepath)\n",
    "all_sesh_with_ca1_eid = data[\"all_sesh_with_ca1_eid_unique\"]\n",
    "del data\n",
    "\n",
    "\n",
    "# run the processes with the specified number of cores:\n",
    "with Pool(pool_size, initializer=init_pool, initargs=(queue,)) as p:\n",
    "    p.map(process_session, all_sesh_with_ca1_eid[0:2])\n",
    "\n",
    "queue.put(\"kill\")\n",
    "listener.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONE_ibl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
