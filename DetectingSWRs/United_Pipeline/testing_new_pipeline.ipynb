{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing new pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured for dataset: abi_visual_coding\n",
      "Pool size: 6\n",
      "Output directory: /space/scratch/SWR_final_pipeline/testing_dir\n",
      "SWR output directory: allen_viscoding_swr_murphylab2024\n"
     ]
    }
   ],
   "source": [
    "# United SWR detector\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io, signal, stats\n",
    "from scipy.signal import lfilter\n",
    "import scipy.ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import ripple_detection\n",
    "from ripple_detection import filter_ripple_band\n",
    "import ripple_detection.simulate as ripsim  # for making our time vectors\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "import sys\n",
    "from multiprocessing import Pool, Process, Queue, Manager, set_start_method\n",
    "import yaml\n",
    "import string\n",
    "\n",
    "# Get loader type from environment variable with a default value\n",
    "#DATASET_TO_PROCESS = os.environ.get('DATASET_TO_PROCESS', 'ibl').lower()\n",
    "DATASET_TO_PROCESS = 'abi_visual_behaviour'\n",
    "valid_datasets = ['ibl', 'abi_visual_behaviour', 'abi_visual_coding']\n",
    "if DATASET_TO_PROCESS not in valid_datasets:\n",
    "    raise ValueError(f\"DATASET_TO_PROCESS must be one of {valid_datasets}, got '{DATASET_TO_PROCESS}'\")\n",
    "\n",
    "\n",
    "# Lazy loading of the appropriate loader class\n",
    "if DATASET_TO_PROCESS == 'ibl':\n",
    "    from IBL_loader import ibl_loader\n",
    "elif DATASET_TO_PROCESS == 'abi_visual_behaviour':\n",
    "    from ABI_visual_behaviour_loader import abi_visual_behaviour_loader\n",
    "elif DATASET_TO_PROCESS == 'abi_visual_coding':\n",
    "    from ABI_visual_coding_loader import abi_visual_coding_loader\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset type: {DATASET_TO_PROCESS}\")\n",
    "\n",
    "# Load the configuration from a YAML file\n",
    "config_path = os.environ.get('CONFIG_PATH', 'united_detector_config.yaml')\n",
    "with open(config_path, \"r\") as f:\n",
    "    # Parse the YAML content\n",
    "    raw_content = f.read()\n",
    "    # Replace environment variables\n",
    "    for key, value in os.environ.items():\n",
    "        raw_content = raw_content.replace(f\"${key}\", value)\n",
    "    # Load the YAML\n",
    "    full_config = yaml.safe_load(raw_content)\n",
    "\n",
    "# Extract the unified output directory first\n",
    "output_dir = full_config.get(\"output_dir\", \"\")\n",
    "\n",
    "# Load common settings\n",
    "pool_size = full_config[\"pool_sizes\"][DATASET_TO_PROCESS]\n",
    "gamma_event_thresh = full_config[\"gamma_event_thresh\"]\n",
    "ripple_band_threshold = full_config[\"ripple_band_threshold\"]\n",
    "movement_artifact_ripple_band_threshold = full_config[\"movement_artifact_ripple_band_threshold\"]\n",
    "run_name = full_config[\"run_name\"]\n",
    "save_lfp = full_config[\"save_lfp\"]\n",
    "\n",
    "# Load dataset-specific settings\n",
    "if DATASET_TO_PROCESS == 'ibl':\n",
    "    # IBL specific settings\n",
    "    dataset_config = full_config[\"ibl\"]\n",
    "    gamma_filters_path = full_config[\"filters\"][\"gamma_filters\"]\n",
    "    oneapi_cache_dir = dataset_config[\"oneapi_cache_dir\"]\n",
    "    swr_output_dir = dataset_config[\"swr_output_dir\"]\n",
    "    dont_wipe_these_sessions = dataset_config[\"dont_wipe_these_sessions\"]\n",
    "    session_npz_filepath = dataset_config[\"session_npz_filepath\"]\n",
    "    # Additional IBL-specific variables if needed\n",
    "    \n",
    "elif DATASET_TO_PROCESS == 'abi_visual_behaviour':\n",
    "    # ABI (Allen) specific settings\n",
    "    dataset_config = full_config[\"abi_visual_behaviour\"]\n",
    "    gamma_filters_path = full_config[\"filters\"][\"gamma_filters\"]\n",
    "    #sdk_cache_dir = dataset_config[\"sdk_cache_dir\"]\n",
    "    swr_output_dir = dataset_config[\"swr_output_dir\"]\n",
    "    dont_wipe_these_sessions = dataset_config[\"dont_wipe_these_sessions\"]\n",
    "    only_brain_observatory_sessions = dataset_config[\"only_brain_observatory_sessions\"]\n",
    "    # Setting up the ABI Cache (where data is held, what is present or absent)\n",
    "    #manifest_path = os.path.join(sdk_cache_dir, \"manifest.json\")\n",
    "    # There's no session_npz_filepath for ABI in the consolidated config\n",
    "\n",
    "elif DATASET_TO_PROCESS == 'abi_visual_coding':\n",
    "    # ABI (Allen) specific settings\n",
    "    dataset_config = full_config[\"abi_visual_coding\"]\n",
    "    gamma_filters_path = full_config[\"filters\"][\"gamma_filters\"]\n",
    "    #sdk_cache_dir = dataset_config[\"sdk_cache_dir\"]\n",
    "    swr_output_dir = dataset_config[\"swr_output_dir\"]\n",
    "    dont_wipe_these_sessions = dataset_config[\"dont_wipe_these_sessions\"]\n",
    "    only_brain_observatory_sessions = dataset_config[\"only_brain_observatory_sessions\"]\n",
    "    # Setting up the ABI Cache (where data is held, what is present or absent)\n",
    "    #manifest_path = os.path.join(sdk_cache_dir, \"manifest.json\")\n",
    "    # There's no session_npz_filepath for ABI in the consolidated config\n",
    "\n",
    "print(f\"Configured for dataset: {DATASET_TO_PROCESS}\")\n",
    "print(f\"Pool size: {pool_size}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"SWR output directory: {swr_output_dir}\")\n",
    "\n",
    "\n",
    "# FUNCTIONS\n",
    "# subprocess is a default module\n",
    "def call_bash_function(bash_command=\"\"):\n",
    "    # example bash comand:\n",
    "    # bash_command = \"source /path/to/your/bash_script.sh && your_bash_function\"\n",
    "    process = subprocess.Popen(bash_command, stdout=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        print(\"Bash function executed successfully.\")\n",
    "        print(\"Output:\", output.decode(\"utf-8\"))\n",
    "    else:\n",
    "        print(\"Error:\", error.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# Assuming you have your signal_array, b, and a defined as before\n",
    "def finitimpresp_filter_for_LFP(\n",
    "    LFP_array, samplingfreq, lowcut=1, highcut=250, filter_order=101\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter the LFP array using a finite impulse response filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    LFP_array : np.array\n",
    "        The LFP array.\n",
    "    samplingfreq : float\n",
    "        The sampling frequency of the LFP array.\n",
    "    lowcut : float\n",
    "        The lowcut frequency.\n",
    "    highcut : float\n",
    "        The highcut frequency.\n",
    "    filter_order : int\n",
    "        The filter order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The filtered LFP array.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * samplingfreq\n",
    "\n",
    "    # Design the FIR bandpass filter using scipy.signal.firwin\n",
    "    fir_coeff = signal.firwin(\n",
    "        filter_order,\n",
    "        [lowcut / nyquist, highcut / nyquist],\n",
    "        pass_zero=False,\n",
    "        fs=samplingfreq,\n",
    "    )\n",
    "\n",
    "    # Apply the FIR filter to your signal_array\n",
    "    # filtered_signal = signal.convolve(LFP_array, fir_coeff, mode='same', method='auto')\n",
    "    filtered_signal = signal.lfilter(fir_coeff, 1.0, LFP_array, axis=0)\n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def event_boundary_detector(\n",
    "    time,\n",
    "    five_to_fourty_band_power_df,\n",
    "    envelope=True,\n",
    "    minimum_duration=0.02,\n",
    "    maximum_duration=0.4,\n",
    "    threshold_sd=2.5,\n",
    "    envelope_threshold_sd=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    For detecting gamma events.\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    five_to_fourty_band_power_df : np.array\n",
    "        The power of the signal in the 5-40 Hz band.\n",
    "    envelope : bool\n",
    "        Whether to use the envelope threshold.\n",
    "    minimum_duration : float\n",
    "        The minimum duration of an event.\n",
    "    maximum_duration : float\n",
    "        The maximum duration of an event.\n",
    "    threshold_sd : float\n",
    "        The threshold in standard deviations.\n",
    "    envelope_threshold_sd : float\n",
    "        The envelope threshold in standard deviations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.\n",
    "\n",
    "    \"\"\"\n",
    "    # make df to fill\n",
    "    row_of_info = {\n",
    "        \"start_time\": [],\n",
    "        \"end_time\": [],\n",
    "        \"duration\": [],\n",
    "    }\n",
    "\n",
    "    # sharp_wave_events_df = pd.DataFrame()\n",
    "    # scored_wave_power = stats.zscore(five_to_fourty_band_df)\n",
    "\n",
    "    # compute our power threshold\n",
    "    # wave_band_sd_thresh = np.std(five_to_fourty_band_df)*threshold_sd\n",
    "    five_to_fourty_band_power_df = stats.zscore(five_to_fourty_band_power_df)\n",
    "    past_thresh = five_to_fourty_band_power_df >= threshold_sd\n",
    "\n",
    "    # now we expand the sections that are past thresh up to the points that\n",
    "    # are past the envelope thresh, so not all sections above envelope thresh are true\n",
    "    # but those sections which alse contain a region past the detection threshold are included\n",
    "    def expand_sections(z_scores, boolean_array, thresh):\n",
    "        # Find indices where boolean_array is True\n",
    "        true_indices = np.where(boolean_array)[0]\n",
    "\n",
    "        # Initialize an array to keep track of expanded sections\n",
    "        expanded_sections = np.zeros_like(z_scores, dtype=bool)\n",
    "\n",
    "        # Iterate over true_indices and expand sections\n",
    "        for index in true_indices:\n",
    "            # Find the start and end of the current section\n",
    "            start = index\n",
    "            end = index\n",
    "\n",
    "            # Expand section to the left (while meeting conditions)\n",
    "            while start > 0 and z_scores[start - 1] > thresh:\n",
    "                start -= 1\n",
    "\n",
    "            # Expand section to the right (while meeting conditions)\n",
    "            while end < len(z_scores) - 1 and z_scores[end + 1] > thresh:\n",
    "                end += 1\n",
    "\n",
    "            # Check if the expanded section contains a point above envelope_threshold_sd in z_scores\n",
    "            if any(z_scores[start : end + 1] > thresh):\n",
    "                expanded_sections[start : end + 1] = True\n",
    "\n",
    "        # Update the boolean_array based on expanded_sections\n",
    "        boolean_array = boolean_array | expanded_sections\n",
    "\n",
    "        return boolean_array\n",
    "\n",
    "    if envelope == True:\n",
    "        past_thresh = expand_sections(\n",
    "            z_scores=five_to_fourty_band_power_df,\n",
    "            boolean_array=past_thresh,\n",
    "            thresh=envelope_threshold_sd,\n",
    "        )\n",
    "\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info[\"start_time\"] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info[\"end_time\"] = time[ends]\n",
    "\n",
    "    row_of_info[\"duration\"] = [\n",
    "        row_of_info[\"end_time\"][i] - row_of_info[\"start_time\"][i]\n",
    "        for i in range(0, len(row_of_info[\"start_time\"]))\n",
    "    ]\n",
    "\n",
    "    # turn the dictionary into adataframe\n",
    "    sharp_wave_events_df = pd.DataFrame(row_of_info)\n",
    "\n",
    "    # filter for the duration range we want\n",
    "    in_duration_range = (sharp_wave_events_df.duration > minimum_duration) & (\n",
    "        sharp_wave_events_df.duration < maximum_duration\n",
    "    )\n",
    "    sharp_wave_events_df = sharp_wave_events_df[in_duration_range]\n",
    "\n",
    "    return sharp_wave_events_df\n",
    "\n",
    "\n",
    "def event_boundary_times(time, past_thresh):\n",
    "    \"\"\"\n",
    "    Finds the times of a vector of true statements and returns values from another\n",
    "    array representing the times\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    past_thresh : np.array\n",
    "        The boolean array of the signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.\n",
    "    \"\"\"\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info[\"start_time\"] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info[\"end_time\"] = time[ends]\n",
    "\n",
    "    row_of_info[\"duration\"] = [\n",
    "        row_of_info[\"end_time\"][i] - row_of_info[\"start_time\"][i]\n",
    "        for i in range(0, len(row_of_info[\"start_time\"]))\n",
    "    ]\n",
    "\n",
    "    # turn the dictionary into adataframe\n",
    "    events_df = pd.DataFrame(row_of_info)\n",
    "\n",
    "    return events_df\n",
    "\n",
    "\n",
    "def peaks_time_of_events(events, time_values, signal_values):\n",
    "    \"\"\"\n",
    "    Computes the times when ripple power peaks in the events\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        The events dataframe.\n",
    "    time_values : np.array\n",
    "        The time values for the signal.\n",
    "    signal_values : np.array\n",
    "        The signal values for the signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The times of the peaks in the ripple power signal.\n",
    "    \"\"\"\n",
    "\n",
    "    # looks for the peaks in the ripple power signal, value of zscored raw lfp peak and returns time of peak\n",
    "    signal_values_zscore = stats.zscore(signal_values)\n",
    "    peak_times = []\n",
    "    for start, end in zip(events[\"start_time\"], events[\"end_time\"]):\n",
    "        window_idx = (time_values >= start) & (time_values <= end)\n",
    "        ripple_lfp_zscore_signal = signal_values_zscore[window_idx]\n",
    "        maxpoint = np.argmax(ripple_lfp_zscore_signal)\n",
    "        rippletimepoints = time_values[window_idx]\n",
    "        peak_times.append(rippletimepoints[maxpoint])\n",
    "    return np.array(peak_times)\n",
    "\n",
    "\n",
    "def resample_signal(signal, times, new_rate):\n",
    "    \"\"\"\n",
    "    Resample a 2D signal array to a new sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    signal (np.array): 2D array where each column is a source and each row is a time point.\n",
    "    times (np.array): 1D array of times corresponding to the rows of the signal array.\n",
    "    new_rate (float): The new sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "    new_signal (np.array): The resampled signal array.\n",
    "    new_times (np.array): The times corresponding to the rows of the new signal array.\n",
    "    \"\"\"\n",
    "    nsamples_new = int(len(times) * new_rate / (len(times) / times[-1]))\n",
    "    new_times = np.linspace(times[0], times[-1], nsamples_new)\n",
    "    new_signal = np.zeros((signal.shape[0], nsamples_new))\n",
    "\n",
    "    for i in range(signal.shape[0]):\n",
    "        interp_func = interpolate.interp1d(\n",
    "            times, signal[i, :], bounds_error=False, fill_value=\"extrapolate\"\n",
    "        )\n",
    "        new_signal[i, :] = interp_func(new_times)\n",
    "\n",
    "    return new_signal, new_times\n",
    "\n",
    "def listener_process(queue):\n",
    "    \"\"\"\n",
    "    This function listens for messages from the logging module and writes them to a log file.\n",
    "    It sets the logging level to MESSAGE so that only messages with level MESSAGE or higher are written to the log file.\n",
    "    This is a level we created to be between INFO and WARNING, so to see messages from this code and errors  but not other\n",
    "    messages that are mostly irrelevant and make the log file too large and uninterpretable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queue : multiprocessing.Queue\n",
    "        The queue to get messages from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    root = logging.getLogger()\n",
    "    h = logging.FileHandler(\n",
    "        f\"ibl_detector_{swr_output_dir}_{run_name}_app.log\", mode=\"w\"\n",
    "    )\n",
    "    f = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "    h.setFormatter(f)\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "\n",
    "    while True:\n",
    "        message = queue.get()\n",
    "        if message == \"kill\":\n",
    "            break\n",
    "        logger = logging.getLogger(message.name)\n",
    "        logger.handle(message)\n",
    "\n",
    "def init_pool(*args):\n",
    "    h = logging.handlers.QueueHandler(queue)\n",
    "    root = logging.getLogger()\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "\n",
    "\n",
    "# ABI Loaders\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "from scipy import signal, interpolate\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import (\n",
    "    VisualBehaviorNeuropixelsProjectCache,\n",
    ")\n",
    "\n",
    "# Use the Allen SDK to get sessions\n",
    "cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir='/space/scratch/allen_visbehave_data')\n",
    "\n",
    "class abi_visual_behaviour_loader_test:\n",
    "    def __init__(self, session_id):\n",
    "        \"\"\"\n",
    "        Initialize the ABI loader with a session ID.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        session_id : int\n",
    "            The ABI ecephys session ID\n",
    "        \"\"\"\n",
    "        self.session_id = session_id\n",
    "        self.cache = None\n",
    "        self.session = None\n",
    "        self.probe_id_list = None\n",
    "        self.probes_of_interest = None\n",
    "        \n",
    "    def set_up(self, cache_directory=None):\n",
    "        \"\"\"\n",
    "        Sets up the EcephysProjectCache and loads the session.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cache_directory : str, optional\n",
    "            Directory where to store the cache. If None, uses default.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : abi_loader\n",
    "            Returns the instance for method chaining.\n",
    "        \"\"\"\n",
    "        # Set up the cache\n",
    "        config_path = os.environ.get('CONFIG_PATH', 'united_detector_config.yaml')\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_content = f.read()\n",
    "            full_config = yaml.safe_load(config_content)\n",
    "        dataset_config = full_config[\"abi_visual_behaviour\"]\n",
    "        sdk_cache_dir = dataset_config[\"sdk_cache_dir\"]\n",
    "        manifest_path = os.path.join(sdk_cache_dir, \"manifest.json\")\n",
    "        #cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=sdk_cache_dir)\n",
    "        \n",
    "        if cache_directory is not None:\n",
    "            #self.cache = EcephysProjectCache(manifest=manifest_path, fetch_api=EcephysProjectCache.from_warehouse(cache_directory))\n",
    "            self.cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=sdk_cache_dir)\n",
    "        else:\n",
    "            self.cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=sdk_cache_dir)\n",
    "            \n",
    "        # Load the session\n",
    "        self.session = self.cache.get_ecephys_session(ecephys_session_id=self.session_id)\n",
    "        self.session.channels = self.session.get_channels()\n",
    "        \n",
    "        print(f\"Session {self.session_id} loaded\")\n",
    "        return self\n",
    "    \n",
    "    def has_ca1_channels(self):\n",
    "        \"\"\"\n",
    "        Checks if the session includes CA1 channels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if CA1 channels exist, False otherwise\n",
    "        \"\"\"\n",
    "        has_ca1 = np.isin(\"CA1\", list(self.session.channels.structure_acronym.unique()))\n",
    "        \n",
    "        if not has_ca1:\n",
    "            print(f\"Session {self.session_id} does not have CA1 channels\")\n",
    "            \n",
    "        return has_ca1\n",
    "    \n",
    "    def get_probes_with_ca1(self):\n",
    "        \"\"\"\n",
    "        Gets the list of probes that have CA1 channels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of probe IDs with CA1 channels\n",
    "        \"\"\"\n",
    "        # Get probes with LFP data\n",
    "        probes_table_df = self.cache.get_probe_table()\n",
    "        valid_lfp = probes_table_df[probes_table_df[\"has_lfp_data\"]]\n",
    "        \n",
    "        # Get probes for this session\n",
    "        self.probe_id_list = list(\n",
    "            valid_lfp[valid_lfp.ecephys_session_id == self.session_id].index\n",
    "        )\n",
    "        \n",
    "        # Find probes with CA1 channels\n",
    "        self.probes_of_interest = []\n",
    "        for probe_id in self.probe_id_list:\n",
    "            has_ca1_and_exists = np.isin(\n",
    "                \"CA1\",\n",
    "                list(\n",
    "                    self.session.channels[\n",
    "                        self.session.channels.probe_id == probe_id\n",
    "                    ].structure_acronym.unique()\n",
    "                ),\n",
    "            )\n",
    "            if has_ca1_and_exists:\n",
    "                self.probes_of_interest.append(probe_id)\n",
    "        \n",
    "        print(f\"Found {len(self.probes_of_interest)} probes with CA1 channels\")\n",
    "        return self.probes_of_interest\n",
    "    \n",
    "    def process_probe(self, probe_id, filter_ripple_band_func=None):\n",
    "        \"\"\"\n",
    "        Processes a single probe to extract CA1 and control channels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        probe_id : int\n",
    "            ID of the probe to process\n",
    "        filter_ripple_band_func : function, optional\n",
    "            Function to filter for ripple band\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with processing results\n",
    "        \"\"\"\n",
    "        print(f\"Processing probe: {probe_id}\")\n",
    "        \n",
    "        # Get LFP for the probe\n",
    "        lfp = self.session.get_lfp(probe_id)\n",
    "        og_lfp_obj_time_vals = lfp.time.values\n",
    "        # Get control channels outside hippocampus\n",
    "        idx = self.session.channels.probe_id == probe_id\n",
    "        organisedprobechans = self.session.channels[idx].sort_values(\n",
    "            by=\"probe_vertical_position\"\n",
    "        )\n",
    "        organisedprobechans = organisedprobechans[\n",
    "            np.isin(organisedprobechans.index.values, lfp.channel.values)\n",
    "        ]\n",
    "        \n",
    "        # Find channels outside hippocampus\n",
    "        not_a_ca1_chan = np.logical_not(\n",
    "            np.isin(\n",
    "                organisedprobechans.structure_acronym,\n",
    "                [\"CA3\", \"CA2\", \"CA1\", \"HPF\", \"EC\", \"DG\"],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Choose two random channels\n",
    "        take_two = np.random.choice(\n",
    "            organisedprobechans.index[not_a_ca1_chan], 2, replace=False\n",
    "        )\n",
    "        control_channels = []\n",
    "        \n",
    "        # Get LFP for control channels\n",
    "        for channel_outside_hp in take_two:\n",
    "            movement_control_channel = lfp.sel(channel=channel_outside_hp)\n",
    "            movement_control_channel = movement_control_channel.to_numpy()\n",
    "            # Resample to match CA1 data\n",
    "            movement_control_channel, lfp_time_index = self.resample_signal(movement_control_channel, lfp.time.values, 1500.0)\n",
    "            # needed for ripple detector method\n",
    "            #movement_control_channel = interp_func(lfp_time_index)\n",
    "            movement_control_channel = movement_control_channel[:, None]\n",
    "            control_channels.append(movement_control_channel)\n",
    "        \n",
    "        # Get CA1 channels for this probe\n",
    "        ca1_chans = self.session.channels.probe_channel_number[\n",
    "            (self.session.channels.probe_id == probe_id)\n",
    "            & (self.session.channels.structure_acronym == \"CA1\")\n",
    "        ]\n",
    "        ca1_idx = np.isin(lfp.channel.values, ca1_chans.index.values)\n",
    "        ca1_idx = lfp.channel.values[ca1_idx]\n",
    "        \n",
    "        # Select CA1 channels\n",
    "        lfp_ca1 = lfp.sel(channel=ca1_idx)\n",
    "        del lfp\n",
    "        lfp_ca1 = lfp_ca1.to_pandas()\n",
    "        lfp_ca1_chans = lfp_ca1.columns\n",
    "        lfp_ca1 = lfp_ca1.to_numpy()\n",
    "        \n",
    "        # Check for NaNs\n",
    "        if np.isnan(lfp_ca1).any():\n",
    "            print(f\"NaN detected in LFP data for probe {probe_id}, skipping\")\n",
    "            return None\n",
    "        \n",
    "        # Resample to 1500 Hz\n",
    "        lfp_ca1, lfp_time_index = self.resample_signal(\n",
    "            lfp_ca1, og_lfp_obj_time_vals, 1500.0\n",
    "        )\n",
    "        \n",
    "        # Find channel with highest ripple power if function provided\n",
    "        if filter_ripple_band_func is not None:\n",
    "            lfp_ca1_rippleband = filter_ripple_band_func(lfp_ca1)\n",
    "            highest_rip_power = np.abs(signal.hilbert(lfp_ca1_rippleband)) ** 2\n",
    "            highest_rip_power = highest_rip_power.max(axis=0)\n",
    "            \n",
    "            # Get channel with highest ripple power\n",
    "            peak_chan_idx = highest_rip_power.argmax()\n",
    "            this_chan_id = int(lfp_ca1_chans[peak_chan_idx])\n",
    "            peakrippleband = lfp_ca1_rippleband[:, peak_chan_idx]\n",
    "            peakripchan_lfp_ca1 = lfp_ca1[:, lfp_ca1_chans == this_chan_id]\n",
    "        else:\n",
    "            peak_chan_idx = None\n",
    "            this_chan_id = None\n",
    "            peakrippleband = None\n",
    "            peakripchan_lfp_ca1 = None\n",
    "        del lfp_ca1\n",
    "\n",
    "        \n",
    "        # Collect results\n",
    "        results = {\n",
    "            'probe_id': probe_id,\n",
    "            #'lfp_ca1': lfp_ca1,\n",
    "            'lfp_time_index': lfp_time_index,\n",
    "            'ca1_chans': lfp_ca1_chans,\n",
    "            'control_lfps': control_channels,\n",
    "            'control_channels': take_two,\n",
    "            'peak_ripple_chan_idx': peak_chan_idx,\n",
    "            'peak_ripple_chan_id': this_chan_id,\n",
    "            'peak_ripple_chan_raw_lfp': peakripchan_lfp_ca1,\n",
    "            'chan_id_string': str(this_chan_id) if this_chan_id is not None else None,\n",
    "            'rippleband': peakrippleband\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def resample_signal(self, signal_data, time_values, target_fs=1500.0):\n",
    "        \"\"\"\n",
    "        Resamples a signal to the target sampling frequency.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        signal_data : numpy.ndarray\n",
    "            Signal data to resample\n",
    "        time_values : numpy.ndarray\n",
    "            Time values corresponding to the signal data\n",
    "        target_fs : float, optional\n",
    "            Target sampling frequency\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (resampled_signal, new_time_values)\n",
    "        \"\"\"\n",
    "        # Create new time index\n",
    "        t_start = time_values[0]\n",
    "        t_end = time_values[-1]\n",
    "        dt_new = 1.0 / target_fs\n",
    "        n_samples = int(np.ceil((t_end - t_start) / dt_new))\n",
    "        new_time_values = t_start + np.arange(n_samples) * dt_new\n",
    "        \n",
    "        # Resample signal\n",
    "        if signal_data.ndim == 1:\n",
    "            # For 1D signals\n",
    "            interp_func = interpolate.interp1d(\n",
    "                time_values, signal_data, bounds_error=False, fill_value=\"extrapolate\"\n",
    "            )\n",
    "            resampled = interp_func(new_time_values)\n",
    "        else:\n",
    "            # For multi-channel signals\n",
    "            #resampled = np.zeros((signal_data.shape[0], len(new_time_values)))\n",
    "            resampled = np.zeros((len(new_time_values), signal_data.shape[1]))\n",
    "            for i in range(signal_data.shape[1]):\n",
    "                interp_func = interpolate.interp1d(\n",
    "                    time_values, signal_data[:, i], bounds_error=False, fill_value=\"extrapolate\"\n",
    "                )\n",
    "                resampled[:, i] = interp_func(new_time_values)\n",
    "        \n",
    "        return resampled, new_time_values\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Cleans up resources to free memory.\n",
    "        \"\"\"\n",
    "        self.session = None\n",
    "        \n",
    "\n",
    "session_id = \n",
    "\"\"\"\n",
    "This function takes in a session_id (eid in the IBL) and loops through the probes in that session,\n",
    "for each probe it finds the CA1 channel with the highest ripple power and uses that\n",
    "channel to detect SWR events.  It also detects gamma events and movement artifacts\n",
    "on two channels outside of the brain.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "session_id : int\n",
    "    The session id for the session to be processed.\n",
    "queue : multiprocessing.Queue\n",
    "    The queue to send messages to the listener process for recording errors.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "None\n",
    "but...\n",
    "Saves the following files to the folder specified by swr_output_dir_path.\n",
    "\n",
    "Notes:\n",
    "- The LFP is interpolated to 1500 Hz for all channels used.\n",
    "- The SWR detector used is the Karlsson ripple detector from the ripple_detection module.\n",
    "- The folders are titled by session and all files contain the name of the probe and the channel they originated from\n",
    "\"\"\"\n",
    "\n",
    "process_stage = f\"Starting the process, session{str(session_id)}\"  # for debugging\n",
    "probe_id = \"Not Loaded Yet\"\n",
    "one_exists = False\n",
    "# Add this near the beginning of the function\n",
    "data_files = None\n",
    "process_stage = \"Starting the process\"  # for debugging\n",
    "probe_id = \"Not Loaded Yet\"\n",
    "\n",
    "# Create session subfolder\n",
    "session_subfolder = \"swrs_session_\" + str(session_id)\n",
    "session_subfolder = os.path.join(swr_output_dir_path, session_subfolder)\n",
    "\n",
    "try:\n",
    "    # Set up brain atlas\n",
    "    process_stage = \"Setting up brain atlas\"\n",
    "    #ba = AllenAtlas()\n",
    "    #br = BrainRegions()\n",
    "    \n",
    "    process_stage = \"Session loaded, checking if directory exists\"\n",
    "    # Check if directory already exists\n",
    "    if os.path.exists(session_subfolder):\n",
    "        raise FileExistsError(f\"The directory {session_subfolder} already exists.\")\n",
    "    else:\n",
    "        os.makedirs(session_subfolder)\n",
    "        \n",
    "    if save_lfp == True:\n",
    "        # Create subfolder for lfp data\n",
    "        session_lfp_subfolder = \"lfp_session_\" + str(session_id)\n",
    "        session_lfp_subfolder = os.path.join(lfp_output_dir_path, session_lfp_subfolder)\n",
    "        os.makedirs(session_lfp_subfolder, exist_ok=True)\n",
    "    \n",
    "    # Initialize and set up the IBL loader\n",
    "    process_stage = \"Setting up IBL loader\"\n",
    "    \n",
    "    if DATASET_TO_PROCESS == 'ibl':\n",
    "        loader = ibl_loader(session_id)\n",
    "    elif DATASET_TO_PROCESS == 'abi_visual_behaviour':\n",
    "        loader = abi_visual_behaviour_loader(session_id)\n",
    "    elif DATASET_TO_PROCESS == 'abi_visual_coding':\n",
    "        loader = abi_visual_coding_loader(session_id)\n",
    "    loader.set_up()\n",
    "    one_exists = True  # Mark that we have a connection for error handling\n",
    "    \n",
    "    # Get probe IDs and names\n",
    "    process_stage = \"Getting probe IDs and names\"\n",
    "    if DATASET_TO_PROCESS == 'abi_visual_coding':\n",
    "        probenames = None\n",
    "        probelist = loader.get_probes_with_ca1()\n",
    "    elif DATASET_TO_PROCESS == 'abi_visual_behaviour':\n",
    "        probenames = None\n",
    "        probelist = loader.get_probes_with_ca1()\n",
    "    elif DATASET_TO_PROCESS == 'ibl':\n",
    "        probelist, probenames = loader.get_probe_ids_and_names()\n",
    "\n",
    "    process_stage = \"Running through the probes in the session\"\n",
    "    icount = 0\n",
    "    # Process each probe\n",
    "    for this_probe in range(len(probelist)):\n",
    "        if icount > 0:\n",
    "            break\n",
    "        icount = icount + 1\n",
    "        \n",
    "        if DATASET_TO_PROCESS == 'ibl':\n",
    "            probe_name = probenames[this_probe]\n",
    "        probe_id = probelist[this_probe]  # Always get the probe_id from probelist\n",
    "        print(f\"Processing probe: {str(probe_id)}\")\n",
    "\n",
    "        # Process the probe and get results\n",
    "        process_stage = f\"Processing probe with id {str(probe_id)}\"\n",
    "        if DATASET_TO_PROCESS == 'abi_visual_coding':\n",
    "            results = loader.process_probe(probe_id, filter_ripple_band)  # Use probe_id, not this_probe\n",
    "        elif DATASET_TO_PROCESS == 'abi_visual_behaviour':\n",
    "            results = loader.process_probe(probe_id, filter_ripple_band)  # Use probe_id, not this_probe\n",
    "        elif DATASET_TO_PROCESS == 'ibl':\n",
    "            results = loader.process_probe(this_probe, filter_ripple_band)  # Use probe_id, not this_probe\n",
    "        # Skip if no results (no CA1 channels or no bin file)\n",
    "        if results is None:\n",
    "            print(f\"No results for probe {probe_id}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Extract results\n",
    "        #lfp_ca1 = results['lfp_ca1']\n",
    "        peakripple_chan_raw_lfp = results['peak_ripple_chan_raw_lfp']\n",
    "        lfp_time_index = results['lfp_time_index']\n",
    "        ca1_chans = results['ca1_chans']\n",
    "        outof_hp_chans_lfp = results['control_lfps']\n",
    "        take_two = results['control_channels']\n",
    "        peakrippleband = results['rippleband']\n",
    "        this_chan_id = results['peak_ripple_chan_id']\n",
    "\n",
    "        # Filter to gamma band\n",
    "        gamma_band_ca1 = np.convolve(\n",
    "            peakripple_chan_raw_lfp.reshape(-1), gamma_filter, mode=\"same\"\n",
    "        )\n",
    "\n",
    "        # write our lfp to file\n",
    "        np.savez(\n",
    "            os.path.join(\n",
    "                session_lfp_subfolder,\n",
    "                f\"probe_{probe_id}_channel_{this_chan_id}_lfp_ca1_peakripplepower.npz\",\n",
    "            ),\n",
    "            lfp_ca1=peakripple_chan_raw_lfp,\n",
    "        )\n",
    "        np.savez(\n",
    "            os.path.join(\n",
    "                session_lfp_subfolder,\n",
    "                f\"probe_{probe_id}_channel_{this_chan_id}_lfp_time_index_1500hz.npz\",\n",
    "            ),\n",
    "            lfp_time_index = lfp_time_index,\n",
    "        )\n",
    "        print(f\"outof_hp_chans_lfp : {outof_hp_chans_lfp}\")\n",
    "        for i in range(2):\n",
    "            channel_outside_hp = take_two[i]\n",
    "            channel_outside_hp = \"channelsrawInd_\" + str(channel_outside_hp)\n",
    "            np.savez(\n",
    "                os.path.join(\n",
    "                    session_lfp_subfolder,\n",
    "                    f\"probe_{probe_id}_channel_{channel_outside_hp}_lfp_control_channel.npz\",\n",
    "                ),\n",
    "                lfp_control_channel=outof_hp_chans_lfp[i],\n",
    "            )\n",
    "\n",
    "        # create a dummy speed vector\n",
    "        dummy_speed = np.zeros_like(peakrippleband)\n",
    "        print(\"Detecting Putative Ripples\")\n",
    "        # we add a dimension to peakrippleband because the ripple detector needs it\n",
    "        process_stage = f\"Detecting Putative Ripples on probe with id {str(probe_id)}\"\n",
    "        \n",
    "        Karlsson_ripple_times = ripple_detection.Karlsson_ripple_detector(\n",
    "            time=lfp_time_index,\n",
    "            zscore_threshold=ripple_band_threshold,\n",
    "            filtered_lfps=peakrippleband[:, None],\n",
    "            speed=dummy_speed,\n",
    "            sampling_frequency=1500.0,\n",
    "        )\n",
    "\n",
    "        Karlsson_ripple_times = Karlsson_ripple_times[\n",
    "            Karlsson_ripple_times.duration < 0.25\n",
    "        ]\n",
    "        print(\"Done\")\n",
    "        # adds some stuff we want to the file\n",
    "\n",
    "        # ripple band power\n",
    "        peakrippleband_power = np.abs(signal.hilbert(peakrippleband)) ** 2\n",
    "        Karlsson_ripple_times[\"Peak_time\"] = peaks_time_of_events(\n",
    "            events=Karlsson_ripple_times,\n",
    "            time_values=lfp_time_index,\n",
    "            signal_values=peakrippleband_power,\n",
    "        )\n",
    "        speed_cols = [\n",
    "            col for col in Karlsson_ripple_times.columns if \"speed\" in col\n",
    "        ]\n",
    "        Karlsson_ripple_times = Karlsson_ripple_times.drop(columns=speed_cols)\n",
    "        csv_filename = (\n",
    "            f\"probe_{probe_id}_channel_{this_chan_id}_karlsson_detector_events.csv\"\n",
    "        )\n",
    "        csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "        Karlsson_ripple_times.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "        print(\"Writing to file.\")\n",
    "        print(\"Detecting gamma events.\")\n",
    "\n",
    "        # compute this later, I will have a seperate script called SWR filtering which will do this\n",
    "        process_stage = f\"Detecting Gamma Events on probe with id {str(probe_id)}\"\n",
    "        \n",
    "        gamma_power = np.abs(signal.hilbert(gamma_band_ca1)) ** 2\n",
    "        gamma_times = event_boundary_detector(\n",
    "            time=lfp_time_index,\n",
    "            threshold_sd=gamma_event_thresh,\n",
    "            envelope=False,\n",
    "            minimum_duration=0.015,\n",
    "            maximum_duration=float(\"inf\"),\n",
    "            five_to_fourty_band_power_df=gamma_power,\n",
    "        )\n",
    "        print(\"Done\")\n",
    "        csv_filename = (\n",
    "            f\"probe_{probe_id}_channel_{this_chan_id}_gamma_band_events.csv\"\n",
    "        )\n",
    "        csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "        gamma_times.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "\n",
    "        # movement artifact detection\n",
    "        process_stage = f\"Detecting Movement Artifacts on probe with id {probe_id}\"\n",
    "        \n",
    "        for i in [0, 1]:\n",
    "            channel_outside_hp = take_two[i]\n",
    "            process_stage = f\"Detecting Movement Artifacts on control channel {channel_outside_hp} on probe {probe_id}\"\n",
    "            # process control channel ripple times\n",
    "            ripple_band_control = outof_hp_chans_lfp[i]\n",
    "            dummy_speed = np.zeros_like(ripple_band_control)\n",
    "            ripple_band_control = filter_ripple_band(ripple_band_control)\n",
    "            rip_power_controlchan = np.abs(signal.hilbert(ripple_band_control)) ** 2\n",
    "            \n",
    "            print(f\"ripple_band_control shape: {ripple_band_control.shape}, length: {len(ripple_band_control)}\")\n",
    "            print(f\"lfp_time_index shape: {lfp_time_index.shape}, length: {len(lfp_time_index)}\")\n",
    "            print(f\"dummy_speed shape: {dummy_speed.shape}, length: {len(dummy_speed)}\")\n",
    "            \n",
    "            if DATASET_TO_PROCESS == 'abi_visual_behaviour':\n",
    "                lfp_time_index = lfp_time_index.reshape(-1)\n",
    "                dummy_speed = dummy_speed.reshape(-1)\n",
    "            if DATASET_TO_PROCESS == 'ibl':\n",
    "                # Reshape to ensure consistent (n_samples, n_channels) format for detector\n",
    "                # Prevents memory error when pd.notnull() creates boolean arrays with shape (n, n)\n",
    "                rip_power_controlchan = rip_power_controlchan.reshape(-1,1)\n",
    "            \n",
    "            movement_controls = ripple_detection.Karlsson_ripple_detector(\n",
    "                time=lfp_time_index.reshape(-1),  # if this doesnt work try adding .reshape(-1)\n",
    "                filtered_lfps=rip_power_controlchan,  # indexing [:,None] is not needed here, rip_power_controlchan is already 2d (nsamples, 1)\n",
    "                speed=dummy_speed.reshape(-1),  # if this doesnt work try adding .reshape(-1)\n",
    "                zscore_threshold=movement_artifact_ripple_band_threshold,\n",
    "                sampling_frequency=1500.0,\n",
    "            )\n",
    "            speed_cols = [\n",
    "                col for col in movement_controls.columns if \"speed\" in col\n",
    "            ]\n",
    "            movement_controls = movement_controls.drop(columns=speed_cols)\n",
    "            # write to file name\n",
    "            channel_outside_hp = \"channelsrawInd_\" + str(channel_outside_hp)  # no cjannel id in IBL dataset, so this will do instead\n",
    "            csv_filename = f\"probe_{probe_id}_channel_{channel_outside_hp}_movement_artifacts.csv\"\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            movement_controls.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "            print(\"Done Probe id \" + str(probe_id))\n",
    "\n",
    "    # deleting the session folder\n",
    "    # del one  # so that we can delete the session folder, note sr and ssl need to be deleted as well, already done earlier\n",
    "    if 'loader' in locals() and loader is not None:\n",
    "        loader.cleanup()\n",
    "    process_stage = \"All processing done, Deleting the session folder\"\n",
    "\n",
    "    # in the session\n",
    "    logging.log(MESSAGE, f\"Processing complete for id {session_id}.\")\n",
    "\n",
    "\n",
    "\n",
    "# set up the logging\n",
    "log_file = os.environ.get('LOG_FILE', f\"{DATASET_TO_PROCESS}_detector_{swr_output_dir}_{run_name}_app.log\")\n",
    "MESSAGE = 25  # Define a custom logging level, between INFO (20) and WARNING (30)\n",
    "logging.addLevelName(MESSAGE, \"MESSAGE\")\n",
    "\n",
    "# Set up file handler for logging\n",
    "file_handler = logging.FileHandler(log_file, mode=\"w\")\n",
    "formatter = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Set up root logger - but don't remove existing handlers\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(MESSAGE)  # Only log MESSAGE level and above\n",
    "root_logger.addHandler(file_handler)\n",
    "\n",
    "# Prevent propagation of lower-level warnings to the root logger\n",
    "for logger_name in ['hdmf', 'pynwb', 'spikeglx', 'ripple_detection']:\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.propagate = False  # Don't send these to the root logger\n",
    "\n",
    "\n",
    "# loading filters (crates artifacts in first and last ~ 3.5 seconds of recordings, remember to clip these off)\n",
    "# I don't think I need this it's at the start of my files\n",
    "gamma_filter = np.load(gamma_filters_path)\n",
    "gamma_filter = gamma_filter[\"arr_0\"]\n",
    "\n",
    "# Searching for datasets\n",
    "brain_acronym = \"CA1\"\n",
    "# query sessions endpoint\n",
    "# sessions, sess_details = one.search(atlas_acronym=brain_acronym, query_type='remote', details=True)\n",
    "\n",
    "swr_output_dir_path = os.path.join(output_dir, swr_output_dir)\n",
    "os.makedirs(swr_output_dir_path, exist_ok=True)\n",
    "sessions_without_ca1 = np.array([])\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "if save_lfp == True:\n",
    "    lfp_output_dir_path = os.path.join(output_dir, swr_output_dir + \"_lfp_data\")\n",
    "    os.makedirs(lfp_output_dir_path, exist_ok=True)\n",
    "\n",
    "queue = Queue()\n",
    "listener = Process(target=listener_process, args=(queue,))\n",
    "listener.start()\n",
    "\n",
    "if DATASET_TO_PROCESS == \"abi_visual_coding\":\n",
    "    # If processing Allen data\n",
    "    data_file_path = os.path.join(\"session_id_lists\", \"allen_viscoding_ca1_session_ids.npz\")\n",
    "    data = np.load(data_file_path)\n",
    "    all_sesh_with_ca1_eid = data[\"data\"]\n",
    "    del data\n",
    "    print(f\"Loaded {len(all_sesh_with_ca1_eid)} sessions from {data_file_path}\")\n",
    "\n",
    "if DATASET_TO_PROCESS == \"abi_visual_behaviour\":\n",
    "    # If processing Allen data\n",
    "    data_file_path = os.path.join(\"session_id_lists\", \"allen_visbehave_ca1_session_ids.npz\")\n",
    "    data = np.load(data_file_path)\n",
    "    all_sesh_with_ca1_eid = data[\"data\"]\n",
    "    del data\n",
    "    print(f\"Loaded {len(all_sesh_with_ca1_eid)} sessions from {data_file_path}\")\n",
    "\n",
    "elif DATASET_TO_PROCESS == \"ibl\":\n",
    "    # If processing IBL data\n",
    "    session_file_path = os.path.join(\"session_id_lists\", session_npz_filepath)\n",
    "    data = np.load(session_file_path)\n",
    "    all_sesh_with_ca1_eid = data[\"all_sesh_with_ca1_eid_unique\"]\n",
    "    del data\n",
    "    print(f\"Loaded {len(all_sesh_with_ca1_eid)} sessions from {session_file_path}\")\n",
    "\n",
    "\n",
    "# run the processes with the specified number of cores:\n",
    "with Pool(pool_size, initializer=init_pool, initargs=(queue,)) as p:\n",
    "    p.map(process_session, all_sesh_with_ca1_eid[10:11])\n",
    "\n",
    "queue.put(\"kill\")\n",
    "listener.join()\n",
    "\n",
    "# Find and clean up empty session folders\n",
    "print(f\"Checking for empty session folders in {swr_output_dir_path}\")\n",
    "empty_folder_count = 0\n",
    "\n",
    "for folder_name in os.listdir(swr_output_dir_path):\n",
    "    folder_path = os.path.join(swr_output_dir_path, folder_name)\n",
    "    \n",
    "    # Check if it's a directory and starts with the session prefix\n",
    "    if os.path.isdir(folder_path) and folder_name.startswith(\"swrs_session_\"):\n",
    "        # Check if the directory is empty\n",
    "        if not os.listdir(folder_path):\n",
    "            session_id = folder_name.replace(\"swrs_session_\", \"\")\n",
    "            logging.log(MESSAGE, f\"Empty session folder found and removed: {session_id}\")\n",
    "            print(f\"Removing empty session folder: {folder_path}\")\n",
    "            \n",
    "            # Remove the empty directory\n",
    "            os.rmdir(folder_path)\n",
    "            empty_folder_count += 1\n",
    "\n",
    "print(f\"Removed {empty_folder_count} empty session folders\")\n",
    "logging.log(MESSAGE, f\"Processing complete. Removed {empty_folder_count} empty session folders.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONE_ibl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
