{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing new pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured for dataset: abi\n",
      "Pool size: 6\n",
      "Output directory: /space/scratch/SWR_final_pipeline/testing_dir\n",
      "SWR output directory: allen_visbehave_swr_murphylab2024\n"
     ]
    }
   ],
   "source": [
    "# pipeline that works on the 1500 Hz data from all the \n",
    "# probes\n",
    "\n",
    "# IBL SWR detector\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io, signal, stats\n",
    "from scipy.signal import lfilter\n",
    "import scipy.ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import ripple_detection\n",
    "from ripple_detection import filter_ripple_band\n",
    "import ripple_detection.simulate as ripsim  # for making our time vectors\n",
    "from tqdm import tqdm\n",
    "#from iblatlas.atlas import AllenAtlas\n",
    "#from iblatlas.regions import BrainRegions\n",
    "\n",
    "#from one.api import ONE\n",
    "#import spikeglx\n",
    "#from brainbox.io.one import load_channel_locations\n",
    "#from brainbox.io.spikeglx import Streamer\n",
    "#from brainbox.io.one import SpikeSortingLoader\n",
    "#from neurodsp.voltage import destripe_lfp\n",
    "#from ibllib.plots import Density\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "import sys\n",
    "from multiprocessing import Pool, Process, Queue, Manager, set_start_method\n",
    "import yaml\n",
    "import string\n",
    "\n",
    "# Get loader type from environment variable with a default value\n",
    "#DATASET_TO_PROCESS = os.environ.get('DATASET_TO_PROCESS', 'ibl').lower()\n",
    "#DATASET_TO_PROCESS = os.environ.get('DATASET_TO_PROCESS', 'abi').lower()  \n",
    "DATASET_TO_PROCESS = 'abi'\n",
    "\n",
    "# Lazy loading of the appropriate loader class\n",
    "if DATASET_TO_PROCESS == 'ibl':\n",
    "    from IBL_loader import ibl_loader\n",
    "elif DATASET_TO_PROCESS == 'abi':\n",
    "    from ABI_loader import abi_loader\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset type: {DATASET_TO_PROCESS}\")\n",
    "\n",
    "# Load the configuration from a YAML file\n",
    "config_path = os.environ.get('CONFIG_PATH', 'united_detector_config.yaml')\n",
    "with open(config_path, \"r\") as f:\n",
    "    config_content = f.read()\n",
    "    full_config = yaml.safe_load(config_content)\n",
    "\n",
    "# Extract the unified output directory first\n",
    "output_dir = full_config.get(\"output_dir\", \"\")\n",
    "\n",
    "# Load common settings\n",
    "pool_size = full_config[\"pool_sizes\"][DATASET_TO_PROCESS]\n",
    "gamma_event_thresh = full_config[\"gamma_event_thresh\"]\n",
    "ripple_band_threshold = full_config[\"ripple_band_threshold\"]\n",
    "movement_artifact_ripple_band_threshold = full_config[\"movement_artifact_ripple_band_threshold\"]\n",
    "run_name = full_config[\"run_name\"]\n",
    "save_lfp = full_config[\"save_lfp\"]\n",
    "\n",
    "# Load dataset-specific settings\n",
    "if DATASET_TO_PROCESS == 'ibl':\n",
    "    # IBL specific settings\n",
    "    dataset_config = full_config[\"ibl\"]\n",
    "    gamma_filters_path = full_config[\"filters\"][\"gamma_filters\"][\"ibl\"]\n",
    "    oneapi_cache_dir = dataset_config[\"oneapi_cache_dir\"]\n",
    "    swr_output_dir = dataset_config[\"swr_output_dir\"]\n",
    "    dont_wipe_these_sessions = dataset_config[\"dont_wipe_these_sessions\"]\n",
    "    session_npz_filepath = dataset_config[\"session_npz_filepath\"]\n",
    "    # Additional IBL-specific variables if needed\n",
    "    \n",
    "elif DATASET_TO_PROCESS == 'abi':\n",
    "    # ABI (Allen) specific settings\n",
    "    dataset_config = full_config[\"abi\"]\n",
    "    gamma_filters_path = full_config[\"filters\"][\"gamma_filters\"][\"allen\"]\n",
    "    theta_filter_path = full_config[\"filters\"][\"theta_filter\"]\n",
    "    #sdk_cache_dir = dataset_config[\"sdk_cache_dir\"]\n",
    "    swr_output_dir = dataset_config[\"swr_output_dir\"]\n",
    "    dont_wipe_these_sessions = dataset_config[\"dont_wipe_these_sessions\"]\n",
    "    only_brain_observatory_sessions = dataset_config[\"only_brain_observatory_sessions\"]\n",
    "    # Setting up the ABI Cache (where data is held, what is present or absent)\n",
    "    #manifest_path = os.path.join(sdk_cache_dir, \"manifest.json\")\n",
    "    # There's no session_npz_filepath for ABI in the consolidated config\n",
    "\n",
    "print(f\"Configured for dataset: {DATASET_TO_PROCESS}\")\n",
    "print(f\"Pool size: {pool_size}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"SWR output directory: {swr_output_dir}\")\n",
    "\n",
    "\n",
    "# FUNCTIONS\n",
    "# subprocess is a default module\n",
    "def call_bash_function(bash_command=\"\"):\n",
    "    # example bash comand:\n",
    "    # bash_command = \"source /path/to/your/bash_script.sh && your_bash_function\"\n",
    "    process = subprocess.Popen(bash_command, stdout=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        print(\"Bash function executed successfully.\")\n",
    "        print(\"Output:\", output.decode(\"utf-8\"))\n",
    "    else:\n",
    "        print(\"Error:\", error.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# Assuming you have your signal_array, b, and a defined as before\n",
    "def finitimpresp_filter_for_LFP(\n",
    "    LFP_array, samplingfreq, lowcut=1, highcut=250, filter_order=101\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter the LFP array using a finite impulse response filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    LFP_array : np.array\n",
    "        The LFP array.\n",
    "    samplingfreq : float\n",
    "        The sampling frequency of the LFP array.\n",
    "    lowcut : float\n",
    "        The lowcut frequency.\n",
    "    highcut : float\n",
    "        The highcut frequency.\n",
    "    filter_order : int\n",
    "        The filter order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The filtered LFP array.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * samplingfreq\n",
    "\n",
    "    # Design the FIR bandpass filter using scipy.signal.firwin\n",
    "    fir_coeff = signal.firwin(\n",
    "        filter_order,\n",
    "        [lowcut / nyquist, highcut / nyquist],\n",
    "        pass_zero=False,\n",
    "        fs=samplingfreq,\n",
    "    )\n",
    "\n",
    "    # Apply the FIR filter to your signal_array\n",
    "    # filtered_signal = signal.convolve(LFP_array, fir_coeff, mode='same', method='auto')\n",
    "    filtered_signal = signal.lfilter(fir_coeff, 1.0, LFP_array, axis=0)\n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def event_boundary_detector(\n",
    "    time,\n",
    "    five_to_fourty_band_power_df,\n",
    "    envelope=True,\n",
    "    minimum_duration=0.02,\n",
    "    maximum_duration=0.4,\n",
    "    threshold_sd=2.5,\n",
    "    envelope_threshold_sd=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    For detecting gamma events.\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    five_to_fourty_band_power_df : np.array\n",
    "        The power of the signal in the 5-40 Hz band.\n",
    "    envelope : bool\n",
    "        Whether to use the envelope threshold.\n",
    "    minimum_duration : float\n",
    "        The minimum duration of an event.\n",
    "    maximum_duration : float\n",
    "        The maximum duration of an event.\n",
    "    threshold_sd : float\n",
    "        The threshold in standard deviations.\n",
    "    envelope_threshold_sd : float\n",
    "        The envelope threshold in standard deviations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.\n",
    "\n",
    "    \"\"\"\n",
    "    # make df to fill\n",
    "    row_of_info = {\n",
    "        \"start_time\": [],\n",
    "        \"end_time\": [],\n",
    "        \"duration\": [],\n",
    "    }\n",
    "\n",
    "    # sharp_wave_events_df = pd.DataFrame()\n",
    "    # scored_wave_power = stats.zscore(five_to_fourty_band_df)\n",
    "\n",
    "    # compute our power threshold\n",
    "    # wave_band_sd_thresh = np.std(five_to_fourty_band_df)*threshold_sd\n",
    "    five_to_fourty_band_power_df = stats.zscore(five_to_fourty_band_power_df)\n",
    "    past_thresh = five_to_fourty_band_power_df >= threshold_sd\n",
    "\n",
    "    # now we expand the sections that are past thresh up to the points that\n",
    "    # are past the envelope thresh, so not all sections above envelope thresh are true\n",
    "    # but those sections which alse contain a region past the detection threshold are included\n",
    "    def expand_sections(z_scores, boolean_array, thresh):\n",
    "        # Find indices where boolean_array is True\n",
    "        true_indices = np.where(boolean_array)[0]\n",
    "\n",
    "        # Initialize an array to keep track of expanded sections\n",
    "        expanded_sections = np.zeros_like(z_scores, dtype=bool)\n",
    "\n",
    "        # Iterate over true_indices and expand sections\n",
    "        for index in true_indices:\n",
    "            # Find the start and end of the current section\n",
    "            start = index\n",
    "            end = index\n",
    "\n",
    "            # Expand section to the left (while meeting conditions)\n",
    "            while start > 0 and z_scores[start - 1] > thresh:\n",
    "                start -= 1\n",
    "\n",
    "            # Expand section to the right (while meeting conditions)\n",
    "            while end < len(z_scores) - 1 and z_scores[end + 1] > thresh:\n",
    "                end += 1\n",
    "\n",
    "            # Check if the expanded section contains a point above envelope_threshold_sd in z_scores\n",
    "            if any(z_scores[start : end + 1] > thresh):\n",
    "                expanded_sections[start : end + 1] = True\n",
    "\n",
    "        # Update the boolean_array based on expanded_sections\n",
    "        boolean_array = boolean_array | expanded_sections\n",
    "\n",
    "        return boolean_array\n",
    "\n",
    "    if envelope == True:\n",
    "        past_thresh = expand_sections(\n",
    "            z_scores=five_to_fourty_band_power_df,\n",
    "            boolean_array=past_thresh,\n",
    "            thresh=envelope_threshold_sd,\n",
    "        )\n",
    "\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info[\"start_time\"] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info[\"end_time\"] = time[ends]\n",
    "\n",
    "    row_of_info[\"duration\"] = [\n",
    "        row_of_info[\"end_time\"][i] - row_of_info[\"start_time\"][i]\n",
    "        for i in range(0, len(row_of_info[\"start_time\"]))\n",
    "    ]\n",
    "\n",
    "    # turn the dictionary into adataframe\n",
    "    sharp_wave_events_df = pd.DataFrame(row_of_info)\n",
    "\n",
    "    # filter for the duration range we want\n",
    "    in_duration_range = (sharp_wave_events_df.duration > minimum_duration) & (\n",
    "        sharp_wave_events_df.duration < maximum_duration\n",
    "    )\n",
    "    sharp_wave_events_df = sharp_wave_events_df[in_duration_range]\n",
    "\n",
    "    return sharp_wave_events_df\n",
    "\n",
    "\n",
    "def event_boundary_times(time, past_thresh):\n",
    "    \"\"\"\n",
    "    Finds the times of a vector of true statements and returns values from another\n",
    "    array representing the times\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.array\n",
    "        The time values for the signal.\n",
    "    past_thresh : np.array\n",
    "        The boolean array of the signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the start and end times of the events.\n",
    "    \"\"\"\n",
    "    # Find the indices where consecutive True values start\n",
    "    starts = np.where(past_thresh & ~np.roll(past_thresh, 1))[0]\n",
    "    row_of_info[\"start_time\"] = time[starts]\n",
    "    # Find the indices where consecutive True values end\n",
    "    ends = np.where(past_thresh & ~np.roll(past_thresh, -1))[0]\n",
    "    row_of_info[\"end_time\"] = time[ends]\n",
    "\n",
    "    row_of_info[\"duration\"] = [\n",
    "        row_of_info[\"end_time\"][i] - row_of_info[\"start_time\"][i]\n",
    "        for i in range(0, len(row_of_info[\"start_time\"]))\n",
    "    ]\n",
    "\n",
    "    # turn the dictionary into adataframe\n",
    "    events_df = pd.DataFrame(row_of_info)\n",
    "\n",
    "    return events_df\n",
    "\n",
    "\n",
    "def peaks_time_of_events(events, time_values, signal_values):\n",
    "    \"\"\"\n",
    "    Computes the times when ripple power peaks in the events\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        The events dataframe.\n",
    "    time_values : np.array\n",
    "        The time values for the signal.\n",
    "    signal_values : np.array\n",
    "        The signal values for the signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The times of the peaks in the ripple power signal.\n",
    "    \"\"\"\n",
    "\n",
    "    # looks for the peaks in the ripple power signal, value of zscored raw lfp peak and returns time of peak\n",
    "    signal_values_zscore = stats.zscore(signal_values)\n",
    "    peak_times = []\n",
    "    for start, end in zip(events[\"start_time\"], events[\"end_time\"]):\n",
    "        window_idx = (time_values >= start) & (time_values <= end)\n",
    "        ripple_lfp_zscore_signal = signal_values_zscore[window_idx]\n",
    "        maxpoint = np.argmax(ripple_lfp_zscore_signal)\n",
    "        rippletimepoints = time_values[window_idx]\n",
    "        peak_times.append(rippletimepoints[maxpoint])\n",
    "    return np.array(peak_times)\n",
    "\n",
    "\n",
    "def resample_signal(signal, times, new_rate):\n",
    "    \"\"\"\n",
    "    Resample a 2D signal array to a new sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    signal (np.array): 2D array where each column is a source and each row is a time point.\n",
    "    times (np.array): 1D array of times corresponding to the rows of the signal array.\n",
    "    new_rate (float): The new sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "    new_signal (np.array): The resampled signal array.\n",
    "    new_times (np.array): The times corresponding to the rows of the new signal array.\n",
    "    \"\"\"\n",
    "    nsamples_new = int(len(times) * new_rate / (len(times) / times[-1]))\n",
    "    new_times = np.linspace(times[0], times[-1], nsamples_new)\n",
    "    new_signal = np.zeros((signal.shape[0], nsamples_new))\n",
    "\n",
    "    for i in range(signal.shape[0]):\n",
    "        interp_func = interpolate.interp1d(\n",
    "            times, signal[i, :], bounds_error=False, fill_value=\"extrapolate\"\n",
    "        )\n",
    "        new_signal[i, :] = interp_func(new_times)\n",
    "\n",
    "    return new_signal, new_times\n",
    "\n",
    "def listener_process(queue):\n",
    "    \"\"\"\n",
    "    This function listens for messages from the logging module and writes them to a log file.\n",
    "    It sets the logging level to MESSAGE so that only messages with level MESSAGE or higher are written to the log file.\n",
    "    This is a level we created to be between INFO and WARNING, so to see messages from this code and errors  but not other\n",
    "    messages that are mostly irrelevant and make the log file too large and uninterpretable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queue : multiprocessing.Queue\n",
    "        The queue to get messages from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    root = logging.getLogger()\n",
    "    h = logging.FileHandler(\n",
    "        f\"ibl_detector_{swr_output_dir}_{run_name}_app.log\", mode=\"w\"\n",
    "    )\n",
    "    f = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "    h.setFormatter(f)\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n",
    "\n",
    "    while True:\n",
    "        message = queue.get()\n",
    "        if message == \"kill\":\n",
    "            break\n",
    "        logger = logging.getLogger(message.name)\n",
    "        logger.handle(message)\n",
    "\n",
    "def init_pool(*args):\n",
    "    h = logging.handlers.QueueHandler(queue)\n",
    "    root = logging.getLogger()\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(MESSAGE)  # Set logging level to MESSAGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_session(session_id):\n",
    "    \"\"\"\n",
    "    This function takes in a session_id (eid in the IBL) and loops through the probes in that session,\n",
    "    for each probe it finds the CA1 channel with the highest ripple power and uses that\n",
    "    channel to detect SWR events.  It also detects gamma events and movement artifacts\n",
    "    on two channels outside of the brain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session_id : int\n",
    "        The session id for the session to be processed.\n",
    "    queue : multiprocessing.Queue\n",
    "        The queue to send messages to the listener process for recording errors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    but...\n",
    "    Saves the following files to the folder specified by swr_output_dir_path.\n",
    "    \n",
    "    Notes:\n",
    "    - The LFP is interpolated to 1500 Hz for all channels used.\n",
    "    - The SWR detector used is the Karlsson ripple detector from the ripple_detection module.\n",
    "    - The folders are titled by session and all files contain the name of the probe and the channel they originated from\n",
    "    \"\"\"\n",
    "    \n",
    "    process_stage = f\"Starting the process, session{str(session_id)}\"  # for debugging\n",
    "    probe_id = \"Not Loaded Yet\"\n",
    "    one_exists = False\n",
    "    # Add this near the beginning of the function\n",
    "    data_files = None\n",
    "    process_stage = \"Starting the process\"  # for debugging\n",
    "    probe_id = \"Not Loaded Yet\"\n",
    "    \n",
    "    # Create session subfolder\n",
    "    session_subfolder = \"swrs_session_\" + str(session_id)\n",
    "    session_subfolder = os.path.join(swr_output_dir_path, session_subfolder)\n",
    "    \n",
    "    try:\n",
    "        # Set up brain atlas\n",
    "        process_stage = \"Setting up brain atlas\"\n",
    "        #ba = AllenAtlas()\n",
    "        #br = BrainRegions()\n",
    "        \n",
    "        process_stage = \"Session loaded, checking if directory exists\"\n",
    "        # Check if directory already exists\n",
    "        if os.path.exists(session_subfolder):\n",
    "            raise FileExistsError(f\"The directory {session_subfolder} already exists.\")\n",
    "        else:\n",
    "            os.makedirs(session_subfolder)\n",
    "            \n",
    "        if save_lfp == True:\n",
    "            # Create subfolder for lfp data\n",
    "            session_lfp_subfolder = \"lfp_session_\" + str(session_id)\n",
    "            session_lfp_subfolder = os.path.join(lfp_output_dir_path, session_lfp_subfolder)\n",
    "            os.makedirs(session_lfp_subfolder, exist_ok=True)\n",
    "        \n",
    "        # Initialize and set up the IBL loader\n",
    "        process_stage = \"Setting up IBL loader\"\n",
    "        \n",
    "        if DATASET_TO_PROCESS == 'ibl':\n",
    "            loader = ibl_loader(session_id)\n",
    "        elif DATASET_TO_PROCESS == 'abi':\n",
    "            loader = abi_loader(session_id)\n",
    "        loader.set_up()\n",
    "        one_exists = True  # Mark that we have a connection for error handling\n",
    "        \n",
    "        # Get probe IDs and names\n",
    "        process_stage = \"Getting probe IDs and names\"\n",
    "        if DATASET_TO_PROCESS == 'ibl':\n",
    "            probelist, probenames = loader.get_probe_ids_and_names()\n",
    "        elif DATASET_TO_PROCESS == 'abi':\n",
    "            probenames = None\n",
    "            probelist = loader.get_probes_with_ca1()\n",
    "\n",
    "        process_stage = \"Running through the probes in the session\"\n",
    "        # Process each probe\n",
    "        for this_probe in range(len(probelist)):\n",
    "            if DATASET_TO_PROCESS == 'ibl':\n",
    "                probe_name = probenames[this_probe]\n",
    "            probe_id = probelist[this_probe]  # Always get the probe_id from probelist\n",
    "            print(f\"Processing probe: {str(probe_id)}\")\n",
    "\n",
    "            # Process the probe and get results\n",
    "            process_stage = f\"Processing probe with id {str(probe_id)}\"\n",
    "            results = loader.process_probe(probe_id, filter_ripple_band)  # Use probe_id, not this_probe\n",
    "\n",
    "            # Skip if no results (no CA1 channels or no bin file)\n",
    "            if results is None:\n",
    "                print(f\"No results for probe {probe_id}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Extract results\n",
    "            #lfp_ca1 = results['lfp_ca1']\n",
    "            peakrippleband = lfp_ca1_ripppleband[:, highest_rip_power.argmax()]\n",
    "            peakripple_chan_raw_lfp = lfp_ca1[:, highest_rip_power.argmax()]\n",
    "            lfp_time_index = results['lfp_time_index']\n",
    "            ca1_chans = results['ca1_chans']\n",
    "            outof_hp_chans_lfp = results['control_lfps']\n",
    "            take_two = results['control_channels']\n",
    "\n",
    "            # From here, continue with your existing processing steps:\n",
    "            # - Filter for ripple band\n",
    "            lfp_ca1_ripppleband = filter_ripple_band(lfp_ca1)\n",
    "            highest_rip_power = np.abs(signal.hilbert(lfp_ca1_ripppleband)) ** 2\n",
    "            highest_rip_power = highest_rip_power.max(axis=0)\n",
    "\n",
    "            # Get channel ID and peak ripple data\n",
    "            this_chan_id = \"channelsrawInd_\" + str(ca1_chans[highest_rip_power.argmax()])\n",
    "            peakrippleband = lfp_ca1_ripppleband[:, highest_rip_power.argmax()]\n",
    "            peakripple_chan_raw_lfp = lfp_ca1[:, highest_rip_power.argmax()]\n",
    "\n",
    "            # Filter to gamma band\n",
    "            gamma_band_ca1 = np.convolve(\n",
    "                peakripple_chan_raw_lfp.reshape(-1), gamma_filter, mode=\"same\"\n",
    "            )\n",
    "\n",
    "            # write our lfp to file\n",
    "            np.savez(\n",
    "                os.path.join(\n",
    "                    session_lfp_subfolder,\n",
    "                    f\"probe_{probe_id}_channel_{this_chan_id}_lfp_ca1_peakripplepower.npz\",\n",
    "                ),\n",
    "                lfp_ca1=peakripple_chan_raw_lfp,\n",
    "            )\n",
    "            np.savez(\n",
    "                os.path.join(\n",
    "                    session_lfp_subfolder,\n",
    "                    f\"probe_{probe_id}_channel_{this_chan_id}_lfp_time_index_1500hz.npz\",\n",
    "                ),\n",
    "                lfp_time_index=lfp_time_index,\n",
    "            )\n",
    "            for i in range(2):\n",
    "                channel_outside_hp = take_two[i]\n",
    "                channel_outside_hp = \"channelsrawInd_\" + str(channel_outside_hp)\n",
    "                np.savez(\n",
    "                    os.path.join(\n",
    "                        session_lfp_subfolder,\n",
    "                        f\"probe_{probe_id}_channel_{channel_outside_hp}_lfp_control_channel.npz\",\n",
    "                    ),\n",
    "                    lfp_control_channel=outof_hp_chans_lfp[i],\n",
    "                )\n",
    "\n",
    "            del lfp_ca1  # clear up some memory\n",
    "\n",
    "            # create a dummy speed vector\n",
    "            dummy_speed = np.zeros_like(peakrippleband)\n",
    "            print(\"Detecting Putative Ripples\")\n",
    "            # we add a dimension to peakrippleband because the ripple detector needs it\n",
    "            process_stage = f\"Detecting Putative Ripples on probe with id {str(probe_id)}\"\n",
    "            \n",
    "            Karlsson_ripple_times = ripple_detection.Karlsson_ripple_detector(\n",
    "                time=lfp_time_index,\n",
    "                zscore_threshold=ripple_band_threshold,\n",
    "                filtered_lfps=peakrippleband[:, None],\n",
    "                speed=dummy_speed,\n",
    "                sampling_frequency=1500.0,\n",
    "            )\n",
    "\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times[\n",
    "                Karlsson_ripple_times.duration < 0.25\n",
    "            ]\n",
    "            print(\"Done\")\n",
    "            # adds some stuff we want to the file\n",
    "\n",
    "            # ripple band power\n",
    "            peakrippleband_power = np.abs(signal.hilbert(peakrippleband)) ** 2\n",
    "            Karlsson_ripple_times[\"Peak_time\"] = peaks_time_of_events(\n",
    "                events=Karlsson_ripple_times,\n",
    "                time_values=lfp_time_index,\n",
    "                signal_values=peakrippleband_power,\n",
    "            )\n",
    "            speed_cols = [\n",
    "                col for col in Karlsson_ripple_times.columns if \"speed\" in col\n",
    "            ]\n",
    "            Karlsson_ripple_times = Karlsson_ripple_times.drop(columns=speed_cols)\n",
    "            csv_filename = (\n",
    "                f\"probe_{probe_id}_channel_{this_chan_id}_karlsson_detector_events.csv\"\n",
    "            )\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            Karlsson_ripple_times.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "            print(\"Writing to file.\")\n",
    "            print(\"Detecting gamma events.\")\n",
    "\n",
    "            # compute this later, I will have a seperate script called SWR filtering which will do this\n",
    "            process_stage = f\"Detecting Gamma Events on probe with id {str(probe_id)}\"\n",
    "            \n",
    "            gamma_power = np.abs(signal.hilbert(gamma_band_ca1)) ** 2\n",
    "            gamma_times = event_boundary_detector(\n",
    "                time=lfp_time_index,\n",
    "                threshold_sd=gamma_event_thresh,\n",
    "                envelope=False,\n",
    "                minimum_duration=0.015,\n",
    "                maximum_duration=float(\"inf\"),\n",
    "                five_to_fourty_band_power_df=gamma_power,\n",
    "            )\n",
    "            print(\"Done\")\n",
    "            csv_filename = (\n",
    "                f\"probe_{probe_id}_channel_{this_chan_id}_gamma_band_events.csv\"\n",
    "            )\n",
    "            csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "            gamma_times.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "\n",
    "            # movement artifact detection\n",
    "            process_stage = f\"Detecting Movement Artifacts on probe with id {probe_id}\"\n",
    "            \n",
    "            for i in [0, 1]:\n",
    "                channel_outside_hp = take_two[i]\n",
    "                process_stage = f\"Detecting Movement Artifacts on control channel {channel_outside_hp} on probe {probe_id}\"\n",
    "                # process control channel ripple times\n",
    "                ripple_band_control = outof_hp_chans_lfp[i]\n",
    "                dummy_speed = np.zeros_like(ripple_band_control)\n",
    "                ripple_band_control = filter_ripple_band(ripple_band_control)\n",
    "                rip_power_controlchan = np.abs(signal.hilbert(ripple_band_control)) ** 2\n",
    "                \n",
    "                print(f\"ripple_band_control shape: {ripple_band_control.shape}, length: {len(ripple_band_control)}\")\n",
    "                print(f\"lfp_time_index shape: {lfp_time_index.shape}, length: {len(lfp_time_index)}\")\n",
    "                print(f\"dummy_speed shape: {dummy_speed.shape}, length: {len(dummy_speed)}\")\n",
    "                \n",
    "                movement_controls = ripple_detection.Karlsson_ripple_detector(\n",
    "                    time=lfp_time_index.reshape(-1),  # if this doesnt work try adding .reshape(-1)\n",
    "                    filtered_lfps=rip_power_controlchan,  # indexing [:,None] is not needed here, rip_power_controlchan is already 2d (nsamples, 1)\n",
    "                    speed=dummy_speed.reshape(-1),  # if this doesnt work try adding .reshape(-1)\n",
    "                    zscore_threshold=movement_artifact_ripple_band_threshold,\n",
    "                    sampling_frequency=1500.0,\n",
    "                )\n",
    "                speed_cols = [\n",
    "                    col for col in movement_controls.columns if \"speed\" in col\n",
    "                ]\n",
    "                movement_controls = movement_controls.drop(columns=speed_cols)\n",
    "                # write to file name\n",
    "                channel_outside_hp = \"channelsrawInd_\" + str(channel_outside_hp)  # no cjannel id in IBL dataset, so this will do instead\n",
    "                csv_filename = f\"probe_{probe_id}_channel_{channel_outside_hp}_movement_artifacts.csv\"\n",
    "                csv_path = os.path.join(session_subfolder, csv_filename)\n",
    "                movement_controls.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "                print(\"Done Probe id \" + str(probe_id))\n",
    "\n",
    "        # deleting the session folder\n",
    "        # del one  # so that we can delete the session folder, note sr and ssl need to be deleted as well, already done earlier\n",
    "        if 'loader' in locals() and loader is not None:\n",
    "            loader.cleanup()\n",
    "        process_stage = \"All processing done, Deleting the session folder\"\n",
    "        one_exists = False\n",
    "        # Get the file path of the session folder trim it and then use it to delete the folder\n",
    "        s = str(data_files[0])\n",
    "\n",
    "        # Find the index of \"raw_ephys_data\" in the string\n",
    "        index = s.find(\"raw_ephys_data\")\n",
    "\n",
    "        # Remove everything after \"raw_ephys_data\" including that string\n",
    "        s = s[:index]\n",
    "\n",
    "        # Remove the substring \"PosixPath('\"\n",
    "        s = s.replace(\"PosixPath('\", \"\")\n",
    "\n",
    "        # Remove the trailing slash\n",
    "        s = s.rstrip(\"/\")\n",
    "\n",
    "        # Define the bash command to delete the folder\n",
    "        cmd = f\"rm -r {s}\"\n",
    "\n",
    "        # Execute the bash command\n",
    "        os.system(cmd)\n",
    "\n",
    "        # in the session\n",
    "        logging.log(MESSAGE, f\"Processing complete for id {session_id}.\")\n",
    "    except Exception:\n",
    "\n",
    "        if data_files is not None:\n",
    "            # to avoid an exception where data files have not been created yet\n",
    "            # Get the file path of the session folder trim it and then use it to delete the folder\n",
    "            s = str(data_files[0])\n",
    "\n",
    "            # Find the index of \"raw_ephys_data\" in the string\n",
    "            index = s.find(\"raw_ephys_data\")\n",
    "\n",
    "            # Remove everything after \"raw_ephys_data\" including that string\n",
    "            s = s[:index]\n",
    "\n",
    "            # Remove the substring \"PosixPath('\"\n",
    "            s = s.replace(\"PosixPath('\", \"\")\n",
    "\n",
    "            # Remove the trailing slash\n",
    "            s = s.rstrip(\"/\")\n",
    "\n",
    "            # Define the bash command to delete the folder\n",
    "            cmd = f\"rm -rf {s}\"\n",
    "\n",
    "            # Execute the bash command\n",
    "            os.system(cmd)\n",
    "        # Check if the session subfolder is empty\n",
    "        if os.path.exists(session_subfolder) and not os.listdir(session_subfolder):\n",
    "            # If it is, delete it\n",
    "            os.rmdir(session_subfolder)\n",
    "            logging.log(\n",
    "                MESSAGE,\n",
    "                \"PROCESSING FAILED REMOVING EMPTY SESSION SWR DIR :  session id %s \",\n",
    "                session_id,\n",
    "            )\n",
    "        # if there is an error we want to know about it, but we dont want it to stop the loop\n",
    "        # so we will print the error to a file and continue\n",
    "        logging.error(\n",
    "            \"Error in session: %s, probe id: %s, Process Error at : \",\n",
    "            session_id,\n",
    "            probe_id,\n",
    "            process_stage,\n",
    "        )\n",
    "        logging.error(traceback.format_exc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom one.api import ONE\\nata = np.load(session_npz_filepath)\\nall_sesh_with_ca1_eid = data[\"all_sesh_with_ca1_eid_unique\"]\\nsession_id = all_sesh_with_ca1_eid[0]\\nONE.setup(base_url=\"https://openalyx.internationalbrainlab.org\", silent=True)\\none = ONE(password=\"international\", silent=True)\\nprint(one.search_terms())\\nprobelist, probenames = one.eid2pid(session_id)\\nprobe_name = probenames[0]\\ncollectionname = f\"alf/{probe_name}/pykilosort\"\\nchannels = one.load_object(session_id, \"channels\", collection=collectionname)\\nchannels\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from one.api import ONE\n",
    "ata = np.load(session_npz_filepath)\n",
    "all_sesh_with_ca1_eid = data[\"all_sesh_with_ca1_eid_unique\"]\n",
    "session_id = all_sesh_with_ca1_eid[0]\n",
    "ONE.setup(base_url=\"https://openalyx.internationalbrainlab.org\", silent=True)\n",
    "one = ONE(password=\"international\", silent=True)\n",
    "print(one.search_terms())\n",
    "probelist, probenames = one.eid2pid(session_id)\n",
    "probe_name = probenames[0]\n",
    "collectionname = f\"alf/{probe_name}/pykilosort\"\n",
    "channels = one.load_object(session_id, \"channels\", collection=collectionname)\n",
    "channels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 sessions from allen_visbehave_ca1_session_ids.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1044385384 loaded\n",
      "Found 4 probes with CA1 channels\n",
      "Processing probe: 1044506933\n",
      "Processing probe: 1044506933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1044594870 loaded\n",
      "Found 4 probes with CA1 channels\n",
      "Processing probe: 1044791094\n",
      "Processing probe: 1044791094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.5.1 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.2.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3894227/2344585702.py\", line 94, in process_session\n",
      "    lfp_ca1 = results['lfp_ca1']\n",
      "KeyError: 'lfp_ca1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/handlers.py\", line 1461, in emit\n",
      "    self.enqueue(self.prepare(record))\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/handlers.py\", line 1444, in prepare\n",
      "    msg = self.format(record)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3046, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3101, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3488, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3894227/498840834.py\", line 41, in <module>\n",
      "    with Pool(pool_size, initializer=init_pool, initargs=(queue,)) as p:\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/context.py\", line 119, in Pool\n",
      "    return Pool(processes, initializer, initargs, maxtasksperchild,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 212, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 303, in _repopulate_pool\n",
      "    return self._repopulate_pool_static(self._ctx, self.Process,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n",
      "    w.start()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/popen_fork.py\", line 71, in _launch\n",
      "    code = process_obj._bootstrap(parent_sentinel=child_r)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/tmp/ipykernel_3894227/2344585702.py\", line 300, in process_session\n",
      "    logging.error(\n",
      "Message: 'Error in session: %s, probe id: %s, Process Error at : '\n",
      "Arguments: (1044385384, 1044506933, 'Processing probe with id 1044506933')\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3894227/2344585702.py\", line 94, in process_session\n",
      "    lfp_ca1 = results['lfp_ca1']\n",
      "KeyError: 'lfp_ca1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/handlers.py\", line 1461, in emit\n",
      "    self.enqueue(self.prepare(record))\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/handlers.py\", line 1444, in prepare\n",
      "    msg = self.format(record)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3046, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3101, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3488, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3894227/498840834.py\", line 41, in <module>\n",
      "    with Pool(pool_size, initializer=init_pool, initargs=(queue,)) as p:\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/context.py\", line 119, in Pool\n",
      "    return Pool(processes, initializer, initargs, maxtasksperchild,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 212, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 303, in _repopulate_pool\n",
      "    return self._repopulate_pool_static(self._ctx, self.Process,\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n",
      "    w.start()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/popen_fork.py\", line 71, in _launch\n",
      "    code = process_obj._bootstrap(parent_sentinel=child_r)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/acampbell/miniconda3/envs/allensdk_env/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/tmp/ipykernel_3894227/2344585702.py\", line 300, in process_session\n",
      "    logging.error(\n",
      "Message: 'Error in session: %s, probe id: %s, Process Error at : '\n",
      "Arguments: (1044594870, 1044791094, 'Processing probe with id 1044791094')\n"
     ]
    }
   ],
   "source": [
    "# set up the logging\n",
    "MESSAGE = 25  # Define a custom logging level, between INFO (20) and WARNING (30)\n",
    "\n",
    "# loading filters (crates artifacts in first and last ~ 3.5 seconds of recordings, remember to clip these off)\n",
    "# I don't think I need this it's at the start of my files\n",
    "gamma_filter = np.load(gamma_filters_path)\n",
    "gamma_filter = gamma_filter[\"arr_0\"]\n",
    "\n",
    "# Searching for datasets\n",
    "brain_acronym = \"CA1\"\n",
    "# query sessions endpoint\n",
    "# sessions, sess_details = one.search(atlas_acronym=brain_acronym, query_type='remote', details=True)\n",
    "\n",
    "swr_output_dir_path = os.path.join(output_dir, swr_output_dir)\n",
    "os.makedirs(swr_output_dir_path, exist_ok=True)\n",
    "sessions_without_ca1 = np.array([])\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "if save_lfp == True:\n",
    "    lfp_output_dir_path = os.path.join(output_dir, swr_output_dir + \"_lfp_data\")\n",
    "    os.makedirs(lfp_output_dir_path, exist_ok=True)\n",
    "\n",
    "queue = Queue()\n",
    "listener = Process(target=listener_process, args=(queue,))\n",
    "listener.start()\n",
    "\n",
    "if DATASET_TO_PROCESS == \"abi\":\n",
    "    # If processing Allen data\n",
    "    data = np.load(\"allen_visbehave_ca1_session_ids.npz\")\n",
    "    all_sesh_with_ca1_eid = data[\"data\"]\n",
    "    del data\n",
    "    print(f\"Loaded {len(all_sesh_with_ca1_eid)} sessions from allen_visbehave_ca1_session_ids.npz\")\n",
    "else:\n",
    "    # If processing IBL data\n",
    "    data = np.load(session_npz_filepath)\n",
    "    all_sesh_with_ca1_eid = data[\"all_sesh_with_ca1_eid_unique\"]\n",
    "    del data\n",
    "    print(f\"Loaded {len(all_sesh_with_ca1_eid)} sessions from {session_npz_filepath}\")\n",
    "\n",
    "# run the processes with the specified number of cores:\n",
    "with Pool(pool_size, initializer=init_pool, initargs=(queue,)) as p:\n",
    "    p.map(process_session, all_sesh_with_ca1_eid[0:2])\n",
    "\n",
    "queue.put(\"kill\")\n",
    "listener.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONE_ibl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
