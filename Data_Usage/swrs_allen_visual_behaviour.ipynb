{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b92443",
   "metadata": {},
   "source": [
    "\n",
    "# SWR Data Analysis for ABI Visual Behavior Dataset\n",
    "\n",
    "This notebook is designed for analyzing Sharp Wave Ripple (SWR) events in the ABI Visual Behavior dataset.\n",
    "It requires the allensdk_env conda environment to be loaded.\n",
    "\n",
    "The analysis includes:\n",
    "1. Loading and filtering SWR events\n",
    "2. Aligning events with CA1 units\n",
    "3. Analyzing unit responses to ripples\n",
    "4. Aligning events with running speed\n",
    "\n",
    "Note: Time is the unifying factor across all analyses - all data can be aligned using timestamps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666255b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3269104349.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    lfp_session_path =\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import mannwhitneyu\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorNeuropixelsProjectCache\n",
    "\n",
    "# Configuration\n",
    "CACHE_DIR = \"/space/scratch/allen_visbehave_data\"\n",
    "SWR_INPUT_DIR = \"/space/scratch/SWR_final_pipeline/osf_campbellmurphy2025_v2_final\"\n",
    "DATASET_NAME = \"allen_visbehave_swr_murphylab2024\"\n",
    "SESSION_ID = 1047969464  # Example session ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4729e62",
   "metadata": {},
   "source": [
    "## 1. Loading and Filtering SWR Events\n",
    "\n",
    "The SWR events are stored in compressed CSV files within session-specific folders.\n",
    "The folder structure is:\n",
    "```\n",
    "SWR_INPUT_DIR/\n",
    "└── DATASET_NAME/\n",
    "    └── swrs_session_{SESSION_ID}/\n",
    "        ├── probe_*_channel_*_putative_swr_events.csv.gz\n",
    "        ├── probe_*_channel_*_gamma_band_events.csv.gz\n",
    "        └── ...\n",
    "```\n",
    "\n",
    "Let's load and filter the events based on our criteria:\n",
    "- 3SD < power_max_zscore < 10SD\n",
    "- No gamma or movement overlap\n",
    "- Minimum sw_peak_power > 1SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_swr_events(session_id):\n",
    "    \"\"\"Load and filter SWR events for a given session.\"\"\"\n",
    "    session_path = os.path.join(SWR_INPUT_DIR, DATASET_NAME, f\"swrs_session_{session_id}\")\n",
    "    event_files = glob.glob(os.path.join(session_path, \"probe_*_channel_*_putative_swr_events.csv.gz\"))\n",
    "    \n",
    "    all_events = []\n",
    "    for event_file in event_files:\n",
    "        try:\n",
    "            events_df = pd.read_csv(event_file, compression='gzip')\n",
    "            if not events_df.empty:\n",
    "                all_events.append(events_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {event_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_events:\n",
    "        return pd.DataFrame([])\n",
    "    \n",
    "    events = pd.concat(all_events, ignore_index=True)\n",
    "    \n",
    "    # Filter events based on criteria\n",
    "    filtered = events[\n",
    "        (events['power_max_zscore'] > 3) & \n",
    "        (events['power_max_zscore'] < 10) &\n",
    "        (~events['overlaps_with_gamma']) &\n",
    "        (~events['overlaps_with_movement']) &\n",
    "        (events['sw_peak_power'] > 1)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Total events: {len(events)}\")\n",
    "    print(f\"Filtered events: {len(filtered)}\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949f6d9",
   "metadata": {},
   "source": [
    "## 2. Aligning with CA1 Units\n",
    "\n",
    "We'll analyze how CA1 units respond to SWR events using a simple Mann-Whitney U test.\n",
    "This is a relatively simple method that compares firing rates during SWR events vs baseline periods.\n",
    "Note: This analysis can take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791bd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unit_significance(session, unit_id, events_df, bin_size=0.01):\n",
    "    \"\"\"Calculate Mann-Whitney U test for a unit's firing during vs outside SWR events.\"\"\"\n",
    "    spike_times = session.spike_times[unit_id]\n",
    "    \n",
    "    if events_df.empty:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    session_start = events_df['start_time'].min()\n",
    "    session_end = events_df['end_time'].max()\n",
    "    time_bins = np.arange(session_start, session_end + bin_size, bin_size)\n",
    "    spike_counts = np.histogram(spike_times, bins=time_bins)[0]\n",
    "    \n",
    "    # Create during mask\n",
    "    during_mask = np.zeros_like(spike_counts, dtype=bool)\n",
    "    for _, event in events_df.iterrows():\n",
    "        start_bin = int((event['start_time'] - session_start) / bin_size)\n",
    "        end_bin = int((event['end_time'] - session_start) / bin_size)\n",
    "        during_mask[start_bin:end_bin] = True\n",
    "    \n",
    "    during_samples = spike_counts[during_mask]\n",
    "    baseline_samples = spike_counts[~during_mask]\n",
    "    \n",
    "    try:\n",
    "        stat, pval = mannwhitneyu(during_samples, baseline_samples, alternative='two-sided')\n",
    "        effect = np.mean(during_samples) - np.mean(baseline_samples)\n",
    "        return effect, pval\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def find_responding_units(session, events_df, target_region='CA1'):\n",
    "    \"\"\"Find units that respond significantly to SWR events.\"\"\"\n",
    "    cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=CACHE_DIR)\n",
    "    units = cache.get_unit_table()\n",
    "    \n",
    "    # Filter for good units in target region\n",
    "    region_units = units[\n",
    "        (units['ecephys_session_id'] == session.ecephys_session_id) & \n",
    "        (units['structure_acronym'] == target_region) &\n",
    "        (units['quality'] == 'good') &\n",
    "        (units['valid_data'] == True)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for unit_id in region_units.index:\n",
    "        effect, pval = calculate_unit_significance(session, unit_id, events_df)\n",
    "        if not np.isnan(effect):\n",
    "            results.append({\n",
    "                'unit_id': unit_id,\n",
    "                'effect': effect,\n",
    "                'pval': pval\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5baaba",
   "metadata": {},
   "source": [
    "## 3. Aligning with Running Speed\n",
    "\n",
    "We'll analyze how SWR events relate to running speed.\n",
    "Note: Running speed data may have gaps that need to be checked session by session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_swr_running_alignment(session, events_df, window=0.5):\n",
    "    \"\"\"Plot SWR events aligned with running speed.\"\"\"\n",
    "    running_speed = session.running_speed\n",
    "    \n",
    "    # Print running speed data info\n",
    "    print(\"\\nRunning Speed Data Info:\")\n",
    "    print(f\"Number of samples: {len(running_speed)}\")\n",
    "    print(f\"Time range: {running_speed['timestamps'].min():.2f} to {running_speed['timestamps'].max():.2f}\")\n",
    "    \n",
    "    # Check for gaps in running speed data\n",
    "    run_times = running_speed['timestamps'].values\n",
    "    run_gaps = np.diff(run_times)\n",
    "    median_gap = np.median(run_gaps)\n",
    "    large_gaps = np.where(run_gaps > 5 * median_gap)[0]\n",
    "    if len(large_gaps) > 0:\n",
    "        print(f\"\\nFound {len(large_gaps)} large gaps in running speed data\")\n",
    "        print(\"Large gaps at times:\", run_times[large_gaps])\n",
    "    \n",
    "    # Plot running speed with SWR events\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(running_speed['timestamps'], running_speed['speed'], 'b-', alpha=0.5, label='Running Speed')\n",
    "    \n",
    "    # Mark SWR events\n",
    "    for _, event in events_df.iterrows():\n",
    "        plt.axvspan(event['start_time'], event['end_time'], \n",
    "                   color='red', alpha=0.2, label='SWR Event' if _ == 0 else \"\")\n",
    "        if not np.isnan(event['power_peak_time']):\n",
    "            plt.axvline(event['power_peak_time'], color='red', linestyle=':', \n",
    "                       label='SWR Peak' if _ == 0 else \"\")\n",
    "    \n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Running Speed (cm/s)')\n",
    "    plt.title('SWR Events Aligned with Running Speed')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17e208",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's how to use the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aea352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load session\n",
    "cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=CACHE_DIR)\n",
    "session = cache.get_ecephys_session(SESSION_ID)\n",
    "\n",
    "# Load and filter SWR events\n",
    "swr_events = load_and_filter_swr_events(SESSION_ID)\n",
    "\n",
    "# Find responding CA1 units\n",
    "responding_units = find_responding_units(session, swr_events)\n",
    "print(\"\\nResponding Units:\")\n",
    "print(responding_units.sort_values('pval').head())\n",
    "\n",
    "# Plot alignment with running speed\n",
    "plot_swr_running_alignment(session, swr_events) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
